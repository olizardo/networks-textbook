[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Networks",
    "section": "",
    "text": "Welcome\nThis is an introductory textbook on social networks to be used in conjunction with an undergraduate lecture class on social networks (SOCIOL 111) taught in the department of sociology at UCLA."
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#the-correlation-distance",
    "href": "lesson-sna-blockmodeling.html#the-correlation-distance",
    "title": "19  Blockmodeling",
    "section": "19.1 The Correlation Distance",
    "text": "19.1 The Correlation Distance\nIt turns out there is an even fancier way to find out whether two nodes in a graph are structurally similar. It relies on a more complicated measure of distance called the correlation distance. This measure compares the row (or column) vectors of nodes in a graph and returns a number between \\(-1\\) and \\(+1\\). When it comes to structural equivalence, the correlation distance works like this:\n\nPairs of structurally equivalent nodes get a \\(+1\\). Nodes that are almost structurally equivalent but not quite (they are structurally similar) get a positive number larger than zero. The closer that number is to \\(+1\\) the more structurally similar the two nodes are.\nNodes that are completely different from one another (that is connect to completely disjoint sets of neighbors) get a \\(-1\\). In this case, nodes are opposites: Every time one node i has a \\(1\\) in their row vector in the adjacency matrix, the other has a \\(0\\) and vice versa. Nodes that are structurally dissimilar thus get a negative number between zero and \\(-1\\). The closer that number is to \\(-1\\), the more structurally dissimilar the two nodes are.\nNodes that have an even combination of similarities and differences in their pattern of connectivity to others get a number close to zero, with \\(0\\) indicating that two nodes have an even number of commonalities and differences.\n\nThe correlation distance between two nodes k and l is computed using the following formula:\n\\[\n    d^{corr}_{k, l} =\n    \\frac{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j}) \\times\n    (a_{(l)j} - \\overline{a}_{(l)j})\n    }\n    {\n    \\sqrt{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j})^2 \\times\n    \\sum_j\n    (a_{(l)j} - \\overline{a}_{(l)j})^2\n        }\n    }\n\\tag{19.1}\\]\nEquation 19.1 looks like a monstrously complicated one, but it is actually not that involved.\nLet’s go through each components that we have already encountered in the lesson structural equivalence and structural similarity:\n\n\\(a_{(k)j}\\) is the row (or column) vector for node k in the adjacency matrix.\n\\(a_{(l)j}\\) is the row (or column) vector for node l in the adjacency matrix.\n\nNow let’s introduce ourselves to some new friends. For instance, what the heck is \\(\\overline{a}_{(k)j}\\)? The little “bar” on top the \\(a\\) indicates that we are taking the mean or the average of the elements of the row vector.\nIn equation form:\n\\[\n\\overline{a}_{(k)j} = \\frac{\\sum_j a_{kj}}{N}\n\\tag{19.2}\\]\nIn Equation 19.2, \\(\\sum_i a_{kj}\\) is the sum of all the elements in the vector, and \\(N\\) is the length of the vector, which is equivalent to the order of the graph from which adjacency matrix came from (the number of nodes in the network).\nSo for instance, in Table 17.1, the row vector for node A is:\n\n\n\n\nTable 19.1: Row vector for node A.\n\n\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\nWhich implies:\n\\[\n\\sum_i a_{(A)j} = 0 + 0 + 1 + 1 + 0 + 0 = 2\n\\]\nAnd we know that \\(N = 6\\), so that implies:\n\\[\n\\overline{a}_{(A)j} = \\frac{\\sum_i a_{Aj}}{N} = \\frac{2}{6}=0.33\n\\] The term \\(\\overline{a}_{(k)j}\\) is called the row mean for node k in the adjacency matrix. Just like we can compute row means, we can also compute column means by using the elements of the column vector \\(\\overline{a}_{(k)i}\\)\nNow that we know what the row means are, we can make sense of the term \\((a_{(k)j} - \\overline{a}_{(k)j})\\) in Equation 19.1. This is a vector composed of the differences between the row vector entries in the adjacency matrix and the row mean for that node. So in the case of node A and the row vector in Table 19.1 that would imply:\n\n\nTable 19.2: Row vector of mean differences for node A.\n\n\n\n\n(a) Vector of row entries minus the row mean.\n\n\n\n\n\n\n\n\n\n\n(0 - 0.33)\n(0 - 0.33)\n(1 - 0.33)\n(1 - 0.33)\n(0 - 0.33)\n(0 - 0.33)\n\n\n\n\n\n\n\n\n(b) Result of subtracting row mean from row entries.\n\n\n-0.33\n-0.33\n0.67\n0.67\n-0.33\n-0.33\n\n\n\n\n\n\nWhich implies: \\[\n\\sum_j (a_{(k)j} - \\overline{a}_{(k)j}) = -0.33 -0.33 + 0.67 + 0.67 -0.33 -0.33 = 0.02\n\\]\nThe numerator of Equation 19.1, just says: “Take the entries in the row vector for the first node, and create a new vector composed of the those entries minus the row means and sum the vector. Then do the same for the other node and multiply the two numbers” And in the denominator of the equation we just square the same vectors sum them, multiply each of the two numbers and take the square root of the result product. Once we have the numerator and denominator we can evaluate the fraction and compute the correlation distance between those two nodes.\nWhen we do that for each pair of nodes in Table 17.1, we end up with the structural similarity matrix shown in Table 19.3, based on the correlation distance.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.71\n-0.71\n0.71\n-0.32\n\n\nB\n1\n–\n-0.71\n-0.71\n0.71\n-0.32\n\n\nC\n-0.71\n-0.71\n–\n1\n-1\n0.45\n\n\nD\n-0.71\n-0.71\n1\n–\n-1\n0.45\n\n\nE\n0.71\n0.71\n-1\n-1\n–\n-0.45\n\n\nF\n-0.32\n-0.32\n0.45\n0.45\n-0.45\n–\n\n\n\nTable 19.3: Correlation distance matrix for an undirected graph.\n\n\nIn Table 19.3, the structurally equivalent pairs of nodes, A and B and C and D have \\(d^{corr} = 1.0\\). Nodes that are completely non-equivalent like C and E and D and E have \\(d^{corr} = -1.0\\). Nodes that are structurally similar, but not equivalent, like nodes C and F (\\(d^{corr} = 0.45\\)) get a positive number that is less than \\(1.0\\)."
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#iterated-correlational-distances-concor",
    "href": "lesson-sna-blockmodeling.html#iterated-correlational-distances-concor",
    "title": "19  Blockmodeling",
    "section": "19.2 Iterated Correlational Distances: CONCOR",
    "text": "19.2 Iterated Correlational Distances: CONCOR\nWhat happens if we were to try to use Equation 19.1 to find the correlation distance of a correlation distance matrix? If we were to do this and use Table 19.3 as our input matrix, we end up with Table 19.4 (a).\n\n19.2.1 We Need to go Deepah\nNow, as Leo (when stuck in a dream, about a dream, about a dream…) always says: “We need to go deeper.”1 And, indeed, we can. We can take the correlation distance of the nodes based on Table 19.4 (a). If we do that, we end up with the entries in Table 19.4 (b). If we keep on going, we end up with the entries in Table 19.4 (c). Note that in Table 19.4 (c), there are only two values for all the entries: \\(+1\\) and \\(-1\\)!\n\n\nTable 19.4: Iterated correlation distances.\n\n\n\n\n(a) Second Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.97\n-0.97\n0.97\n-0.84\n\n\nB\n1\n–\n-0.97\n-0.97\n0.97\n-0.84\n\n\nC\n-0.97\n-0.97\n–\n1\n-1\n0.84\n\n\nD\n-0.97\n-0.97\n1\n–\n-1\n0.84\n\n\nE\n0.97\n0.97\n-1\n-1\n–\n-0.84\n\n\nF\n-0.84\n-0.84\n0.84\n0.84\n-0.84\n–\n\n\n\n\n\n\n(b) Third Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-0.99\n\n\nB\n1\n–\n-1\n-1\n1\n-0.99\n\n\nC\n-1\n-1\n–\n1\n-1\n0.99\n\n\nD\n-1\n-1\n1\n–\n-1\n0.99\n\n\nE\n1\n1\n-1\n-1\n–\n-0.99\n\n\nF\n-0.99\n-0.99\n0.99\n0.99\n-0.99\n–\n\n\n\n\n\n\n\n\n(c) Fourth Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-1\n\n\nB\n1\n–\n-1\n-1\n1\n-1\n\n\nC\n-1\n-1\n–\n1\n-1\n1\n\n\nD\n-1\n-1\n1\n–\n-1\n1\n\n\nE\n1\n1\n-1\n-1\n–\n-1\n\n\nF\n-1\n-1\n1\n1\n-1\n–\n\n\n\n\n\n\nMore importantly, as shown in Table 19.5, the structurally equivalent nodes have exactly the same pattern of \\(+1\\)s and \\(-1\\)s across the rows. This procedure of iterated correlations, invented by a team of sociologists and psychologists at Harvard University in the mid-1970s (Breiger, Boorman, and Arabie 1975), is called CONCOR—and acronym for the hard to remember title of “convergence of iterate correlations’’—and is designed to extract structurally equivalent positions from networks even when the input matrix is just based on structural similarities.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    -- \n    1 \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    B \n    1 \n    -- \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    C \n    -1 \n    -1 \n    -- \n    1 \n    -1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    1 \n    -- \n    -1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    -1 \n    -1 \n    -- \n    -1 \n  \n  \n    F \n    -1 \n    -1 \n    1 \n    1 \n    -1 \n    -- \n  \n\n\n\nTable 19.5:  Structurally Equivalent Positions in an undirected graph."
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#blockmodeling",
    "href": "lesson-sna-blockmodeling.html#blockmodeling",
    "title": "19  Blockmodeling",
    "section": "19.3 Blockmodeling",
    "text": "19.3 Blockmodeling\nThe example we considered previously concerns the relatively small network shown in Figure 17.1. What happens when we apply the method of iterated correlations (CONCOR) to a bigger network, something like the one shown in Figure 19.1?\n\n\n\n\n\nFigure 19.1: An undirected graph.\n\n\n\n\nWell, we can begin by computing the correlation distance across all ,the \\(V=22\\) nodes in that network using Equation 19.1. The result of that looks like Table 19.6 (a). Note that even before we do any iterated correlations of correlation matrices we can see that the peripheral, single-connection nodes \\(E, F, G, H\\), \\(I, J, K, L, M\\) and \\(N, O, P, Q, R\\) are perfectly structurally equivalent. This makes sense, because all the nodes in each of these three groups have identical neighborhoods, since they happen to be connected to the same central node \\(A\\) for the first group, \\(B\\) for the second group and \\(C\\) for the third group. Note also that \\(U\\) and \\(V\\) are structurally equivalent, since their neighborhoods are the same: Their single connection is to node \\(S\\).\n\n\nTable 19.6: Correlation Distance Matrices Corresponding to an Undirected Graph.\n\n\n\n\n(a) Original Correlation Distance Matrix. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1.00 \n    -0.15 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    0.05 \n    -0.13 \n    -0.13 \n  \n  \n    B \n    -0.15 \n    1.00 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    C \n    -0.15 \n    -0.15 \n    1.00 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    D \n    -0.24 \n    -0.24 \n    -0.24 \n    1.00 \n    0.34 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    -0.16 \n    -0.09 \n    -0.09 \n  \n  \n    T \n    -0.19 \n    -0.19 \n    -0.19 \n    0.34 \n    1.00 \n    0.69 \n    0.69 \n    0.69 \n    0.69 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.13 \n    0.69 \n    0.69 \n  \n  \n    E \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    F \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    G \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    H \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    I \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    J \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    K \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    L \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    M \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    N \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    O \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    P \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    Q \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    R \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    S \n    0.05 \n    -0.24 \n    -0.24 \n    -0.16 \n    -0.13 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    1.00 \n    -0.09 \n    -0.09 \n  \n  \n    U \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n  \n    V \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(b) Original Correlation Distance Matrix After Ten Iterations. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(c) Correlation Distance Matrix in (a) with Rows and Columns Reshuffled to Show Hidden Pattern. \n \n  \n      \n    V \n    U \n    S \n    H \n    G \n    F \n    E \n    T \n    C \n    A \n    B \n    R \n    Q \n    P \n    O \n    N \n    M \n    L \n    K \n    J \n    D \n    I \n  \n \n\n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWhat happens when we take the correlation distance of the correlation distance matrix shown in Table 19.6 (a), and the correlation distance of the resulting matrix, and keep going until we only have zeros and ones? The results is Table 19.6 (b). This matrix seems to reveal a much deeper pattern of commonalities in structural positions across the nodes in Figure 19.1. In fact, if we were a conspiracy theorist like Charlie from Always Sunny in Philadelphia, we might even surmise that there is a secret pattern that can be revealed if we reshuffled the rows and the columns of the matrix (without changing any of the numbers of course!).2\nIf we do that, we end up with Table 19.6 (c). So it turns out that there is indeed a secret pattern! The reshuffling shows that the nodes in the network can be divided into two blocks such within blocks all nodes are structurally similar (and some structurally equivalent) and across blocks, all nodes are structurally dissimilar. Thus \\(V, U, S, H, G, F, E, T, C, A, B\\) are members of one structurally similar block (let’s called them “Block 1”), and nodes \\(R, Q, P, O, N, M, L, K, J, D, I\\) are members of another structurally similar block (let’s called them “Block 2”). Nodes in “Block 1” are structurally dissimilar from nodes in “Block 2,” but structurally similar to one another and vice versa. To illustrate, Figure 19.2 is the same as Figure 19.1, but this time nodes are colored by their memberships in two separate blocks.\n\n\n\n\n\nFigure 19.2: An undirected graph with block membership indicated by node color.\n\n\n\n\nNote that we haven’t changed any of the information in Table 19.6 (b) to get Table 19.6 (c). If you check, the row and column entries for each node in both figures are identical. It’s just that we changed the way the rows ordered vertically and the way the columns are ordered horizontally. For instance, node \\(A\\)’s pattern of connections is negatively correlated with node \\(I\\)’s in Table 19.6 (b), and has the same negative correlation entry in Table 19.6 (c). The same goes for each one of node \\(A\\)’s other correlations, and the same for each node in the table. Table 19.6 (b) and Table 19.6 (c) contain the same information it’s just that Table 19.6 (c) makes it easier to see a hidden pattern.\nThis property of the method of iterated correlations is the basis of a strategy for uncovering blocks of structurally similar actors in a network developed by a team of sociologists, physicists, and mathematicians working at Harvard in the 1970s. The technique is called blockmodeling (White, Boorman, and Breiger 1976). Let’s see how it works.\n\n19.3.1 We Need to go Deepah\nOf course, as Leo always says: “We need to go deeper.” And indeed we can. What happens if we do the same analysis as above, but this time in the two node-induced subgraphs defined by the set of structurally similar nodes in each of the two blocks we uncovered in the original graph? Then we get Table 19.7 (a) and Table 19.7 (b).\n\n\nTable 19.7: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    B \n    A \n    C \n    S \n    V \n    U \n    T \n    E \n    F \n    H \n    G \n  \n \n\n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    I \n    J \n    K \n    M \n    L \n    D \n    N \n    O \n    P \n    R \n    Q \n  \n \n\n  \n    I \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWe can see that Table 19.6 (a) separates our original Block 1 into two further sub-blocks. Let’s call them “Block 1a” and “Block 1b.” Block 1a is composed of nodes \\(A, B, C, S, U, V\\) and Block 1b is composed of nodes \\(E, F, G, H, T\\). Table 19.6 (b) separates our original Block 2 into three further sub-blocks. There’s the block composed of nodes \\(I, J, K, L, M\\). Let’s call this “Block 2a”, the block composed of nodes \\(N, O, P, Q, R\\). Let’s call this “Block 2b.” Then, there’s node \\(D\\). Note that this node is only structurally similar to itself and is neither similar nor dissimilar to the other nodes in the subgraph \\(d^{corr} = 0\\), so it occupies a position all by itself! Let’s call it “Block 2c.”\n\n\nTable 19.8: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    S \n    C \n    B \n    A \n    V \n    U \n  \n \n\n  \n    S \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    V \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    U \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    B \n    C \n    A \n    S \n  \n \n\n  \n    B \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    S \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\nNow let’s do a couple of final splits of the subgraph composed of nodes \\(A, B, C, S, U, V\\). This is shown in Table 19.8. The first split separates nodes in block \\(A, B, C, S\\) from those in block \\(U, V\\) (Table 19.8 (a)). The second splits the nodes in subgraph \\(A, B, C, S\\) into two blocks composed of \\(A, S\\) and \\(B, C\\), respectively (Table 19.8 (b)).\n\n\n\n\n\nFigure 19.3: An undirected graph with block membership indicated by node color.\n\n\n\n\nFigure 19.3 shows the nodes in Figure 19.1 colored according to our final block partition. It is clear that the blockmodeling approach captures patterns of structural similarity. For instance, all the single-connection nodes connected to more central nodes get assigned to their own position: Block 1b: \\(E, F, G, H, T\\), Block 2a: \\(I, J, K, L, M\\), and Block 2b: \\(N, O, P, Q, R\\). The most central node \\(D\\) (in terms of Eigenvector centrality) occupies a unique position in the graph. Two of the three central nodes (in terms of degree centrality) \\(B, C\\) get assigned to their own position. Meanwhile \\(A, S\\) form their own structurally similar block. Finally, \\(U, V\\) also form their own structurally similar block as both are structurally equivalent in the orignal graph.\n\n\n19.3.2 The Blocked Adjancency Matrix\nWhat happens if we were to go back fo the adjacency matrix corresponding to Figure 19.1, and then reshuffle the rows and columns to correspond to all these wonderful blocks we have uncovered? Well, we would en up with something like Table 19.9. This is called the blocked adjacency matrix. In the blocked adjacency matrix, the division between the nodes corresponding to each block of structurally similar nodes in Table 19.7 and Table 19.8 is marked by thick black lines going across the rows and columns.\nEach diagonal rectangle in Table 19.9 corresponds to within-block connections. Each off-diagonal rectangle corresponds to between block connections. There are two kinds of rectangles in the blocked adjacency matrix. First, there are rectangles that only contains zero entries. These are called zero blocks. For instance the top-left rectangle in Table 19.9 is a zero block. Then there rectangles that have some non-zero entries in them (ones, since this is a binary adjacency matrix). These are called one blocks. For instance, the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) is a one block.\n\n\n\n\n\n \n  \n      \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    T \n    E \n    F \n    G \n    H \n    B \n    C \n    A \n    S \n    U \n    V \n    D \n  \n \n\n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    M \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    O \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    P \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    Q \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    R \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    T \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    S \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n  \n  \n    U \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    V \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\nTable 19.9:  Blocked adjancency matrix. \n\n\nZero-blocks indicate that the members of the row block don’t have any connections with the members the column block (which can include themselves!). For instance, the zero-block in the top-left corner of the blocked adjacency matrix in Table 19.9 indicates that the members of this block are not connected to one another in the network (and we can verify from Figure 19.3 that this is indeed the case).\nOne blocks indicate that the members of the column block share some connections with the members of the column block (which can also include themselves!). For instance, the one-block in the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) tells us that members of this block are connected to at least one member of the \\(I, J, K, L, M\\) block (and we can verify from Figure 19.3 that this is indeed the case, since \\(B\\) is connected to all of them).\n\n\n19.3.3 The Image Matrix\nFrom this reshuffled adjacency matrix, we can get to a reduced image matrix containing the relations not between the nodes in the graph, but between the blocks in the graph. The way we proceed to construct the image matrix is as follows:\n\nFirst we create an empty matrix \\(\\mathbf{B}\\) of dimensions \\(b \\times b\\) where \\(B\\) is the number of blocks in the blockmodel. In our example, \\(b = 7\\) so the image matrix has seven rows and seven columns. The \\(ij^{th}\\) cell in the image matrix \\(\\mathbf{B}\\) records the relationship between row block i and column block j in the blockmodel.\nSecond, we put a zero in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 19.9 is a zero-block.\nThird, we put a one in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 19.9 is a one-block.\n\nThe result is Table 19.10.\n\n\n\n\n\n \n  \n      \n    I, J, K, L \n    N, O, P, Q, R \n    T, E, F, G, H \n    B, C \n    A, S \n    U, V \n    D \n  \n \n\n  \n    I, J, K, L \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    N, O, P, Q, R \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    T, E, F, G, H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B, C \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A, S \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    U, V \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\nTable 19.10:  Image matrix Corresponding to the blockmodel of an Undirected Graph. \n\n\nSo the big blocked adjacency matrix in Table 19.6 (c) can be reduced to the image matrix shown Table 19.10, summarizing the relations between the blocks in the graph. This matrix, can then even be represented as a graph, so that we can see the pattern of relations between blocks! This is shown in Figure 19.4\n\n\n\n\n\nFigure 19.4: Graph representation of reduced image matrix from a blockmodel.\n\n\n\n\nThis is how blockmodeling works!"
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#references",
    "href": "lesson-sna-blockmodeling.html#references",
    "title": "19  Blockmodeling",
    "section": "References",
    "text": "References\n\n\n\n\nBreiger, Ronald L, Scott A Boorman, and Phipps Arabie. 1975. “An Algorithm for Clustering Relational Data with Applications to Social Network Analysis and Comparison with Multidimensional Scaling.” Journal of Mathematical Psychology 12 (3): 328–83.\n\n\nWhite, Harrison C, Scott A Boorman, and Ronald L Breiger. 1976. “Social Structure from Multiple Networks. I. Blockmodels of Roles and Positions.” American Journal of Sociology 81 (4): 730–80."
  },
  {
    "objectID": "lesson-sna-centrality.html#the-big-three-centrality-metrics",
    "href": "lesson-sna-centrality.html#the-big-three-centrality-metrics",
    "title": "21  Centrality",
    "section": "21.1 The “big three” centrality metrics",
    "text": "21.1 The “big three” centrality metrics\nLinton Freeman (1979), in the aforementioned paper, defined the “big three” classic centrality metrics, roughly corresponding to the extent that a node accumulates one of the three network goods mentioned above. - So the degree centrality metric deal with nodes that have more edges directly incident upon them (Nieminen 1974). - The closeness centrality metric has to do with nodes that can reach more nodes via smallest shortest paths and thus accumulate as many of these paths in which they figure as the origin node as possible (Sabidussi 1966). - Finally, the betweenness centrality metric has to do with a node’s accumulation of the largest share of shortest paths in which they intermediate between two other nodes, and thus featuring them as one of the inner nodes in the paths between others (Freeman 1977).\nOther centrality metrics can be seen as generalizations or special cases of any of these three basic notions (Borgatti 2005).\nThe rest of the lesson goes over the basic interpretation and calculation (using the graph theory and matrix algebra tools discussed in previous lessons) of “big three” centrality metrics."
  },
  {
    "objectID": "lesson-sna-centrality.html#the-star-graph",
    "href": "lesson-sna-centrality.html#the-star-graph",
    "title": "21  Centrality",
    "section": "21.2 The Star Graph",
    "text": "21.2 The Star Graph\nFreeman showed that the three basic measures reach their theoretical maximum for the central node in a star graph, such as the one shown in Figure 21.1).\n\n\n\n\n\nFigure 21.1: A star graph\n\n\n\n\nA star graph is a graph containing a central or inner node (in Figure 21.1, node A), who is connected to all the other nodes in the graph, called the satellite or outer nodes (in Figure 21.1, nodes B through F). These nodes in contrast have only one connection and that is to the central node, none among themselves.\nBecause of these restrictions, it is easy to see that if \\(G = (V, E)\\) is a star graph of order \\(n\\), then we know that that graph size \\(m = |E|\\) (the size of the edge set), has to be \\(n-1\\). So in the example shown in Figure 21.1, \\(n =7\\) and \\(m = n-1 = 7-1=6\\). Neat!"
  },
  {
    "objectID": "lesson-sna-centrality.html#degree-centrality",
    "href": "lesson-sna-centrality.html#degree-centrality",
    "title": "21  Centrality",
    "section": "21.3 Degree Centrality",
    "text": "21.3 Degree Centrality\nThe most basic way of defining centrality is simply as a measure of how many alters an ego is connected to. This simply takes a node’s degree as introduced in the lesson on graph theory, and begins to consider this measure as a reflection of importance of the node in the network. The logic is that those with more direct connections to others, compared to those with fewer, hold a more prominent place in the network.\nOnce we have constructed the adjacency matrix for the network (A), then degree centrality is easy to calculate. As Equation 21.1 for a given node i the degree centrality is given by summing the entries of its corresponding row.\n\\[\n  C_i^{DEG} = \\sum_{j= 1}^{n}a_{ij}\n\\tag{21.1}\\]\nEquation 21.1 thus ranks each node in the graph based on the number of other nodes that it is adjacent to. Just like real life, some nodes will be popular (they will be adjacent to lots of other nodes), while others will be unpopular.\nAlthough it might seem a simple task to just add up the number of connections of each node, that is essentially what the below mathematical equation is doing! Mathematical notation plays an important role in expressing network measures in succinct formats.\nFor instance, if we were to use Equation 21.1 to calculate the degree centrality of each node from the symmetric adjacency matrix corresponding to the graph shown in Figure 4.1 then we would end up with the following degree centralities for each node:\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 21.1: Degree centralities of nodes in an undirected graph."
  },
  {
    "objectID": "lesson-sna-centrality.html#indegree-and-outdegree-centrality",
    "href": "lesson-sna-centrality.html#indegree-and-outdegree-centrality",
    "title": "21  Centrality",
    "section": "21.4 Indegree and Outdegree Centrality",
    "text": "21.4 Indegree and Outdegree Centrality\nIf we are talking about a directed graph, then there are two types of degree centralities that can be calculated. On the one hand, we may be interested in how central a node is in terms of sociability or expansiveness that is how many other nodes in the graph a given node sends links to. This is called the outdegree centrality of that node, written as \\(C_i^{OUT}\\). As with the undirected case, this is computed by summing across the rows of the asymmetric adjacency matrix corresponding to the directed graph in question, using Equation 21.1:\n\\[\n  C_i^{OUT} = \\sum_ja_{ij}\n\\tag{21.2}\\]\nHowever, in a directed graph, we may also be interested in how popular or sought after by others a given node is. That is, how many other actors send ties to that node. In which case we need to sum across the columns of the asymmetric adjacency matrix, and modify the formula as follows:\n\\[\n  C_i^{IN} = \\sum_ia_{ij}\n\\tag{21.3}\\]\nNote that in this version of the equation, we are summing over j (the columns) not over i (the rows) as given by subscript under the \\(\\sum\\) symbol.\nFor instance, if we were to use equations Equation 21.2 and Equation 21.2 to calculate the outdegree and indegree centrality of each node from the asymmetric adjacency matrix corresponding to the graph shown in Figure 4.2), then we would up with the following centralities for each node:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nOutdegre\n2\n2\n1\n1\n2\n1\n2\n\n\nIndegree\n2\n3\n1\n3\n0\n2\n0\n\n\n\nTable 21.2: Out and Indegree centralities of nodes in a directed graph.\n\n\nJust like the degree centrality for undirected graphs, the outdegree and indegree centralities rank each node in a directed graph. The first, outdegree centrality, ranks each node based on the number of other nodes that they are connected to. This is a kind of popularity based on sociability, or the tendency to seek out the company of others. The second, indegree centrality, ranks each node in the graph based on the number of other nodes that connect to that node. This is a kind of popularity based on on being sought after a kind of status.\n\n21.4.1 Normalized Degree Centrality\nWhen we compute the degree centrality of a node, are counting the number of other nodes that they are connected to. Obviously, the more nodes there are to connect to, the more opportunities there will be to reach a larger number. But what happens if we wanted to compare the degree centrality of nodes in two very different networks?\nFor instance, if your high-school has one thousand people and you have twenty friends, that’s very different from having twenty friends in a high-school of only one hundred people. It seems like the second person, with twenty friends (covering 20% of the population) in a high-school of one-hundred people is definitely more popular than the second person with twenty friends (covering 2% of the population), in a high school with one thousand people.\nThat’s why Freeman Freeman (1979) proposed normalizing the degree centrality of each node by the maximum possible it can take in a given network. As you may have guessed, the maximum degree in a network is \\(N-1\\) the order of the graph minus one. Essentialy, everyone but you!\nWe can compute the normalized degree centrality using the following equation:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{C_{i}^{DEG}}{N-1}\n\\tag{21.4}\\]\nWhere we just divide the regular degree centrality computed using Equation 21.1 by the order of the graph minus one. This will be equal to \\(1.0\\) if a person knows everyone and \\(0\\) is a person knows no one. For all the other nodes it will be a number between zero and one.\nMoreover, this measure is sensitive to the order of the graph. Thus, for a person with twenty friends in a high-school of a thousand people, the normalized degree centrality is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{1000-1}= 0.02\n\\tag{21.5}\\]\nBut for the person with the same twenty friends in a high-school of one-hundred people, it is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{100-1}= 0.20\n\\tag{21.6}\\]\nIndicating that a person with the same number of friends in the smaller place is indeed more central!"
  },
  {
    "objectID": "lesson-sna-centrality.html#closeness-centrality",
    "href": "lesson-sna-centrality.html#closeness-centrality",
    "title": "21  Centrality",
    "section": "21.5 Closeness Centrality",
    "text": "21.5 Closeness Centrality\nSometimes it not important how many people you directly connected to. Instead, what is important is that you are indirectly connected to a lot of others. As we saw in the lesson on indirect connectivity, the best way to conceptualize indirect connectivity in social networks is via the idea of shortest paths. So if you can reach the most other people in the network via shortest paths with only a few hops, then you are better connected that someone who has to use longer paths to reach the same other people.\n\n\n\n\n\nFigure 21.2: An undirected graph showing the node with the maximum closeness centrality (in red).\n\n\n\n\nThis insight serves as an inspiration for a measure of centrality based on closeness. The closeness between two nodes is the inverse of the geodesic distance them (Bavelas 1950). Recall that the geodesic distance is given by the length of the shortest path linking two nodes in the graph. The smallest the length of the shortest path separating two nodes in the graph, the closer the two nodes and vice versa.\nRemember that for any number \\(n\\), the mathematical operation of taking the inverse simply means dividing one by that number. So, the inverse of \\(n\\) is \\(\\frac{1}{n}\\). This means that if \\(d_{ij}\\) is the geodesic distance between nodes i and j in graph \\(G\\), then the closeness between two nodes is \\(\\frac{1}{d+_{ij}}\\).\nThe information on the pairwise geodesic distances between every pair of nodes in a given graph is captured in the geodesic distance matrix, as discussed in ?sec-specmats. For instance, take the graph shown in Figure 21.2. The distance matrix for this graph is shown in Table 21.3.\n\n\n\n\n\n\n\n\nE\nA\nM\nL\nB\nJ\nG\nN\nF\nH\nD\nC\nK\nI\n\n\n\n\nE\n0\n1\n2\n1\n2\n1\n2\n1\n2\n1\n3\n2\n2\n2\n\n\nA\n1\n0\n3\n1\n3\n2\n2\n2\n3\n2\n2\n1\n1\n2\n\n\nM\n2\n3\n0\n3\n1\n2\n2\n1\n2\n2\n1\n3\n2\n4\n\n\nL\n1\n1\n3\n0\n3\n2\n1\n2\n2\n2\n2\n1\n2\n1\n\n\nB\n2\n3\n1\n3\n0\n1\n2\n2\n1\n2\n2\n2\n3\n4\n\n\nJ\n1\n2\n2\n2\n1\n0\n2\n1\n1\n2\n2\n1\n3\n3\n\n\nG\n2\n2\n2\n1\n2\n2\n0\n3\n1\n2\n1\n2\n2\n2\n\n\nN\n1\n2\n1\n2\n2\n1\n3\n0\n2\n1\n2\n2\n2\n3\n\n\nF\n2\n3\n2\n2\n1\n1\n1\n2\n0\n1\n1\n2\n2\n3\n\n\nH\n1\n2\n2\n2\n2\n2\n2\n1\n1\n0\n2\n3\n1\n3\n\n\nD\n3\n2\n1\n2\n2\n2\n1\n2\n1\n2\n0\n3\n1\n3\n\n\nC\n2\n1\n3\n1\n2\n1\n2\n2\n2\n3\n3\n0\n2\n2\n\n\nK\n2\n1\n2\n2\n3\n3\n2\n2\n2\n1\n1\n2\n0\n3\n\n\nI\n2\n2\n4\n1\n4\n3\n2\n3\n3\n3\n3\n2\n3\n0\n\n\n\nTable 21.3: Geodesic distance matrix for an undirected graph.\n\n\nAs shown in Table 21.3, a node like I, who seems to be at the outskirts of the network, also shows up as having the largest geodesic distances from other nodes in the graph. Other nodes, like E, G, and L seem to be “closer” to others, in terms of having to traverse smaller geodesic distances to reach them.\nThat means that we can use the distance table to come up with a measure of centrality called closeness centrality for each node. We can do that by adding up the entries corresponding to each row in the distance matrix (\\(\\sum_j d_{ij}\\)), to get a summary the total pairwise distances separating the node corresponding to row i in the matrix from the other nodes listed in each column j.\nNote that because closeness is better than “farness,” we would want the node with highest closeness centrality to be the one with the smallest sum of pairwise distances. This can be calculated using the following equation:\n\\[\n  C_i^{CLOS} = \\frac{1}{\\sum_jd_{ij}}\n\\tag{21.7}\\]\nIn Equation 21.7, the denominator is the sum across each column j, for each row i in Table 21.3 which corresponds to the distance between node i and each of the other nodes in the graph j (skipping the diagonal cell when \\(i=j\\), because the geodesic distance of node to itself is always zero!).\nAs noted, we take the mathematical inverse of this quantity, dividing one by the sum of the distances, so that way, the smallest number comes out on top and the bigger number comes out on the bottom (since, as we said, we want to measure closeness not “farness.”)\nLet’s see how this work for the graph in Figure 21.2. First, we get the row sums of geodesic distances from Table 21.3. These are shown in the first column of Table 21.4, under the heading “Sum of Distances.” This seems to work; node \\(E\\) has the smallest number here (\\(\\sum_j d_{Ej} = 22\\)) suggesting it can reach the most nodes via the shortest paths. Node \\(I\\) has the largest number (\\(\\sum_j d_{Ij} = 35\\)) indicating it is the most isolated from the other nodes.\n\n\n\n\n\n \n  \n      \n    Sum of Distances (d) \n    Inverse (1/d) \n    Normalized (N-1/d) \n  \n \n\n  \n    E \n    22 \n    0.045 \n    0.59 \n  \n  \n    A \n    25 \n    0.040 \n    0.52 \n  \n  \n    M \n    28 \n    0.036 \n    0.46 \n  \n  \n    L \n    23 \n    0.043 \n    0.57 \n  \n  \n    B \n    28 \n    0.036 \n    0.46 \n  \n  \n    J \n    23 \n    0.043 \n    0.57 \n  \n  \n    G \n    24 \n    0.042 \n    0.54 \n  \n  \n    N \n    24 \n    0.042 \n    0.54 \n  \n  \n    F \n    23 \n    0.043 \n    0.57 \n  \n  \n    H \n    24 \n    0.042 \n    0.54 \n  \n  \n    D \n    25 \n    0.040 \n    0.52 \n  \n  \n    C \n    26 \n    0.038 \n    0.50 \n  \n  \n    K \n    26 \n    0.038 \n    0.50 \n  \n  \n    I \n    35 \n    0.029 \n    0.37 \n  \n\n\n\nTable 21.4:  Sum of geodesic distances for each node in an undirected graph and its inverse. \n\n\nBut we want closeness, not farness, so the second column of Table 21.4 shows what happens when we divide one by the number in the second column. Now, node \\(E\\) has the largest score \\(CC^{CLOS}_E = 0.045\\) which is what we want.\nHowever, because we are dividing one by a relatively large number, we end up with a bunch of small decimal numbers as centrality scores, and like it happened with degree, this number is sensitive to how big the network is (the larger the network, the more likely there is to be really long short paths). So Freeman (1979) proposes a normalized version of closeness that takes into account network size. It is a variation of Equation 21.7:\n\\[\n  C_i^{CLOS} = \\frac{N-1}{\\sum_jd_{ij}}\n\\tag{21.8}\\]\nEquation 21.8 is the same as Equation 21.7, except that instead of dividing one by the sum of distances, we divide \\(N-1\\) by the sum of distances, where \\(N\\) is the order of the graph (the number of nodes). In this case, \\(N=14\\).\nNormalizing the sum of distances shown in the second column of Table 21.4 according to Equation 21.8, gives us the centrality scores shown in the fourth column of the table, under the heading “Normalized.” These scores range from zero to one, with one being the maximum possible closeness centrality score for that graph.\nThe normalized closeness centrality scores listed in the fourth column of Table 21.4 agree with our informal impressions. Node I comes out at the bottom (\\(CC_I^{CLOS} = 0.37\\)), showing it to be the one with the least closeness centrality, given the relatively large geodesic distances separating it from the other nodes in the graph. Node E (marked red in Figure 21.2) comes out on top (\\(CC_E^{CLOS} = 0.59\\)), given its relative geodesic proximity to other nodes in the graph.\nAs we will see later, having closeness centrality information for nodes in a graph can be useful. For instance, if Figure 21.2 was a social network, and we wanted to spread an innovation or a new product among the actors in the fastest amount of time, we would want to give it to node E first. Note however that if something bad (like a disease) was spreading across the network, then it would also be very bad if actor E got it first!4"
  },
  {
    "objectID": "lesson-sna-centrality.html#houston-we-have-a-problem",
    "href": "lesson-sna-centrality.html#houston-we-have-a-problem",
    "title": "21  Centrality",
    "section": "21.6 Houston, We Have a Problem",
    "text": "21.6 Houston, We Have a Problem\nSo far, so good. Closeness seems to be a great measure of node importance, giving us a sense of who can reach most others in a network in the most efficient way. However, what would happen if we tried to compute closeness centrality for a disconnected graph like the one shown in Figure Figure 11.1 (b)? Well, the shortest paths distance matrix for that graph looks like the one in Table 21.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\nInf\nInf\nInf\nInf\n\n\nB\n1\n0\n1\n1\n2\nInf\nInf\nInf\nInf\n\n\nC\n1\n1\n0\n1\n1\nInf\nInf\nInf\nInf\n\n\nD\n1\n1\n1\n0\n1\nInf\nInf\nInf\nInf\n\n\nE\n1\n2\n1\n1\n0\nInf\nInf\nInf\nInf\n\n\nF\nInf\nInf\nInf\nInf\nInf\n0\n1\n1\n1\n\n\nG\nInf\nInf\nInf\nInf\nInf\n1\n0\n1\n1\n\n\nH\nInf\nInf\nInf\nInf\nInf\n1\n1\n0\n1\n\n\nI\nInf\nInf\nInf\nInf\nInf\n1\n1\n1\n0\n\n\n\nTable 21.5: Geodesic distance matrix for an undirected, disconnected graph.\n\n\nNote that in Table 21.5, pairs of nodes that cannot reach one another in the disconnected graph, get a geodesic distance of “Inf” (infinity) in the respective cell of the geodesic distance matrix. This is a problem because when we compute the row sums of the geodesic distance matrix to try to calculate centrality according to Equation 21.7, we get the “numbers” shown in Table 21.6.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n\nTable 21.6: Row sums of a geodesic distance matrix from a disconnected graph.\n\n\nSo that’s a bummer since all the “numbers” in Table 21.6, are just infinity. Not to get too philosophical, but the problem is that when you add any number to “infinity,” the answer is, well, infinity.5 This means that closeness centrality is only defined for connected graphs. When it comes to disconnected graphs, we are out of luck.\nThankfully, there is a solution develoed by Beauchamp (1965). It consists of a modification of Equation 21.7 called harmonic closeness centrality. The formula goes as follows:\n\\[\n  C_i^{HARM} = \\frac{1}{N-1}\\sum_j\\frac{1}{d_{ij}}\n\\tag{21.9}\\]\nNow, this might seem like we just re-arranged the stuff in Equation 21.8, and indeed that’s what we did! But the re-arrangement matters a lot, because it changes the order in which we do the various arithmetic operations (Boldi and Vigna 2014).\nSo, in English, while Equation 21.8 says “first sum the geodesic distances for each node (to get the denominator), and then divide \\(N-1\\) by this sum,” Equation 21.9 says “first divide one by the geodesic distance, and then sum the result of all these divisions, and then multiply this sum by one over \\(N-1\\).\nOnce again, the philosophy of mathematical infinity kicks in here, since the main difference is that one divided by infinity is actually a real number: zero.6\nSo let’s check by taking every entry in Table 21.5 and dividing one by the number in each cell (except for the diagonals, which we don’t care about). The results are shown in Table 21.7.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1.0\n1\n1\n1.0\n0\n0\n0\n0\n\n\nB\n1\n0.0\n1\n1\n0.5\n0\n0\n0\n0\n\n\nC\n1\n1.0\n0\n1\n1.0\n0\n0\n0\n0\n\n\nD\n1\n1.0\n1\n0\n1.0\n0\n0\n0\n0\n\n\nE\n1\n0.5\n1\n1\n0.0\n0\n0\n0\n0\n\n\nF\n0\n0.0\n0\n0\n0.0\n0\n1\n1\n1\n\n\nG\n0\n0.0\n0\n0\n0.0\n1\n0\n1\n1\n\n\nH\n0\n0.0\n0\n0\n0.0\n1\n1\n0\n1\n\n\nI\n0\n0.0\n0\n0\n0.0\n1\n1\n1\n0\n\n\n\nTable 21.7: Reciprocal of the geodesic distance matrix for an undirected, disconnected graph.\n\n\nBeautiful! Now, instead of weird “Inf”s we have zeroes, which is great because we can add stuff to zero and get a real number back. We can then apply Equation 21.9 to the numbers in Table 21.7 (e.g., computing the sum of each row and then multiplying that by \\(\\frac{1}{N-1}\\)) to get the harmonic closeness centrality for each node in Figure 11.1 (b). These are shown in Table 21.8.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.5\n0.44\n0.5\n0.5\n0.44\n0.38\n0.38\n0.38\n0.38\n\n\n\nTable 21.8: Harmonic Closeness Centrality scores for nodes in a disconnected, undirected graph.\n\n\nGreat! Now we have a measure of closeness centrality we can apply to all kinds of graphs, whether they are connected or disconnected."
  },
  {
    "objectID": "lesson-sna-centrality.html#betweenness-centrality",
    "href": "lesson-sna-centrality.html#betweenness-centrality",
    "title": "21  Centrality",
    "section": "21.7 Betweenness Centrality",
    "text": "21.7 Betweenness Centrality\nRecall that in our discussion of shortest paths between pair of nodes in the lesson on indirect connections, we noted the importance of the inner nodes that intervene or mediate between a node that wants to reach another one. Nodes that stand in these brokerage or gatekeeper slots in the network, occupy an important position (Marsden 1983), and this is different from having a lot of contacts (like degree centrality), or being able to reach lots of other nodes by traversing relatively small distances (like closeness centrality). Instead, this is about being in-between the indirect communications of other nodes in graph. We can compute a centrality metric for each node called betweenness centrality that captures this idea Freeman (1980).\n\n\n\n\n\nFigure 21.3: An undirected graph showing the node with the maximum betweenness centrality (in red)\n\n\n\n\nFor instance, let’s say you were actor K in the network shown in Figure 21.2, and you wanted to know who is the person that you depend on the most to communicate with actor J. Here dependence means that you are forced to “go through them” if I wanted to reach N via a shortest path. One way K could figure this out is by listing every shortest path having them as the origin node and having N as the destination node. After you have this list, you can see which of other other nodes shows up as an inner node—an intermediary or gatekeeper—in those paths the most times.\nThis shortest path list would look like this:\n\n\\(\\{KH, HF, FJ\\}\\)\n\\(\\{KD, DF, FJ\\}\\)\n\\(\\{KH, HN, NJ\\}\\)\n\\(\\{KA, AC, CJ\\}\\)\n\\(\\{KA, AE, EJ\\}\\)\n\\(\\{KH, HE, EJ\\}\\)\n\nThere are six shortest paths of length three indirectly connecting actors K and J in Figure 21.2), with nodes \\(\\{A, C, D, E, F, H, N\\}\\) showing up as an inner node in at least one of those paths. To see which other actor in the network is the most frequent intermediary between J and K, we can create a list with the number of times each of these nodes shows up as an intermediary in this shortest path list. This would look like this:\n\n\n\n\n\n\n\nNode\nFreq.\nProp.\n\n\n\n\nA\n2\n0.33\n\n\nC\n1\n0.17\n\n\nD\n1\n0.17\n\n\nE\n2\n0.33\n\n\nF\n2\n0.33\n\n\nH\n3\n0.50\n\n\nN\n1\n0.17\n\n\n\nTable 21.9: Intermediaries between nodes J and K\n\n\nSo it looks like, looking at the second column of Table 21.9, that H is the other actor that J depends on the most to reach K. A better way to quantify this, is to actually look at the proportion of paths linking J and K that a particular other node (like H) shows up in. Let’s call this \\(p_{K(H)J}\\) which can be read as “the proportion of paths between K and J featuring H as an inner node.” This is shown in the third column of Table 21.9 We can write this in equation form like this:\n\\[\n  p_{K(H)J} = \\frac{g_{K(H)J}}{g_{KJ}} = \\frac{3}{6} = 0.5\n\\tag{21.10}\\]\nIn Equation 21.10, \\(g_{K(H)J}\\) is the number of shortest paths linking K and J featuring H as an inner node, and \\(g_{KJ}\\) is the total number of paths linking K and J. Freeman (1980) calls this measure the pair-dependency of actor K on actor H to reach a given node J. In this case, \\(g_{K(H)J} = 3\\) and \\(g_{KJ} = 6\\), which means that actor K depends on actor H for fifty percent of their shortest path access to J. Making H the actor in the network J depends on the most to be able to reach J.\nGeneralizing this approach, we can do the same for each triplet of actors i, j, and k in the network. This is the basis for calculating betweenness centrality. That is, we can count the number of times k stands on the shortest path between two other actors i and j. We can all this number \\(g_{i(k)j}\\). We can then divide it by the total number of shortest paths linking actors i and j in the network, which we refer by \\(g_{ij}\\). Remember that two actors can be indirectly linked by multiple shortest paths of the same length, and that we can figure out how many short paths links pairs of actors in the network using the shortest paths matrix.\nThis ratio, written \\(\\frac{g_{i(k)j}}{g_{ij}}\\) then gives us the proportion of shortest paths in the network that have i and j as the end nodes and that feature k as an intermediary inner node. This can range from zero (no shortest paths between i and j feature node k as an intermediary) to one (all the shortest paths between i and j feature node k as an intermediary).\nWe can then use the following equation to compute the average of this proportion for each node k across each pair of actors in the network i and j:\n\\[\n  C_k^{BET} = \\sum_i \\sum_j \\frac{g_{ikj}}{g_{ij}}\n\\tag{21.11}\\]\nComputing this quantity for the graph shown in Figure 21.3, yields the betweenness centrality scores shown in Table 21.10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nJ\nK\nL\nM\nN\nI\n\n\n\n\n5\n1.5\n3\n6.8\n11.4\n9.3\n6.3\n4.8\n10.8\n3.7\n16.4\n2.8\n5.3\n0\n\n\n\nTable 21.10: Betweenness centrality scores.\n\n\nThe numbers in the Table can be readily interpreted as percentages. Thus, the fact that node J has a a betweenness centrality score of 10.8 tells us that they stand in about 11% of the shortest paths between pairs of nodes in the graph. Interestingly, as shown in Figure 21.3, the node that ends up with the highest betweenness score is L (\\(C_L^{BET} = 16.4\\)), mostly due to the fact that node I, who has the lowest possible betweenness score of zero, depends on this node for access to every other actor in the network.\nNote also that two different nodes end up being ranked first on closeness and betweenness centrality in the same network (compare the red nodes in Figure 21.2 and Figure 21.3). This tells us that closeness and betweenness are analytically distinct measures of node position. One (closeness) gets at reachability, and the other (betweenness) gets at intermediation potential."
  },
  {
    "objectID": "lesson-sna-centrality.html#the-big-three-centralities-in-the-star-graph",
    "href": "lesson-sna-centrality.html#the-big-three-centralities-in-the-star-graph",
    "title": "21  Centrality",
    "section": "21.8 The Big Three Centralities in the Star Graph",
    "text": "21.8 The Big Three Centralities in the Star Graph\nDegree, Closeness, and Betweenness centralities have an interesting property that provides a conceptual connection between them (Freeman 1979). Consider the star graph shown in Figure 21.1 with central node A. The degree, closeness, and betweenness centralities of the different nodes are shown in Table 21.11).\nOf course, by definition, we know beforehand that the central node in a star graph has to have the highest degree, since the degree of peripheral nodes is fixed to one and the degree of the central node is always \\(n-1\\), where \\(n\\) is the graph order.\nHowever, note also that the central node has to have the highest closeness, since it is directed by a path of length one (and edge) to every peripheral node, but each peripheral node can only reach other peripheral nodes in the graph by a path of length two. They are farther away from other nodes than the central node.\nFinally, note that the central node in the star will also always have the highest betweenness because each of the paths of length two connecting every pair of peripheral nodes to one another has to include the central node. So it serves as the intermediary between any communication between peripheral nodes.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nDegree\n6.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nCloseness\n8.2\n4.5\n4.5\n4.5\n4.5\n4.5\n4.5\n\n\nBetwenness\n15.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\nTable 21.11: Centralities in a star graph of order 7.\n\n\nThe mathematical sociologist Linton Freeman (1979) thus thinks that the “big three” centrality measures are the big three precisely because they are maximized for the central node in a star graph."
  },
  {
    "objectID": "lesson-sna-centrality.html#references",
    "href": "lesson-sna-centrality.html#references",
    "title": "21  Centrality",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBavelas, Alex. 1950. “Communication Patterns in Task-Oriented Groups.” The Journal of the Acoustical Society of America 22 (6): 725–30.\n\n\nBeauchamp, Murray A. 1965. “An Improved Index of Centrality.” Behavioral Science 10 (2): 161–63.\n\n\nBoldi, Paolo, and Sebastiano Vigna. 2014. “Axioms for Centrality.” Internet Mathematics 10 (3-4): 222–62.\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nBorgatti, Stephen P, and Martin G Everett. 2006. “A Graph-Theoretic Perspective on Centrality.” Social Networks 28 (4): 466–84.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41.\n\n\n———. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39.\n\n\n———. 1980. “The Gatekeeper, Pair-Dependency and Structural Centrality.” Quality and Quantity 14 (4): 585–92.\n\n\nMarsden, Peter V. 1983. “Restricted Access in Networks and Models of Power.” American Journal of Sociology 88 (4): 686–717.\n\n\nNieminen, Juhani. 1974. “On the Centrality in a Graph.” Scandinavian Journal of Psychology 15 (1): 332–36.\n\n\nSabidussi, Gert. 1966. “The Centrality Index of a Graph.” Psychometrika 31 (4): 581–603."
  },
  {
    "objectID": "lesson-sna-status.html#status-as-indegree-centrality",
    "href": "lesson-sna-status.html#status-as-indegree-centrality",
    "title": "22  Status",
    "section": "22.1 Status as Indegree Centrality",
    "text": "22.1 Status as Indegree Centrality\nConsider a network which could be composed of asymmetric ties indicating some kind of positive regard or esteem that node i has for node j, represented by the directed graph in Figure 22.1, with result adjacency matrix shown in Table 22.1.\nThe directed edges could be “thinks the other person is great,’’ or”respects the other person” or “would take advice from tha person.” Note that all these relations are asymmetric you can think that person A is great, but that does not mean they think the same thing about you.\n\n\n\n\n\nFigure 22.1: A directed graph.\n\n\n\n\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 22.1:  Adjacency matrix corresponding to a directed graph. \n\n\nAn easy approach is to measure the status of each node by counting the number of direct nominations they get from others. This would be trying to measure the status of each node by using their indegree centrality. The results are shown in Table 22.2 (a). According to the table, nodes \\(F\\) and \\(I\\) are the highest status nodes in Figure 22.1 because they each receive five and four nominations respectively They are followed by nodes \\(\\{A, B, E, J\\}\\) who receive three nominations each. Node \\(C\\) is the lowest status, as no one thinks they are important.\nHowever, the problem with using the number of incoming nominations as a measure of status is that the indegree centrality only measures the number of ties that are incoming to each node, but it does not differentiate between who sends each tie. Every nomination counts as the same.\nBut as we noted, the whole point of the idea of status is that you gain status when you receive ties from high-status others, and their status is established by their receiving ties from high status others, and so forth. So indegree centrality won’t do as a measure of status in social networks, if we aim to capture the full idea behind the concept."
  },
  {
    "objectID": "lesson-sna-status.html#using-exogeneous-status-information",
    "href": "lesson-sna-status.html#using-exogeneous-status-information",
    "title": "22  Status",
    "section": "22.2 Using Exogeneous Status Information",
    "text": "22.2 Using Exogeneous Status Information\nAnother possibility is that we check some measure of status that comes from outside the network (the fancy word for this is exogenous). This could be for instance, the position of each node in the organizational chart, with ten indicating a top position and zero indicating an entry-level position. We could record this information using a \\(1 \\times 12\\) column vector where the exogenous status of each node i is given by each entry \\(\\mathbf{b}_i\\). Such an exogenous status score vector is shown in Table 22.2. In the table, larger numbers indicate higher status.\n\n\n\n\n\nTable 22.2: Example of estimating status using exogeneous scores for nodes.\n\n\n\n\n(a) Indegree Centrality \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    3 \n    3 \n    0 \n    1 \n    3 \n    5 \n    1 \n    2 \n    4 \n    3 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Exogenous Status Scores \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    2 \n    5 \n    4 \n    6 \n    8 \n    8 \n    2 \n    4 \n    9 \n    4 \n    9 \n    6 \n  \n\n\n\n\n\n\n\n\n(c) Status Scores Based on Exogeneous Information \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    17 \n    14 \n    0 \n    8 \n    22 \n    24 \n    5 \n    10 \n    17 \n    14 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n\n(d) Status Scores Based on Endogeous Information (In-degree) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    6 \n    11 \n    0 \n    5 \n    12 \n    7 \n    3 \n    1 \n    5 \n    8 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(e) Status Scores Based on All Indirect Paths (Katz) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    12 \n    25 \n    0 \n    8 \n    27 \n    17 \n    12 \n    2 \n    14 \n    19 \n    2 \n    4 \n  \n\n\n\n\n\n\n\n\n(f) Status Scores Based on All Indirect Paths and Exogeneous Status Information (Hubbell) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    74 \n    143 \n    4 \n    56 \n    160 \n    111 \n    66 \n    20 \n    84 \n    119 \n    23 \n    32 \n  \n\n\n\n\n\n\nNow the status score for each person \\(\\mathbf{s}\\) can be determined by taking each of their incoming nominations and weighting them by the exogenous status score of each of the other people, so that nominations from lower status nodes (like node \\(K\\) in Table 22.2 (b)) count for more than those coming from higher status nodes (like node \\(A\\) in Table 22.2 (b)). To calculate the status of each node we add up the status of each of the nodes that point to it.\nHow do we do this? Recall from Chapter 15, that it is always possible to multiply a square matrix times a column vector of the same length as the matrix’s row and column dimensions and that the result is always another column vector of the same length as the original.\nAccordingly, we can get each person’s weighted status score by taking network’s adjacency matrix \\(\\mathbf{A}\\) and multiplying it by the (in this case, \\(12 \\times 1\\)) column vector of exogenous status scores \\(\\mathbf{b}\\). However, because we want to add up the status scores of the nodes that point to a given node, what we want is the product of the transpose of the network adjacency matrix times the vector of status scores:\n\\[\n  \\mathbf{s}^{ex} = \\mathbf{A}' \\mathbf{b}\n\\tag{22.1}\\]\nWhen we do that, we end up with the status scores shown in Table 22.2 (c).\nAs we can see, the status order is a bit different once we take into account the exogenous status of the other people who nominate each node. Yes, node \\(F\\) is still the highest ranked node, and node \\(C\\) is the lowest ranked. However, node \\(I\\) is no longer the second highest status node, that honor now goes to node \\(E\\). The reason is that while \\(I\\) has a larger indegree than \\(E\\), node \\(I\\)’s in-neighbors, as shown in Figure Figure 22.1 and Table 22.1, \\(N_{in}(I) = \\{A, C, G, K\\}\\) are relatively low status (except for \\(K\\)). \\(E\\)’s in-neighbors, by way of contrast, \\(N_{in}(E) = \\{B, F, I\\}\\) are all high to mid-status."
  },
  {
    "objectID": "lesson-sna-status.html#using-endogeneous-network-information",
    "href": "lesson-sna-status.html#using-endogeneous-network-information",
    "title": "22  Status",
    "section": "22.3 Using Endogeneous Network Information",
    "text": "22.3 Using Endogeneous Network Information\nIt turns out, that in many cases, we don’t have exogenous status information on each node in the network to rely on. In that case, we must rely on endogenous network information to determine the status of each of the other nodes.\nOne approach is just to use original indegree centrality scores shown in Table 22.2 (a) as the status of each other the nodes. We can then say that a node is high status if it is pointed to by other nodes who are also pointed to by many other nodes. Conversely, a node is low status if it is pointed to by other nodes that are not pointed to by many other nodes.\n\\[\n  s^{en} = \\mathbf{A}'d_{in}\n\\tag{22.2}\\]\nWhere \\(\\mathbf{d}_{in}\\) is the \\(12 \\times 1\\) column vector of indegree centralities shown in Table 22.2 (a) and \\(\\mathbf{A}'\\) is the transpose of the network’s adjacency matrix shown in Table 22.1. The results are shown in Table 22.2 (d). As we can see, considering only endogenous network information gives us a completely different picture of the status order than using exogeneous information. Now \\(E\\) is definitely the highest status node, and \\(F\\) which was the highest status node based on exogenous considerations drops to fourth place, behind \\(B\\) and \\(J\\).\nLooking at Figure 22.1, we can see why this happened. Take the set of \\(F\\)’s in-neighbors \\(\\{C, D, H, J, L\\}\\). It is easy to see from Table 22.2 (a), that most of these nodes also have low indegree centrality (except for \\(J\\)). So even though \\(F\\) has five nodes pointing toward them, all of them are not very high-status people. By comparison \\(E\\) only has three in-neighbors \\(\\{B, F, I\\}\\), and all three are towards the top in terms of in-degree centrality. \\(E\\) has higher status than \\(F\\) according to \\(s^{en}\\) because the people that choose \\(E\\) are also chosen by many others, which is exactly what we want in a status measure.\nWhile \\(s^{en}\\) seems like a good measure of status, it does have one big drawback. It only counts direct connections. However, it is possible that you get status not just from the nodes that point directly toward you, but from the nodes that point to those other people even if they don’t point toward you (e.g., two step connections), and perhaps from the nodes that point to those two-step alters, and the ones that point to those three-steps away, and so forth. A good status measure should be able to take into account the status of your indirect connections in computing your own staus score. How do we do this?"
  },
  {
    "objectID": "lesson-sna-status.html#a-mathy-interlude",
    "href": "lesson-sna-status.html#a-mathy-interlude",
    "title": "22  Status",
    "section": "22.4 A Mathy Interlude",
    "text": "22.4 A Mathy Interlude\nConsider any number \\(x\\), where \\(x < |1|\\) (remember that \\(|a|\\) means “the absolute value of \\(a\\)), and thus \\(-1 > x < 1\\) (this reads”\\(x\\) is between -1 and +1”). Thus, \\(x\\) can be 0.43, or -0.62, or whatever in that interval. Recall that when we take a number in this interval and we raise it to a power, we end up with a smaller number than we begin with. The bigger the power, the smaller the result. For instance, take \\(x = 0.75\\). For instance:\n\\[\n  x^2 = 0.75^2 = 0.562\n\\] \\[\n  x^5 = 0.75^5 = 0.237\n\\] \\[\n  x^{10} = 0.75^{10} = 0.056\n\\] Because mathematicians are strange people, they like to say things like, “since the result gets closer to zero the bigger the power, then that means that when I raise the number to an infinite power, then the result should approach zero.” In equation terms:\n\\[\nx^{\\infty} = 0.75^{\\infty} \\approx 0\n\\] Since raising a number between -1 and 1 to a big power gets you closer to zero the bigger the power, mathematicians then go on to wonder whether adding up the powers, gets to the point where the sum does not grow anymore. For instance, what is the end point of:\n\\[\n  1 + x + x^2 + x^3 + x^4 + x^5 \\ldots + x^{\\infty}\n\\tag{22.3}\\]\nThe idea is that as we move to the right and add a number between -1 and 1 raised to a bigger and bigger power, we add a smaller and smaller number, such that as we approach infinity, we end up adding such an infinitesimally small number that it might as well be zero. For instance, table Table 22.3 shows the result of raising \\(x\\) to the powers between 2 and 20 for \\(x = 0.75\\).\n\n\n\n\n\n \n  \n    Power \n    Result \n  \n \n\n  \n    2 \n    0.562 \n  \n  \n    3 \n    0.422 \n  \n  \n    4 \n    0.316 \n  \n  \n    5 \n    0.237 \n  \n  \n    6 \n    0.178 \n  \n  \n    7 \n    0.133 \n  \n  \n    8 \n    0.100 \n  \n  \n    9 \n    0.075 \n  \n  \n    10 \n    0.056 \n  \n  \n    11 \n    0.042 \n  \n  \n    12 \n    0.032 \n  \n  \n    13 \n    0.024 \n  \n  \n    14 \n    0.018 \n  \n  \n    15 \n    0.013 \n  \n  \n    16 \n    0.010 \n  \n  \n    17 \n    0.008 \n  \n  \n    18 \n    0.006 \n  \n  \n    19 \n    0.004 \n  \n  \n    20 \n    0.003 \n  \n\n\n\nTable 22.3:  Incresing powers of the number 0.75. \n\n\nNow let us sum \\(1 + 0.75\\) and add the result to the sum of all the numbers in the third column of Table 22.3, to get an approximation to the sum shown in Equation 22.3. The result is 3.99.\nIn turns out, by some bit of mathematical magic, that this number is pretty close to: \\[\n  (1-0.75)^{-1} = \\frac{1}{1 - 0.75} = 4\n\\]\nAs we noted, as the power that we raise the number to approaches \\(\\infty\\), we will be adding a number that is closer to zero, so the result of the infinity sum when \\(x = 0.75\\) will converge towards:\n\\[  \n  1 + x + x^2 + x^3 + x^4 + x^5 \\ldots + x^{\\infty} =\n\\] \\[\n  1 + 0.75 + 0.75^2 + 0.75^3 + 0.75^4 + 0.75^5 \\ldots + 0.75^{\\infty} \\approx 4\n\\] In general terms, for any number \\(x\\) the sum of the following infinite series converges to:\n\\[\n  1 + x + x^2 + x^3 + x^4 + x^5 +...x^\\infty =\n\\] \\[\n(1-x)^{-1} = \\frac{1}{1 - x}\n\\tag{22.4}\\]\nWhy is this bit of math important? We will see next!"
  },
  {
    "objectID": "lesson-sna-status.html#katz-status-score",
    "href": "lesson-sna-status.html#katz-status-score",
    "title": "22  Status",
    "section": "22.5 Katz Status Score",
    "text": "22.5 Katz Status Score\nIn the mid-twentieth century, the great statistician Leo Katz set out to develop a measure of status in social networks that took into account indirect connections. He first observed that the new matrix that results from taking the original adjacency matrix and raising it to a power (using matrix multiplication) has a clear interpretation, as we saw in ?sec-appmat. For instance, if we raise the adjacency matrix to the third power (\\(\\mathbf{A}^3\\)) the resulting matrix will contain, as cell entries, all the walks of \\(l = 3\\) that have node \\(i\\) as the starting node and node \\(j\\) as the destination node, the same goes for any number \\(\\mathbf{A}^k\\).\nKatz saw this as a way to incorporate indirect connections to construct a measure of status using only endogenous information. The idea would be to say that your total status is the sum of number of other people who choose you (or think you are great, or a great source or advice or whatever). However, among the people that choose you the ones that are chosen by many others should count for more. Those are people who are two-steps away from you. But the same should apply to the people who choose those others (people three-steps away from you). Overall, you should get more status from people who choose the people who choose the people, who choose the people…who choose you and you should get more status from the more people who are most likely to be chosen by those others, across any number of steps.\nTo accomplish this, we need to construct a new matrix \\(A^*\\) that incorporates all this information about people’s one step, two-step, three-step, connections to others, then sum rows of that matrix. The resulting vector (\\(s^{Katz}_i\\)) would contain the desired score for each node \\(i\\).\nOne way to proceed would be:\n\\[\nA^* = A + A^2 + A^3 + A5 + \\ldots A^{\\infty}\n\\tag{22.5}\\]\nThere are a couple of problems here. First this sum keeps getting bigger and bigger and it does not have a natural end point (keeps going forever). This is because it is counting the direct connections (\\(A\\)) as much as the very indirect connections, like \\(A^5\\), or the number of indirect links connecting you to others five steps away.\nWhat we want is a way to count the first-step links the most, and then discount the longer-step links, with the discount getting larger the longer the chain. So that three-steps links count for less than two-steps links but count for more than four step links to others and so forth.\nKatz’s great idea is to multiply the original adjacency matrix and its powers by a number \\(\\alpha\\) that was larger than zero, but less than one. This leads to:\n\\[\nA^* = \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty}\n\\tag{22.6}\\]\nNow the difference between Equation 22.5 and Equation 22.6, is that as we saw before (see Equation 22.4), while the sum in Equation 22.5 keeps getting bigger and bigger (the technical term is “diverges”), the one in Equation 22.6, will stop growing, because raising a number less than one and more than zero to a big power will result in a tiny number. The sum will converge rather than diverge.\nMoreover, Katz knew his math, and noted that there is a version of {Equation 22.4} that applies to matrices. This is:\n\\[\nA^* = \\alpha A(I + \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty})\n\\]\n\\[\nA^* = \\alpha A(I-A)^{-1}\n\\tag{22.7}\\]\nWhere \\(I\\) refers to the “identity matrix.” This is simply a matrix with the same dimensions as \\(A\\), but containing ones along the diagonal and zeros everywhere. It functions just like the number “\\(1\\)” does in regular (scalar) multiplication. Thus, for an adjacency matrix \\(A\\) and its respective identity matrix \\(I\\) of the same dimensions:\n\\[\nA \\times I = A\n\\] \\[\nI \\times A = A\n\\] \\[\nA \\times A^{-1} = I\n\\] \\[\nA^{-1} \\times A = I\n\\]\nWhat Katz (1953) proposed is that we can turn the infinite sum part of Equation 22.7 (\\(I + \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty})\\) into just \\((I-A)^{-1}\\) following the principle outlined earlier in Equation 22.4. Just like the endless sum of squares of a number \\(x\\) between \\(-1\\) and and \\(1\\) just turns into just \\(\\frac{1}{1-x}\\), the endless sums of a matrix containing numbers between \\(-1\\) and \\(1\\) as its entries \\(\\alpha A\\) turns into \\((I-\\alpha A)^{-1}\\), with \\(I\\) playing the role of \\(1\\) and raising \\(I-\\alpha A\\) to the power of \\(-1\\) playing the role of taking the reciprocal.1\nKatz showed that the new matrix, \\(A^* = \\alpha A(I-\\alpha A)^{-1}\\) contains all the information we need, as it condenses the sums of all the status that a persons gets from all their connections both direct and indirect, regardless of how indirect, and it weighs each person’s contribution to each other’s persons status by the status of those people (which is calculated in the same way). Math magic to the rescue!\nLet’s see how it works, step by step:\n\nFirst we create the \\(12 \\times 12\\) identity matrix \\(I_{12 \\times 12}\\), show in Table 22.4 (a). As noted, this matrix has twelve ones across the diagonals and zeroes everywhere else. Then we choose a value for alpha. There are obscure mathematical reasons for why this value cannot be too big (depending on \\(A\\)), but for this example \\(\\alpha = 0.45\\) will work.\nSecond, we multiply \\(\\alpha\\) times the original adjacency matrix (shown in Figure 22.1) to get \\(\\alpha A\\). This new matrix is shown in Table 22.4 (b). In the new \\(\\alpha A\\) matrix, for every cell in which there is one in \\(A\\), the value 0.45 now appears in \\(\\alpha A\\).\nThird, we subtract \\(I\\) from \\(\\alpha A\\) , to get \\(I-\\alpha A\\). This new matrix is shown in Table 22.4 (c). Note that what this does is to add ones to the diagonals of \\(\\alpha A\\) and change all the other non-zero entries from positive to negative.\nFourth, we find the matrix that equals the reciprocal of \\(I-\\alpha A\\) (also called the matrix inverse of \\(I-\\alpha A\\)) to get \\((I-\\alpha A)^{-1}\\). The matrix inverse is somewhat involved to calculate for larger matrices like \\(I-\\alpha A\\), so, for now, chalk the numbers in Table 22.4 (d) up to math magic. Essentially you are trying to find a new matrix \\(W\\) such that when you multiply it by \\((I-\\alpha A)\\) you get \\(I\\) as the result.\nFifth we multiply \\(\\alpha A\\) (shown in Table 22.4 (b)) times the new matrix \\((I-\\alpha A)^{-1}\\) (shown in Table 22.4 (d)) to get the answer to \\(\\alpha A(I-\\alpha A)^{-1}\\). This new matrix, called the Katz status similarity matrix is shown in Table 22.4 (e).2 In this matrix, the larger the number in the cell, the more node \\(i\\) is connected to node \\(j\\) via indirect connections.\nFinally, we compute the column sums of the Katz status similarity matrix. In equation form:\n\n\\[\ns^{katz}_i = \\sum_jA^*_{ij}\n\\tag{22.8}\\]\nWith \\(A*\\) computed using Equation 22.7. The resulting scores are shown in Table 22.2 (e) for each node of Figure 22.1.\nAs we can see, according to the Katz’s status score, node \\(E\\) is still the highest status node in the network. They are followed by nodes \\(B\\) and \\(J\\), closely agreeing with the endogenous status scores obtained using the in-degree (Table 22.2 (d)). This makes sense, since the Katz scores can be seen as a generalization of the endogenous degree measure, with the latter taken into account only the first step links, and Katz’s taking into account all the indirect links regardless of lengths (but counting the really long ones very little, and counting the first step ones the most). In this way, the Katz approach is the most comprehensive way to compute the status of nodes using only endogenous network information.\n\n\nTable 22.4: Example of estimating status in social networks using exogeneous and endogeneous information for nodes.\n\n\n\n\n(a) Twelve by twelve identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Adjacency matrix multiplied by alpha \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0.00 \n    0.45 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    B \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n  \n  \n    E \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    F \n    0.00 \n    0.45 \n    0 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    H \n    0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    J \n    0.45 \n    0.45 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    K \n    0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    L \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n  \n\n\n\n\n\n\n\n\n(c) Adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1.00 \n    -0.45 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    B \n    0.00 \n    1.00 \n    0 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    1 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n  \n  \n    E \n    0.00 \n    0.00 \n    0 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    F \n    0.00 \n    -0.45 \n    0 \n    -0.45 \n    -0.45 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    H \n    -0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    J \n    -0.45 \n    -0.45 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n  \n  \n    K \n    -0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    1.00 \n    0.00 \n  \n  \n    L \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    -0.45 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(d) Inverse of adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2.0 \n    2.5 \n    0 \n    0.6 \n    2.4 \n    1.4 \n    1.1 \n    0.1 \n    1.5 \n    2.1 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    1.8 \n    0 \n    0.2 \n    1.3 \n    0.5 \n    0.8 \n    0.0 \n    0.6 \n    0.8 \n    0.0 \n    0.1 \n  \n  \n    C \n    1.6 \n    3.1 \n    1 \n    1.2 \n    3.5 \n    2.8 \n    1.4 \n    0.7 \n    1.9 \n    2.6 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.4 \n    2.8 \n    0 \n    2.2 \n    3.0 \n    2.6 \n    1.2 \n    0.4 \n    1.4 \n    2.2 \n    0.4 \n    1.0 \n  \n  \n    E \n    0.7 \n    1.4 \n    0 \n    0.4 \n    2.3 \n    1.0 \n    0.6 \n    0.1 \n    0.6 \n    1.4 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.1 \n    2.7 \n    0 \n    1.3 \n    3.0 \n    2.8 \n    1.2 \n    0.3 \n    1.2 \n    2.0 \n    0.3 \n    0.6 \n  \n  \n    G \n    0.1 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    1.1 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.1 \n    3.7 \n    0 \n    1.3 \n    3.8 \n    2.9 \n    1.7 \n    1.3 \n    1.8 \n    3.2 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.3 \n    0.6 \n    0 \n    0.2 \n    1.1 \n    0.4 \n    0.3 \n    0.0 \n    1.3 \n    0.6 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.6 \n    3.1 \n    0 \n    1.0 \n    3.0 \n    2.1 \n    1.4 \n    0.2 \n    1.4 \n    3.2 \n    0.2 \n    0.4 \n  \n  \n    K \n    1.1 \n    1.4 \n    0 \n    0.4 \n    1.6 \n    0.8 \n    0.6 \n    0.1 \n    1.2 \n    1.2 \n    1.1 \n    0.2 \n  \n  \n    L \n    1.9 \n    3.5 \n    0 \n    1.3 \n    3.7 \n    2.9 \n    1.6 \n    0.7 \n    1.9 \n    2.9 \n    0.7 \n    1.6 \n  \n\n\n\n\n\n\n\n\n(e) Katz similarity matrix (original adjacency matrix multiplied by alpha times the inverse of the adjacency matrix multiplied by alpha subtracted from the identity matrix) \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0.0 \n    2.5 \n    0 \n    0.6 \n    2.4 \n    1.4 \n    1.1 \n    0.1 \n    1.5 \n    2.1 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    0.0 \n    0 \n    0.2 \n    1.3 \n    0.5 \n    0.8 \n    0.0 \n    0.5 \n    0.8 \n    0.0 \n    0.1 \n  \n  \n    C \n    1.6 \n    3.1 \n    0 \n    1.3 \n    3.6 \n    2.7 \n    1.4 \n    0.7 \n    1.9 \n    2.6 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.4 \n    2.8 \n    0 \n    0.0 \n    3.0 \n    2.6 \n    1.3 \n    0.4 \n    1.4 \n    2.2 \n    0.4 \n    1.0 \n  \n  \n    E \n    0.7 \n    1.4 \n    0 \n    0.4 \n    0.0 \n    0.9 \n    0.6 \n    0.1 \n    0.6 \n    1.4 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.1 \n    2.7 \n    0 \n    1.3 \n    3.0 \n    0.0 \n    1.2 \n    0.2 \n    1.2 \n    2.0 \n    0.2 \n    0.6 \n  \n  \n    G \n    0.1 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    0.0 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.1 \n    3.7 \n    0 \n    1.3 \n    3.8 \n    2.8 \n    1.7 \n    0.0 \n    1.8 \n    3.3 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.3 \n    0.6 \n    0 \n    0.2 \n    1.0 \n    0.4 \n    0.3 \n    0.0 \n    0.0 \n    0.6 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.6 \n    3.2 \n    0 \n    0.9 \n    3.0 \n    2.1 \n    1.4 \n    0.2 \n    1.5 \n    0.0 \n    0.2 \n    0.5 \n  \n  \n    K \n    1.0 \n    1.4 \n    0 \n    0.4 \n    1.6 \n    0.8 \n    0.6 \n    0.0 \n    1.3 \n    1.2 \n    0.0 \n    0.2 \n  \n  \n    L \n    1.9 \n    3.5 \n    0 \n    1.4 \n    3.8 \n    2.9 \n    1.6 \n    0.8 \n    1.9 \n    2.9 \n    0.8 \n    0.0"
  },
  {
    "objectID": "lesson-sna-status.html#hubbells-tweak-on-katzs-score",
    "href": "lesson-sna-status.html#hubbells-tweak-on-katzs-score",
    "title": "22  Status",
    "section": "22.6 Hubbell’s Tweak on Katz’s Score",
    "text": "22.6 Hubbell’s Tweak on Katz’s Score\nSo far, we have discussed two main ways to measure the status of node in a social network, both based on the similar principle that people gain status from being (directly or indirectly) connected to high-status others (and get less status from being directly or indirectly connected to low status others). There are two ways to get a sense of the status of others. On the exogeneous approach, we use some kind of prior ranking or knowledge (see Table 22.2 (b)) on the endogenous approach, we use only information on network connectivity (in-degree or the Katz approach). What if there was a way to combine both approaches and get the best of both worlds?\nThis is exactly what was proposed by Hubbell (1965). It revolves around a relatively small tweak on Katz’s approach. The trick is to take the part of Equation 22.7 that computes the endogenous status based on all indirect links to others (\\((I-\\alpha A)^{-1}\\)) and multiply not by \\(\\alpha A\\), but by the external vector of status \\(b^T\\).\nIn equation form:\n\\[\ns^{hubbell} = A'^*\\mathbf{b}\n\\tag{22.9}\\]\nWhere \\(\\mathbf{b}\\) is the column vector containing the exogenous status information shown in Table 22.2 (b), and \\(A'^*\\). Is the transpose of the matrix \\(A^*\\) which is given by:\n\\[\nA^* = (I - \\alpha A)^{-1}\n\\] The resulting Hubbell status scores are shown in Table 22.2 (f) for each node in Figure 22.1. As we can see, incorporating both endogenous and exogenous status information changes the picture, creating more separation between high and low status nodes.\nNow, node \\(E\\) is the indisputable highest status node in the network, followed, at a distant second and third place, by nodes \\(B\\) and \\(F\\). Combining both endogenous and exogenous sources of information does reveal a deeper status inequalities in social networks."
  },
  {
    "objectID": "lesson-sna-status.html#references",
    "href": "lesson-sna-status.html#references",
    "title": "22  Status",
    "section": "References",
    "text": "References\n\n\n\n\nHubbell, Charles H. 1965. “An Input-Output Approach to Clique Identification.” Sociometry, 377–99.\n\n\nKatz, Leo. 1953. “A New Status Index Derived from Sociometric Analysis.” Psychometrika 18 (1): 39–43.\n\n\nVigna, Sebastiano. 2016. “Spectral Ranking.” Network Science 4 (4): 433–45."
  }
]