[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Networks",
    "section": "",
    "text": "Welcome\nThis is an introductory textbook on social networks to be used in conjunction with an undergraduate lecture class on social networks (SOCIOL 111) taught in the department of sociology at UCLA."
  },
  {
    "objectID": "lesson-what-are-networks.html#what-is-a-network",
    "href": "lesson-what-are-networks.html#what-is-a-network",
    "title": "1  What Are Networks?",
    "section": "1.1 What is a Network?",
    "text": "1.1 What is a Network?\nSo what is a network? Minimally, a network is composed of a set of units or entities. These are some times called nodes or vertices, however what makes these units into a network is the fact that at least some pairs of them are joined by a set of links or ties. These are also sometimes called edges. So a network is essentially a set of nodes some of which are linked together, usually because the units interact in some ways, and the come to be connected via some kind of relationship. In social networks, the nodes can be people and the connections can be any type of social tie (e.g., friendship, enmity, co-working), whether positive or negative, between them.\n\n\n\n\n\nFigure 1.1: Point and line plot of a network.\n\n\n\n\nIn future lessons, we will discuss the different types of ties that can exist in social networks as well as the major social network theories that have been developed by sociologists, anthropologists, and organization theorists to explain why these types of ties exist, how they work, and what benefits (or drawbacks) they have for people and organizations.\nIn a point and line plot such as the one shown in Figure 1.1, networks are represented as pictures. These kinds of pictures are also sometimes called sociograms or network diagrams (the people who don’t like them call them “hairballs”). The convention in these kind of pictures of networks is that the nodes (e.g., people) are drawn as circles (or some other polygon such as a triangles or squares) and the connections, ties, or links between people are drawn as lines or sometimes, if some kind of direction is implied, as arrows.\nThis way of representing networks is occasionally called a “graph”, although we will see that the idea of a graph is a little more abstract than just a picture. So in this class we will make a strong differentiation between pictorial network representations, and the mathematical concept of graph. When referring to the former, we will use the term plot or network diagram, reserving the term graph for the abstract mathematical object. In future lessons, we will get into more details about pictorial (and non-pictorial) ways of thinking about networks and how they connect to other ways of representing them."
  },
  {
    "objectID": "lesson-what-are-networks.html#types-of-networks",
    "href": "lesson-what-are-networks.html#types-of-networks",
    "title": "1  What Are Networks?",
    "section": "1.2 Types of Networks",
    "text": "1.2 Types of Networks\n\n\n\n\n\nFigure 1.2: Types of Networked Systems.\n\n\n\nGiven the very broad definition of networks given in preceding, it should start to dawn on you why people think that networks are everywhere. And, come to think of it, they kind of are! Any system of interacting parts or entities can be depicted and analyzed as a network. That is why the idea of a network cuts across the information (e.g., networks of words in a text), social (networks of students in a school), physical (networks of servers on the internet), biological (networks of neurons in the brain) sciences, and psychological (networks of symptoms of mental disorders). The diagram in Figure 1.2 gives you an idea of the different types of networked systems that exist in the world.\n\n\n\nFigure 1.3: Network of Characters in Game of Thrones. Image created by Marcos Martins Marchetti via https://www.kaggle.com/code/mmmarchetti/game-of-thrones-network-analysis\n\n\nFor instance, some social networks do not even have to be made up of real people! In fictional worlds, such as the Game of Thrones HBO series created from the books written by George R. R. Martin, some characters meet and get to know one another, but others never meet (or more likely in this show, (spoiler!) die before they get to meet). So we can construct a social network made up of acquaintance relations between the characters (who ended up meeting whom) such as the one depicted in Figure 1.3. This is a social network because the links are composed of a psychological or emotional relation between people. In this case the relation “knowing somebody” in the GoT world is an example of a symmetric tie between people (we will see a symmetric tie is in #sec-ties). In Figure 1.3, the size of the name of each character is proportional to the centrality of the actor in the network. As we will see in Chapter 20, centrality is an index of how important a given actor is in a network. So, even if you’ve never watched the show, you’ll know that Tyrion, Daenerys, and Jon are pretty important people in this show!\n\n\n\nFigure 1.4: The U.S. Airport Network. Image created by Jose M Salln via https://jmsallan.netlify.app/blog/plotting-us-airline-airport-networks/\n\n\nOther networks are composed of links between physical technological systems not people. Take, for instance, the various airports (large and small, international and regional) in the United States. Everyday, some number of flights departs from one airport and lands on another. This means that airports in the US are linked via directed (e.g., from/to) ties in a socio-Technical network. This network is “socio-technical because it involves both social and technological links built from the flights that go from airport to the next (see Figure 1.4). The network is social because it is composed of people traveling. The network is also technological because its links are made possible by a complex web of air-flight technology, including planes, radars, in-flight computer systems and so forth. As we will in Chapter 8, the directed links between airports are an example of an asymmetric tie; some airports fly a lot to other airports (e.g., Small Regional to LAX), but other airports (e.g., LAX) only schedule flights to other big airports).\n\n\n\nFigure 1.5: Network of Connectivity Between Brain Regions. Image from https://www.cam.ac.uk/research/news/wiring-the-brain\n\n\nAll of us carry around a very complex and staggeringly large network, composed of millions of nodes and billions of connections. The network is called the brain (see Figure 1.5). The nodes are a special type of biological cell called a neuron. Neurons are special because they have these filaments called dendrites and this long body called an axon. The axon of one neuron links up to the dendrites of another one, generating a large-scale complex network that allows you to breath, eat, drink, think, see, smell and read these pages. The brain is a biological network, because the nodes are biological units (neurons) as are the connections. Other biological networks include ecosystems were the nodes are species and the links are various types of relations between, some antagonistic (predator/prey) and others mutualistic.\n\n\n\nFigure 1.6: Network of Symptoms in the Diagnostic and Statistical Manual of Mental Disorders. Image from https://doi.org/10.1371/journal.pone.0137621\n\n\nThe nodes in a network can also be composed of psychological entities like attitudes, feelings, norms, types of thoughts, or emotions. In these psychological networks two entities are connected if they tend to occur in the same people. For instance, people who are have social anxiety, also experience loneliness, which is linked to depression. Figure 1.6 shows just such a network of symptoms, taken from [Diagnostic and Statistical Manual of Mental Disorders] created by psychological scientists who study psychopathology. In these network, the symptoms that cluster together (shown as nodes of different color) come to define specific types of mental disorders, like “Social Phobia” or “Panic Disorders.”"
  },
  {
    "objectID": "lesson-what-are-networks.html#what-is-a-social-network",
    "href": "lesson-what-are-networks.html#what-is-a-social-network",
    "title": "1  What Are Networks?",
    "section": "1.3 What is A Social Network?",
    "text": "1.3 What is A Social Network?\nAs you can see from the tree chart shown in Figure 1.2, only a subset of networks in the world as social networks. The key difference between social networks and other networks is that social networks have to involve people (or groups of people) and their perceptions, thoughts, interactions, and behaviors. Sometimes interactions between people are mediated by technologies. For instance, people can connect to one another by texting on their phone or traveling by plane, in which case the differentiation between what is a technological and a social network becomes a matter of degree.\nIn this class we will deal with networks that are closer to the “purely social” end of the scale: Those involving people, their perceptions, interactions, sentiments, exchanges, memberships, and relations. The primary perspective that we will take is that of social network analysis as developed in the discipline of sociology since the 1970s.\n\n\n\n\n\nFigure 1.7: Social Science Disciplines that Contribute to and Use Social Network Analysis.\n\n\n\nAs noted, the inter-disciplinary field in charge of studying social networks is called social network analysis (SNA), and is composed of insights from a variety of other social science disciplines such as sociology, anthropology, psychology, communication, and others (see Figure 1.7). SNA in its turn, is part of an even larger interdisciplinary field in charge of studying all types of networks called network science which includes work in physics, computer science, data science, biology, engineering, mathematics, and other fields. The relationship between these different scientific fields is depicted in Figure 1.8.\n\n\n\n\n\nFigure 1.8: The Network of Network Science."
  },
  {
    "objectID": "lesson-what-are-networks.html#the-two-faces-of-social-network-analysis",
    "href": "lesson-what-are-networks.html#the-two-faces-of-social-network-analysis",
    "title": "1  What Are Networks?",
    "section": "1.4 The Two Faces of Social Network Analysis",
    "text": "1.4 The Two Faces of Social Network Analysis\nSocial network analysis has two broad aspects. One, generally referred to as network theory is about figuring out how networks work and what networks do to and for people. In essence, social network theories are general statements about how people behave in networks and how networks themselves “behave”“; that is where network relations come from, what they do, and what consequences they have for the people involved.\nFor instance, the idea of social capital that is, that the connections that you have to others can bring you certain types of benefits, is part of network theory. In fact, as we will see later, a good chunk of network theory (but not all!), such as the theory of structural holes, or the strength of weak ties theory, can be thought of as theories of social capital (Borgatti and Halgin 2011). Other types of network theory deal with how networks of sentiment relations (e.g. likes and dislikes) form, while other tell us about how things flow through networks.\n\n\n\n\n\nFigure 1.9: The two faces of social network analysis.\n\n\n\nAnother branch of social network analysis deals with how to measure various network properties. This branch of social network analysis, called network measurement links social network concepts to some type of mathematical or quantitative representation. Since this branch of network analysis deals with measurement, it is where mathematics and other forms of quantitative representation of networks (such as matrices) come in handy.\nIf math scares you, don’t worry. Our job is to walk you slowly through it. But you still may be asking: Why math though? The beauty of math, is that it allows us to take some fuzzy social science concepts, stated in natural language, such as the idea of “popularity” or “social position” or “strength of connection” and give it a precise representation. That way we can use networks to learn about what makes the social world go round or predict why some people, organizations, or even whole countries are successful and others are not (among other things).\n\n\n\n\n\nFigure 1.10: Levels of analysis in social networks.\n\n\n\nThe two “faces” of SNA (network theory and network measurement) as well as some choice examples are depicted in Figure 1.9. Don’t get nervous if you do not know what the things at the bottom of the diagram (e.g., “density”), means. We will explain them to you in the forthcoming lessons We can develop theories or measure network properties at multiple levels of analysis. Like other complex systems, social networks feature dynamics at multiple nested levels. We will deal with four such leves in what follows. At the node level we may be interested in what properties nodes have by virtue of the connections they have within the network. Both the idea of an ego network and various measures of social position based on centrality are defined at this level.\nAt the dyad and triad levels, we may be interested in the properties that the edges or the links have by virtue of settling into certain configurations. Both the idea of tie strength and various theories dealing with triples of nodes such as Balance Theory (Davis 1963), Strength of Weak Ties Theory (Granovetter 1973), the theory of Structural Holes (Burt 1995), and Simmmelian Tie Theory (Krackhardt 1999) are defined this level. At the level of motifs may be interested in the network substructures or the “lego building blocks” that make up the larger network. For instance, how many configurations of three, four, or five actors can we observe? At the subgroup or community We may be interested in properties that subsets or clusters or nodes have by virtue of the set of connections they share. Here, theories and measures of group cohesion, and community structure in networks have been developed.Finally, we may be interested in measuring properties and theorizing the structure and dynamics of the whole network. This may includes quantities that are sums or averages of features computed at lower levels, or they may include properties applicable to the system as a whole (e.g., whether it would take a short or a long time to get something from one randomly selected person in the network to another). Ideas of whether human networks constitute Small Worlds (Milgram 1967) are defined at this level.\nIn Figure 1.10 we can see how the nested structure of social networks can be depicted. At all levels we can develop specific theories to understand what is happening at that slice or develop special measures designed to link the concepts of those theories to a precise quantitative representation."
  },
  {
    "objectID": "lesson-what-are-networks.html#networks-graphs-and-matrices",
    "href": "lesson-what-are-networks.html#networks-graphs-and-matrices",
    "title": "1  What Are Networks?",
    "section": "1.5 Networks, Graphs, and Matrices",
    "text": "1.5 Networks, Graphs, and Matrices\n’Social network analysis is an influential, and now increasingly widespread, methodological approach for analyzing the social world. Traditionally, sociologists have studied relationships using a variety of observational strategies, both qualitative, such as ethnography and interviews, and quantitative, such as those based on the social survey. However, beginning in earnest in the 1950s, sociologists began to make concerted use of mathematical techniques from a branch of pure mathematics called graph theory and a branch of applied mathematics called matrix algebra to develop scientific models of social relationships and to come up with measures connecting key concepts from social theory, such as roles, prominence, and prestige, to tangible empirical evidence.\nSocial Network Analysis (SNA) is the use of graph-theoretic and matrix algebraic techniques to study social structure and social relationships, which exist in real world networks. While much of this activity has to do with the measurement of social network concepts, Social Network Theory is the branch of social networks that tells us what social networks are, what they do, how they make a difference (negative or positive) in the world, and where networks come from and how they change over time.\n\n\n\nFigure 1.11: The ‘three-step shuffle’ in Social Network Analysis\n\n\nA key skill you will gain by taking this class is to transition swiftly from these three ways of talking about networks, namely, networks as real world systems of social interactions, networks as represented mathematically as graphs, and networks represented quantitatively as matrices. This three-step transition is represented in Figure 1.11. Another skill you will gain by taking this class is how to apply social network theory to understand how real world networks work and change."
  },
  {
    "objectID": "lesson-what-are-networks.html#references",
    "href": "lesson-what-are-networks.html#references",
    "title": "1  What Are Networks?",
    "section": "References",
    "text": "References\n\n\n\n\nBorgatti, Stephen P, and Daniel S Halgin. 2011. “On Network Theory.” Organization Science 22 (5): 1168–81.\n\n\nBurt, Ronald S. 1995. Structural Holes. Harvard University Press.\n\n\nDavis, James A. 1963. “Structural Balance, Mechanical Solidarity, and Interpersonal Relations.” American Journal of Sociology 68 (4): 444–62.\n\n\nGranovetter, Mark S. 1973. “The Strength of Weak Ties.” American Journal of Sociology 78 (6): 1360–80.\n\n\nKrackhardt, David. 1999. “The Ties That Torture: Simmelian Tie Analysis in Organizations.” Research in the Sociology of Organizations 16 (1): 183–210.\n\n\nMilgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67."
  },
  {
    "objectID": "lesson-graph-theory.html#from-ties-to-graphs",
    "href": "lesson-graph-theory.html#from-ties-to-graphs",
    "title": "2  Graph Theory",
    "section": "2.1 From Ties to Graphs",
    "text": "2.1 From Ties to Graphs\nThis is likely not your first sociology course. But even if it is, relationships are relatively intuitive for people. They are all around us: you and your parents, you and your siblings, your siblings and your parents, you and your classmates, your classmates to each other. Affiliation, communication, friendship, hatred: these are the content of social relationships. Two people either have a relationship of some kind, or they do not. Essentially, a relationship is a connection between at least two social actors. We will see that sometimes, when relations clump together into networks, two sets of relations can have different content, but still share the same form (e.g., arrangement, or pattern).\nThe technical social networks term for “relations clumping together into networks” is concatenation (Martin 2009). For instance, every time you introduce two previously unrelated acquaintances to one another, you concatenate two previous disconnected ties into a connected triple."
  },
  {
    "objectID": "lesson-graph-theory.html#social-ties",
    "href": "lesson-graph-theory.html#social-ties",
    "title": "2  Graph Theory",
    "section": "2.2 Social Ties",
    "text": "2.2 Social Ties\nFor example, you are likely enrolled in a social network’s class if you are reading this, and have people you know such as your friends and people you’ve taken prior classes with, but also people you’ve never seen before. It might be obvious that you have a relationship with those people that you know, but do you have a relationship with those you do not know?\n\n\n\nFigure 2.1: “The first picture of a social network, then called a ‘sociogram’ was drawn by Jacob Moreno in 1934. It consisted of the relationships between 19 boys (triangles) and 18 girls (circles) in a 5th grade classroom”\n\n\nThe answer is maybe. It depends on how you define the term social relationship. If you were asked who your friends are, you would tell me that some of your classmates are your friends, and the rest of your classmates are not your friends. If you were asked which of these people in your classroom are your classmates however, everyone would be your classmate, except for your professor or teaching assistants. You share a particular type of relationship with these other people, your classmates, even if you’ve never met them. The word “classmate” even implies a relationship type, one with a different social meaning than “friend.” When analyzing a social network, it is important to first understand what type of social relationship you are analyzing, as it relates directly to what type of conclusions or generalizations you can make about the social world."
  },
  {
    "objectID": "lesson-graph-theory.html#network-boundaries",
    "href": "lesson-graph-theory.html#network-boundaries",
    "title": "2  Graph Theory",
    "section": "2.3 Network Boundaries",
    "text": "2.3 Network Boundaries\nOnce you have a type of social relationship you would like to examine, the next step is to bound the context. If you want to map out all the social relationships in the world, well that’s impossible. Imagine how difficult it would be to map out all the people at your school who are friends with one another. That might be feasible if you have only 1,000 undergraduates, but at a school of 30,000, it would be a nightmare. That is in part why it is so important to bound the social context. The other is to exclude relationships that are not meaningful for your study. Bounding, or to draw boundaries, is to have a rule about what will or will not be included in the study (Laumann, Marsden, and Prensky 1989).\n\n\n\nFigure 2.2: “The Zachary karate club network study was one of the first data collection probjects in the history of SNA. The data are famous for showing how networks could be used to find groups based on the relations between actors.”\n\n\nFor example, if you are interested in who is friends with who in your social networks class, you have bound your study to look at only people who are in your social networks class. One of the most famous social network studies was performed by the anthropologist Wayne W. Zachary (see Figure 2.2 at a college karate club in the 1970s (Zachary 1977). Thus, the 34 members of the karate club and the outside teacher were the actors included in the study because they were the people who were involved in the day to day operations of the karate club at the time data were collected.\nWith a type of social relationship in some bounded context, you can begin to map the social world as a graph. In its most basic form, a graph is essentially a picture of the relationships between different types of social actors. This picture becomes incredibly powerful when we begin to use mathematical concepts to understand how actors relate to each other (mostly what this book is concerned with) or upon what social principles the network may have been formed.\nWhile this class will mostly use the terms node and edge when referring to graphs, these are not the only terms in use among those who use network analysis techniques. Additional names for nodes include, vertex or point. Relationships between two nodes are, in addition to being called edges, are referred to as ties or links. Table Table 6.9 shows the different network lingo people use across disciplines.\n\n\nTable 2.1: Network terminology across disciplines.\n\n\nAcademic Origin\nSocial Actor\nRelationship\n\n\n\n\nGraph Theory\nPoint\nLine\n\n\nNetwork Science\nVertex\nEdge\n\n\nSociology\nActor\nTie\n\n\nComputer Science\nNode\nLink"
  },
  {
    "objectID": "lesson-graph-theory.html#the-building-blocks-of-graphs-edges-and-nodes",
    "href": "lesson-graph-theory.html#the-building-blocks-of-graphs-edges-and-nodes",
    "title": "2  Graph Theory",
    "section": "2.4 The Building Blocks of Graphs: Edges and Nodes",
    "text": "2.4 The Building Blocks of Graphs: Edges and Nodes\nThere is a mathematical definition of a graph which is slightly more technical. A graph is a set, usually represented by the capital letter G.\nFrom high school math, you may remember that the mathematical definition of a set is simply a collection of entities, some of which may be ordered and some of which may themselves be other sets (a set can have sets as its members). In the case of graphs, the entities inside the collection are a set of vertices (also called nodes) and a separate set of edges (also called links).\nA graph is thus a set containing two sets as its members: a set of nodes (usually represented by the capital letter V) and a set of edges (usually represented by the capital letter E).\nIn set theory notation:\n\\[    \n  G = \\{V, E\\}\n\\tag{2.1}\\]\nWhich says that the members of the set defined by the graph, which we call G, are two other sets, called E and V (who themselves have a series of members inside). The usual notation, like in Equation 2.1, is to enclose the members of a set in brackets \\(\\{\\}\\).\n\n2.4.1 Nodes\nThe set of nodes usually represents actors in the real world social network. Point and line diagrams (such as the ones shown in Figure 2.2) are used to represent graphs, these in their turn represent the real social network.\nIn these diagrams nodes (representing actors) are usually drawn as a circle, but they could be any shape or symbol. In social network analysis, actors are often either an individual or an organization, but, as we have seen, in wider applications of the network imagery in the physical and biological sciences (usually going under the banner of network science), nodes can represent anything that links up to other similar entities in a larger system. These include power generation stations and homes, servers and computers, animals in an ecosystem, towns, really anything of substance that we can define some kind of relation on, or from which some type of content can be said to be exchanged.\n\n\n2.4.2 Edges\nEdges represent the presence of a connection or a social tie between two nodes. As we will see this can be a permanent relationship (e.g., “brother of”) or a more fleeting interaction (e.g., a text message, being in the same place at the same time). We will define what social ties are, how many types exist, and what their properties are, in lecture. For now, we can say that in social network analysis, connections are relationships, or links between nodes, and edges in a graph are meant to represent these connections.\nIn graph theory, the set of edges is best thought of as a collection of pairs of nodes, where the two members of the pair are the nodes involved in the social tie. So if node A is linked to node B via some social tie (friendship, study group, coworkers), then AB is a member of the edge set of the relevant graph. In set theory notation, this is usually written as \\(AB \\in E\\) which is read as “the edge AB is a member of the set \\(E\\).” Edges can also be referred to by juxtaposing the two nodes that are connected by the edge. Thus, the edge AB can also be written as \\(V_A V_B\\).\n\n\n\n\n\n\n\n(a) A simple graph.\n\n\n\n\n\n\n\n(b) A G(4, 2) graph\n\n\n\n\n\n\n\n\n\n(c) Another G(4,2) graph.\n\n\n\n\n\n\n\n(d) An unlabeled simple graph.\n\n\n\n\nFigure 2.3: Four simple graphs with four nodes and two edges.\n\n\nIn the case of power generation stations and homes, the edges can represent power lines. Meanwhile, servers and computers are connected by internet cables and wi-fi access, while towns are connected by roads. The existence of edges signal the potential for content to flow, whether that’s power, computer data, or people in cars. In the case of social networks, the content that flows between two nodes are such things as influence, advice, information, and support. But it may also be disease, bullying, or gossip."
  },
  {
    "objectID": "lesson-graph-theory.html#what-is-a-graph",
    "href": "lesson-graph-theory.html#what-is-a-graph",
    "title": "2  Graph Theory",
    "section": "2.5 What is a Graph?",
    "text": "2.5 What is a Graph?\nFigure 2.3(a) shows an example of a point and line network diagram of a graph with four nodes and two edges. Nodes A, B, C and D are circles representing actors A, B, C and D, whose real world social relationships we are interested in studying. The lines drawn between A and B and likewise between B and C represent the edges, indicating the presence of a social tie. Thus the edges, AB and BC appear in the network diagram. The lack of an edge between nodes B and C reflects the absence of a relationship between actors named B and C in the real world.\nSo if we were to write out the graph shown in Figure 2.3(a) in terms of the sets that define the graph, we would say:\n\\[\n  G = \\{E, V\\}\n\\tag{2.2}\\]\n\\[  \nE = \\{AB, AC\\}\n\\tag{2.3}\\]\n\\[\nV = \\{A, B, C, D\\}\n\\tag{2.4}\\]\nThis says that the graph G shown in Figure 2.3(a) is a set with two members, E and V, each of which is its own set. The edge set of G has two members, AB and AC. The node set of G has four members, A, B, C, and D. The number of members in a set is typically referred to as the cardinality of the set, written \\(|N|\\) where \\(N\\) is the name of the set.\nSo, in the Figure 2.3(a) case, the cardinality of the edge set is two \\(|E| = 2\\), and the cardinality of the vertex set is four \\(|V| = 4\\). These two basic graph properties are so important, that they can serve as a “signature” for the graph. Sometimes people will refer to a graph as \\(G(n, m)\\) where \\(n\\) is the cardinality of the vertex set (number of nodes) and \\(m\\) is the cardinality of the edge set (number of links). Thus, the graph in Figure 2.3(a), can be referred to as a \\(G(4, 2)\\) graph. Note that Figure 2.3(a) this is not the only possible \\(G(4, 2)\\) graph. Figure 2.3(b) and Figure 2.3(c) show two other possible variations of a \\(G(4, 2)\\) graph."
  },
  {
    "objectID": "lesson-graph-theory.html#trivial-and-non-trivial-graphs",
    "href": "lesson-graph-theory.html#trivial-and-non-trivial-graphs",
    "title": "2  Graph Theory",
    "section": "2.7 Trivial and Non-Trivial Graphs",
    "text": "2.7 Trivial and Non-Trivial Graphs\nThe \\(G(1, 0)\\) graph, essentially that formed by an isolated person sitting alone in a room with no connections to others (which can represent the case of the Japanese Hikikomori) is sometimes called the trivial graph. Obviously, for purposes of social network analysis, the graphs that are be used are non-trivial.\nThe smallest non-trivial graph, and thus the smallest unit of social analysis, is the \\(G(2, 1)\\) graph, the social unit formed by two people connected by a single link, sometimes referred to as a connected dyad (the \\(G(2, 0\\) graph is a disconnected dyad, like two people stuck in a deserted island who decided to no longer speak to one another). Connected dyads are the “hydrogen atom” of society, the building block from which all other larger networks are built."
  },
  {
    "objectID": "lesson-graph-theory.html#basic-graph-properties",
    "href": "lesson-graph-theory.html#basic-graph-properties",
    "title": "2  Graph Theory",
    "section": "2.8 Basic Graph Properties",
    "text": "2.8 Basic Graph Properties\nGraphs have some basic properties that we need to learn about:\n\nIn a graph, if two nodes are joined together by an edge, they are said be adjacent. So in Figure 2.3(a), nodes A and B are adjacent, as are nodes A and C. Pairs of nodes that are not linked by an edge, like nodes B and C, are said be nonadjacent.\nThe nodes at the two ends of each existing edge are said to be the end vertices of that edge. Each edge has two end vertices. As we have been doing, edges are named by typing together the names of their two end vertices. So the edge that has nodes A and B as its endpoints is called AB.\nIf an edge “touches” a node (e.g., connects it to another node) we say that that edge is incident on that node. So in Figure 2.3(a), the edge AB is incident on both nodes A and B. The relation of incidence will be important in a later lesson when we discuss network metrics computed at the node level like degree centrality.\nFinally, in a graph, nodes that are not connected to any other nodes are called isolates. This means that in Figure 2.3(a), node D is an isolate because it has zero edges incident upon it."
  },
  {
    "objectID": "lesson-graph-theory.html#simple-graphs",
    "href": "lesson-graph-theory.html#simple-graphs",
    "title": "2  Graph Theory",
    "section": "2.9 Simple Graphs",
    "text": "2.9 Simple Graphs\nGraphs like that shown in Figure 2.3 are called simple graphs. There are two requirements for a graph to count as a simple graph:\n\nFirst, there can only be one edge joining two nodes at any time. That is, there cannot be multiple lines linking together the same pair of nodes. We will see in a later lesson that there are types of graphs that relax this restriction.\nThe second requirement is that graph does not contain any edges that have the same node as its two end points. These kind of edges are called loops, and they are edges that connect a node to itself! Clearly this does not make sense for most sociological applications.\n\nWith some exceptions, noted in subsequent lessons, simple graphs can represent most social networks."
  },
  {
    "objectID": "lesson-graph-theory.html#graph-labeling",
    "href": "lesson-graph-theory.html#graph-labeling",
    "title": "2  Graph Theory",
    "section": "2.6 Graph Labeling",
    "text": "2.6 Graph Labeling\nIn Figure 2.3(a-c), the nodes have letters which function as “names” for each. Thus, we can refer to node \\(A\\), or node \\(B\\) and so forth. These names are arbitrary, we can as well use numbers \\(\\{1, 2, 3, ...\\}\\) or a combination of letters and numbers \\(\\{V_1, V_2, V_3, ....\\}\\).\nWhen the nodes of a graph are distinguished from one another using names, the graph is said to be labeled. Sometimes, the names don’t matter, so we don’t specify them, as in Figure 2.3(d). In this case, the graph is said to be unlabeled.\nGraph metrics like the ones we will compute starting on Chapter 5, are the same regardless of whether the graph is labeled or not. Most of the example graphs we will be looking at are of the labeled kind."
  },
  {
    "objectID": "lesson-graph-theory.html#references",
    "href": "lesson-graph-theory.html#references",
    "title": "2  Graph Theory",
    "section": "References",
    "text": "References\n\n\n\n\nLaumann, Edward O, Peter V Marsden, and David Prensky. 1989. “The Boundary Specification Problem in Network Analysis.” In Research Methods in Social Network Analysis, edited by Linton C. Freeman, Douglas R. White, and Antone Kimball Romney, 61–79. New Brunswick: Transaction Publishers.\n\n\nMartin, John Levi. 2009. Social Structures. Princeton University Press.\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73."
  },
  {
    "objectID": "lesson-node-neighborhoods.html#node-neighborhoods",
    "href": "lesson-node-neighborhoods.html#node-neighborhoods",
    "title": "3  Nodes and their Neighborhoods",
    "section": "3.1 Node Neighborhoods",
    "text": "3.1 Node Neighborhoods\nAs we have seen, each node in a graph or order \\(N\\), given by the set \\(V = \\{v_1, v_2, v_3, \\dots v_N\\}\\) may be adjacent to a certain set of other nodes. In graph theory, these are called the node’s neighbors.The neighborhood of a node in a graph is written as \\(\\mathcal{N}(v)\\), where \\(v\\) is the node’s name in the graph. For instance, if we are referring to the neighbors of node A in the graph shown as Figure 3.1 we would write \\(\\mathcal{N}(A)\\).\nThe neighborhood of each node is a proper subset of the larger set of nodes in the graph \\(V\\). This is written as \\(\\forall v: \\mathcal{N}(v) \\subset V\\), which translates from math to English as “for all nodes \\(v\\), the neighborhood of \\(v\\) is a subset of the larger node set \\(V\\).” In Figure 3.1 for instance, \\(\\mathcal{N}(A) = \\{B, C, D, F\\}\\), and \\(\\mathcal{N}(A) \\subset V\\).1\n\n\n\n\n\nFigure 3.1: Another simple graph.\n\n\n\n\n\n3.1.1 Node Neighborhood Intersection\nNote that the neighbor sets of two nodes can have members in common. For instance, in Figure 3.1 we have \\(\\mathcal{N}(A) = \\{B, C, D, F\\}\\) and we also have \\(\\mathcal{N}(D) = \\{A, B, F\\}\\). These two sets share common members!\nSometimes we may be interested in the total number of other people that two nodes share a connection with. Like when you wonder how many people you and your friend are both friends with (or a social media algorithm lets you know). This is called the intersection of the two node neighborhood sets.\nSo if A and D are both nodes in a graph, the intersection of their neighborhood sets gives us a list of the other nodes in the graph they are both connected to. Using set theory notation, this can be written as: \\(\\mathcal{N}(A) \\cap \\mathcal{N}(D) = \\{B, F\\}\\), which says that nodes A and D have B and F as common neighbors.2\nThe cardinality of the sets formed by the intersection of the neighborhoods of all the nodes in the graph gives us the number of common neighbors between each pair of nodes, which may be zero if two neighborhoods sets are disjoint. As we saw in Chapter 2, in set theory, the cardinality of a set is the number of members in that set. Thus, the cardinality of the set \\(\\{A, B, C, D\\}\\) is four. Two sets are disjoint if they have no members in common, which means that their intersection is the empty set. We will see in a later lesson that this quantity has applications for deriving important matrices from graphs and computing some key network metrics in the network.\nNote that two nodes can have common neighbors even if they are not directly connected in the network! So the number of common neighbors is defined for both connected and null dyads.\nFor instance, in Figure 3.1, the intersection of the neighborhoods of nodes D and E exists and it is given by \\(\\mathcal{N}(D) \\cap \\mathcal{N}(E) = \\{B, F\\}\\) even though nodes D and E are not linked (they are nonadjacent).\n\n\n3.1.2 Node Neighborhood Union\nSometimes we may be interested in the total number of other people that two nodes are connected to, regardless of whether both of them are connected to them. Think of this as adding the set of people that you know with the set of people one of your friends knows, counting the people that your friend knows but you don’t, and the people you know but your friend doesn’t. This is called the union of the two node neighborhood sets.\nSo if A and D are both nodes in a graph, the union of their neighborhood sets gives us a list of the total number of other nodes in the graph either one is connected to.\nUsing set theory notation, this can be written as:3\n\\[\n    \\mathcal{N}(A) \\cup \\mathcal{N}(D) = \\{B, C, F\\}\n\\tag{3.1}\\]\nWhich says that nodes A and D have B C, and F as neighbors, but not necessarily common neighbors.\nAs we will see later, the intersection and union of the neighborhood sets can be used as a basis to construct measures of (structural) similarity between nodes in a graph."
  },
  {
    "objectID": "lesson-node-neighborhoods.html#sec-degree",
    "href": "lesson-node-neighborhoods.html#sec-degree",
    "title": "3  Nodes and their Neighborhoods",
    "section": "3.2 Node Degree",
    "text": "3.2 Node Degree\nIn a graph, a given node’s degree can be defined in two ways, both of which lead to the same answer.\nOne way to think about the degree of a given node \\(i\\) in a graph (written \\(k_i\\)) is as the cardinality of the set of neighbors of that node as defined earlier:\n\\[\n  k_i = |\\mathcal{N}(i)|\n\\tag{3.2}\\]\nSo in the graph shown in Figure 3.1, \\(k_A = |\\mathcal{N}(A)| = |\\{B, C, D, F\\}|=4\\).\nAnother way to think about node degree is not as the cardinality of the node neighborhood set, but as a count of edges. In this case, we count the number of edges that have a given node \\(i\\) as one of their endpoints. Recall, that an edge that has a given node as one of their endpoints is said to be incident upon that node. So in the graph shown in Figure 3.1, the set of edges that have node A as one of their endpoints is \\(k_A = \\{AB, AC, AD, AF\\}\\) and \\(|k_A|\\) = 4.\nEither way, computing degree as the cardinality of the node’s neighbor set or as the number of edges incident upon the node, gives us the number of other actors that a given node is connected to in the network. We will see in a later lesson, that this is an important measure of node position called degree centrality (Freeman 1977). In a graph, nodes that have a degree equal to one, and thus have just a single neighbor in the graph, are called the endpoints. Thus, in Figure 3.1, node \\(C\\) is an endpoint."
  },
  {
    "objectID": "lesson-node-neighborhoods.html#references",
    "href": "lesson-node-neighborhoods.html#references",
    "title": "3  Nodes and their Neighborhoods",
    "section": "References",
    "text": "References\n\n\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41."
  },
  {
    "objectID": "lesson-graphs-subgraphs.html#graphs-and-subgraphs",
    "href": "lesson-graphs-subgraphs.html#graphs-and-subgraphs",
    "title": "4  Graphs and their Subgraphs",
    "section": "4.1 Graphs and Subgraphs",
    "text": "4.1 Graphs and Subgraphs\nConsider the graph shown in Figure 4.1(a). If all the actors that you are interested in studying are included here, we would refer to it as the whole network. However, sometimes, even when we collect data on a large number of actors we may be interested in analyzing not the whole network, but only some parts of it. How do we do that?\n\n\n\n\n\n\n\n(a) Original graph\n\n\n\n\n\n\n\n(b) A subgraph of the original graph.\n\n\n\n\n\n\n\n\n\n(c) Another subggraph of the original graph.\n\n\n\n\n\n\n\n(d) An edge deleted subgraph of the original graph.\n\n\n\n\nFigure 4.1: A graph (a) and a subgraph (b).\n\n\nWell, good thing that a graph is actually a set of two sets. If you remember your high school set theory, you can always take a set and consider only a subset of the original members.\nSince graphs are sets, we can do the same thing. A subset of the original nodes (or edges) of a graph, is called a subgraph. So if \\(G =\\{E,V\\}\\) is the original graph, the subgraph \\(G' = \\{E',V'\\}\\) is a subset of \\(G\\), which is written \\(G \\subset G'\\), with the understanding that \\(E' \\subset E\\) and \\(V' \\subset V\\). In mathematics, “\\(\\subset\\)” is the symbol for subset. Thus, \\(A \\subset B\\) is read as “set A is a subset of set B.”\nFor instance, let us say we are interested in just analyzing actors A, B, C, D, and E in the graph shown in Figure 4.1(a). They seem to be a close-knit group of people. In that case, as noted earlier, if we call the original graph \\(G\\) with vertex and edge sets \\(\\{E, V\\}\\)we can define a new subgraph \\(G'\\), whose node subset \\(V'\\) only includes the actors we are interested in studying, in this case \\(V' = \\{A, B, C, D, E\\}\\), where \\(V' \\subset V\\).\nThe subgraph \\(G'\\) is shown in Figure 4.1(b). It looks exactly like we wanted, capturing the relations between an inter-connected subgroup of actors in the original graph. Note that the edge set of the subgraph \\(E'\\) only includes those edges that are incident to the other nodes in the subgraph (as defined in Chapter 2) and omits those in the original graph that are incident to nodes that are not in the subgraph, so \\(E' \\subset E\\). As we will see in a later lesson, well-connected subgroups of actors of an original graph are called a cohesive subset."
  },
  {
    "objectID": "lesson-graphs-subgraphs.html#vertex-and-edge-induced-subgraphs",
    "href": "lesson-graphs-subgraphs.html#vertex-and-edge-induced-subgraphs",
    "title": "4  Graphs and their Subgraphs",
    "section": "4.2 Vertex and Edge-Induced Subgraphs",
    "text": "4.2 Vertex and Edge-Induced Subgraphs\nFor any graph, we can define a subgraph based on any old random subset of the original node set. It is completely up to us. For instance, we could define a new subgraph \\(G''\\) of the original graph shown in Figure 4.1(a), that includes the node set \\(V'' = \\{D, E, G, I\\}\\). That is shown in Figure 4.1(c). That subgraph is weird (composed of two standalone connected dyads) and probably not very useful, but it is a subgraph of the original graph anyways!\nJust like we can define subgraphs based on the node set of a graph, we can define subgraphs based on subsets of the original edge set. For instance, we could pick the edges \\(E' = \\{AB, AC, AJ\\}\\) and define a subgraph based on them, which will necessarily include node set \\(V' = \\{A, B, C, J\\}\\).\nWhen a subgraph is defined by selecting a subset of nodes to keep (the common case) it is called a vertex-induced subgraph of the original graph, like Figure 4.1(b). When a subgraph is defined by picking a subset of edges from the original graph to keep, it is called (you guessed it) an edge-induced subgraph of the original graph."
  },
  {
    "objectID": "lesson-graphs-subgraphs.html#vertex-and-edge-deleted-subgraphs",
    "href": "lesson-graphs-subgraphs.html#vertex-and-edge-deleted-subgraphs",
    "title": "4  Graphs and their Subgraphs",
    "section": "4.3 Vertex and Edge-Deleted Subgraphs",
    "text": "4.3 Vertex and Edge-Deleted Subgraphs\nA common reason for defining subgraphs in social network analysis is when we wonder what a network would look like if were to get rid of an actor or a set of actors. Sometimes this is useful, when we want to get a sense of how important that set of actors is for holding the network together. So it is possible to define a subgraph by deleting nodes. This is written \\(G' = G - \\{a, b, c\\}\\), where \\(\\{a, b, c\\}\\) is the set of nodes we are deleting. So this says “give me a subgraph \\(G'\\) that is equal to the original graph \\(G\\) minus nodes \\(\\{a, b, c\\}\\).” In our previous example, Figure 4.1(b) is the subgraph that results when we delete nodes \\(\\{F, G, H, I, J\\}\\) from the graph in Figure 4.1(a): \\(G' = G - \\{F, G, H, I, J\\}\\). This is called a vertex-deleted subgraph of the original graph.\nThe subgraph that results from removing all the nodes of the original graph (so that the cardinality of the node set is now zero) is called the null graph. The subgraph that results from removing all the nodes of the original graph except for one (so that the cardinality of the node set of the resulting subgraph is equal to one) is called the singleton graph, also called the trivial graph as we saw in Chapter 2 (one is indeed a lonely number in social networks).\nJust like we can create subgraphs by deleting nodes, we can also create subgraphs by removing edges. For instance Figure 4.1(d) is the subgraph that results from removing edges \\(\\{AB, AE, AC, CI, CE, GJ\\}\\) from the graph shown in Figure 4.1(a). This is written \\(G' = G - \\{AB, AE, AC, CI, CE, GJ\\}\\), which says “give me a new graph \\(G'\\) which is equal to the original graph \\(G\\) minus edges \\(\\{AB, AE, AC, CI, CE, GJ\\}\\). This is called the edge-deleted subgraph of the original graph.\nA subgraph created by removing only edges, but leave all the nodes of the original graph intact (like in Figure 4.1(d)) is also called a spanning subgraph of the original graph. The subgraph that results from removing all the edges in a graph, such that the cardinality of the edge set turns to zero, is called the empty graph.\nAs we will see later, subgraphs (as well as vertex and edge deletion) are a useful concept for discussing levels at an “in-between” levels, above the node level but “below” the whole network level: subgroups. However, subgraphs are also useful for network concepts at the node level, because there is a special type of subgraph, called the ego graph that is defined by picking a central node and the nodes that are connected to it, along with the edges connecting the nodes surrounding ego. We will cover ego graphs in Chapter 18."
  },
  {
    "objectID": "lesson-graph-metrics.html#computing-metrics-on-graphs",
    "href": "lesson-graph-metrics.html#computing-metrics-on-graphs",
    "title": "5  Basic Graph Metrics",
    "section": "5.1 Computing Metrics on Graphs",
    "text": "5.1 Computing Metrics on Graphs\nAs graphs are really just representations of real world-networks, they can be as varied as different networks are from one another. Some social networks are small (composed of just a few people), while others are large. In some social networks there is not much connectivity, while other networks feature thousands if not millions of ties between people. We can precisely quantify a lot of these properties of real world networks by computing graph metrics. This is fancy word for counting various numbers most of which are a function of the number of nodes and edges in a graph. In this lesson, we will review the most important graph metrics in social networks.\n\n5.1.1 Graph Order\nThe order of a graph (typically written as \\(n\\)), is the number of nodes in the graph. Technically speaking, keeping in mind that the graph is really a set of a set of nodes and edges, the order is the cardinality of node set \\(n = |V|\\). Cardinality is simply a technical term from set theory indicating how many elements there are in a set. Thus, the graph in Figure 8.1 has an order of 9, while Figure 8.2 has an order of 7.\n\n\n5.1.2 Graph Size\nLikewise, the size of a graph (typically written as \\(m\\)), is the number of edges in the graph, which is (like with nodes) the cardinality of the edge set \\(m = |E|\\). Thus, the size of the graph in Figure 8.1 is 16, while the size of the graph in Figure 8.2 it is 11.\nThis means that if you say the size of the graph in Figure 8.1 is 16, we will all understand that you are referring to the number of edges, as you would use the term order if you were talking about the number of nodes."
  },
  {
    "objectID": "lesson-graph-metrics.html#the-graph-maximum-size",
    "href": "lesson-graph-metrics.html#the-graph-maximum-size",
    "title": "5  Basic Graph Metrics",
    "section": "5.2 The Graph Maximum Size",
    "text": "5.2 The Graph Maximum Size\nIn some cases we may be interested to know how many edges there could be in a graph if everyone had a relationship with everyone else in the network. This is the maximum possible number of edges that could exist in a network of order \\(n\\).\nRecall that, as we discussed previously, the number of edges in a graph is called the graph size. Additionally, a graph in which all the edges that could exist are actually present is called the complete graph. The maximum size of a graph of order \\(n\\) is the number of edges that would exist in that graph is the graph was complete. A graph in which no edges exist (every node is an isolate), is called an empty graph.\nWhile this may seem like a complicated thing, it is actually given by a simple (and important) formula. For an undirected graph this is:\n\\[\n  Max(E)^{u}=\\frac{n(n-1)}{2}\n\\tag{5.1}\\]\nFor a directed graph, the same computation is even simpler. The maximum number of possible edges in this case is:\n\\[\n  Max(E)^d=n(n-1)\n\\tag{5.2}\\]"
  },
  {
    "objectID": "lesson-graph-metrics.html#graph-maximum-size-explainer",
    "href": "lesson-graph-metrics.html#graph-maximum-size-explainer",
    "title": "5  Basic Graph Metrics",
    "section": "5.3 Graph Maximum Size Explainer",
    "text": "5.3 Graph Maximum Size Explainer\nWhere do these formulas for maximum graph size come from? Let’s consider the directed case first. A graph would be complete if every node was connected to every other node. It is easy to see that for each node, this would be every other node in the graph except themselves. So in a complete graph of order \\(n\\), the degree of each node has to be \\(n-1\\). So if we call the degree of the first node in the graph \\((n-1)_1\\), then the degree set (\\(\\mathbf{k}^c\\)) of a complete graph would be:\n\\[\n  \\mathbf{k}^{c} = [(n-1)_1, (n-1)_2, (n-1)_3 \\dots (n-1)_n]  \n\\tag{5.3}\\]\nIt is easy to see the that the sum of degrees (\\(\\sum k_i^c\\)) of the complete graph, will give us the maximum size for any graph of that order. So we can write that sum as:\n\\[\n  \\sum k_i^{c} = \\sum_i^{n} n-1\n\\tag{5.4}\\]\nEssentially we add up \\(n-1\\) to \\(n-1\\) to \\(n-1\\), etc. \\(n\\) times, once for each node in the graph. From basic arithmetic, we know that adding the same number \\(n\\) number of times is the same as multiplying that number by \\(n\\). So therefore, the maximum size of a complete graph of order \\(n\\) is \\(n \\times n-1 = n(n-1)\\), the formula written as Equation 5.2!\nIn the undirected case (Equation 5.1, the procedure is the same, except that now each edge shows up twice in the degree sum, but one of those is redundant. Therefore we divide the whole sum of degrees by 2 to eliminate double-counting, resulting in \\(\\frac{n(n-1)}{2}\\) as the formula for the maximum size of an undirected graph or order \\(n\\).\n\n5.3.1 Applying your knowledge\nYou can now look a genius. If somebody were to ask you:\nQ: In classroom with 10 kindergartners, how many total pairs of friends would exist if every kid was friends with every other kid?\nUsing your graph theory metrics knowledge, you can now pull out your phone calculator and compute:\n\\[\n  \\frac{10(10-1)}{2} =\n  \\frac{10(9)}{2} =\n  \\frac{90}{2} = 45\n\\]\nImpressing your audience immensely!"
  },
  {
    "objectID": "lesson-graph-metrics.html#graph-density",
    "href": "lesson-graph-metrics.html#graph-density",
    "title": "5  Basic Graph Metrics",
    "section": "5.4 Graph Density",
    "text": "5.4 Graph Density\nIn a classic study, the British social anthropologist Elizabeth Bott, made a classic distinction between two types of network structure. According to Bott, some networks are tight-knit and others are loose-knit (Bott 1957). Tight-knit networks feature a lot of connections and actors can reach others via multiple pathways. Loose-knit networks are more sparsely connected and actors are only reachable to one another via a very restricted set of pathways.\nIt was soon realized that a very simple graph metric, called the network density could be a useful index of Bott’s ideas of tight versus loose-knit networks. Tight-knit networks are dense featuring a lot of inter-connections between actors, while loose-knit networks are less dense (Barnes 1969). How can we think of this in terms of graph theory concepts?\n\n5.4.1 Density in Undirected Graphs\nThe density \\(d(G)\\) of a graph is a measure of how many ties between actors exist compared to how many ties between actors are possible, given the graph size (number of nodes) and the graph order (number of links). As such, the density of an undirected graph is quite simply calculated as, the ratio between the observed number of edges \\(m\\) (the cardinality of the edge set), and the graph maximum size as defined using Equation 5.1.\n\\[\n  d(G)^u=\\frac{m}{\\frac{n(n-1)}{2}}=m \\times \\frac{2}{n(n-1)}=\\frac{2m}{n(n-1)}\n\\tag{5.5}\\]\nWhere \\(m\\) is the number of edges (graph size) and \\(n\\) is the number of nodes (graph order) in the network.\nFor the graph shown in Figure 8.1, of size \\(m = 16\\) and order \\(n=9\\), we can use Equation 5.5 to compute the graph density as follows:\n\\[\n  d(G)^u = \\frac{2m}{n(n-1)} =\n  \\frac{2 \\times 16}{9 \\times (9-1)}=\n  \\frac{32}{9 \\times 8} =\n  \\frac{32}{72} =\n  0.44\n\\]\nThe estimated density of the network, \\(d = 0.44\\) tell us that 44% of the total possible number of edges are actually observed. Another way to think about density, is as giving the probability that, if we were to choose two random nodes in the network, this random dyad will have probability \\(p = 0.44\\) of being connected (as opposed to null).\n\n\n5.4.2 Density in Directed Graphs\nTo compute the density of a directed graph, there is no need to multiply the numerator by two, as each edge does single duty. As such, the equation for computing the density of a directed graph is:\n\\[\n  d(G)^d=\\frac{m}{n(n-1)}\n\\tag{5.6}\\]\nWhich is even simpler than in the directed case!\n\n\n5.4.3 Why is density important?\nThe density of a network property is important to consider for two reasons. First, (which is the definition of density!) is that it can help us understand how connected the network is compared to how connected it might be. Second, when comparing two networks with the same number of nodes and the same type of relationships, it can tell us how the networks are different.\nFor example, let us imagine that there are two organizations, each with 10 people in them. The one organization has high density and the other has low density in terms of the interactions among the members. What might be some of the underlying social differences between the two organizations? While we would need more information, we could posit that the one issue might be that information does not transmit very efficiently across the low density organization because it has to go from member to member, rather than diffusing from one member rapidly to all the others. Another issue might be the “hit by a bus” problem, where if one or two members are taken out of the network, you can suffer breakdown because they are no longer there to coordinate the different parts that don’t talk to each other. Denser networks are less vulnerable to disruption due to remove of key nodes."
  },
  {
    "objectID": "lesson-graph-metrics.html#references",
    "href": "lesson-graph-metrics.html#references",
    "title": "5  Basic Graph Metrics",
    "section": "References",
    "text": "References\n\n\n\n\nBarnes, John A. 1969. “Graph Theory and Social Networks: A Technical Comment on Connectedness and Connectivity.” Sociology 3 (2): 215–32.\n\n\nBott, Elizabeth. 1957. Family and Social Network: Roles, Norms, and External Relationships in Ordinary Urban Families. Tavistock Publications."
  },
  {
    "objectID": "lesson-from-graph-to-matrix.html#matrices",
    "href": "lesson-from-graph-to-matrix.html#matrices",
    "title": "6  From Graph to Matrix",
    "section": "6.1 Matrices",
    "text": "6.1 Matrices\nThus, switching to representing social networks as a matrix provides us with more analytic leverage. This was a brilliant idea that first occurred to Elaine Forsyth and Leo Katz in the mid 1940s (Forsyth and Katz 1946). When we represent the network as a matrix, we are able to efficiently calculate features of the network that we would not be able to estimate via “eyeballing.”\nWhat is a matrix?2 A matrix is, quite simply, a set of attributes that represent the values of a particular case. Breaking that explanation down, we can imagine a matrix as in Table 6.1. This common matrix, which we will refer to as an attribute-value matrix, a toy example of which is presented in Table 6.1, seems similar to a spreadsheet. Well, that is because a spreadsheet is a matrix!\n\n\nTable 6.1: Example of a general matrix.\n\n\n\nAttribute 1\nAttribute 2\nAttribute 3\n\n\n\n\nCase 1\nValue 1\nValue 4\nValue 7\n\n\nCase 2\nValue 2\nValue 5\nValue 8\n\n\nCase 3\nValue 3\nValue 6\nValue 9\n\n\nCase 4\nValue 10\nValue 11\nValue 12\n\n\nCase 5\nValue 13\nValue 14\nValue 15\n\n\n\n\nThe most important feature of a matrix is thus its organization into rows and columns. The number of rows and the number of columns define the dimensions of the matrix (like the length and and the width of your apartment define its dimensions in space). So when we say that a matrix is 5 \\(\\times\\) 3 when mean that it has five rows and three columns. When referring to the dimensions of matrix the rows always come first, and the columns always come second. So the more general way of saying is that the dimensions of a matrix are R \\(\\times\\) C, where R is the number of rows and C is the number of columns.\nThe intersection of a particular row (say row 2 in Table 6.1 and a particular column (say column 3 Table 6.1 defines a cell in the matrix. So when referring to a particular value in Table 6.1 we would speak of the \\(ij^{th}\\) cell in the matrix (or \\(c_{ij}\\)), where c is a general stand-in for the value of a given cell, i a general stand-in for a given row, and j is a generic stand-in for a given column. We refer to i as the matrix row index and j as the matrix column index.\nTypically, we give matrices names using boldfaced capital letters, so if we call the matrix shown in Table 6.1, matrix B, then we can refer to a specific cell in the matrix using the notation b\\(_{ij}\\) (note the lowercase), which says “the cell corresponding to row i and column j of the B matrix.”\nThus, in Table 6.1, cell b\\(_{32}\\) refers to the intersection between row 3 (representing case 3) and column 2 (representing attribute 2), where we can find value 6. For instance, let’s say cases are people and attributes are information we collected on each person (e.g., by surveying them) like their age, gender identity, and racial identification and so forth. Thus, if attribute 2 in Table 6.1 was age, and case 3 was a person, then value 6 would record that persons age (e.g., 54 years old).\n\n6.1.1 Relationship Matrices\nWe do not generally use attribute-value matrices to represent networks. Instead, we typically use a particular type of matrix called a relationship matrix. A relationship matrix is when, instead of asking what value of an attribute a case has, we ask about the value of describing how a case relates to other cases. If attribute-value matrices relate cases to attributes, then relationship matrices relate cases to one another (which is precisely the idea behind a “network”).\nTo do that, we put the same list of cases on both the rows and columns the matrix. Thus, we create a matrix with the organizational properties shown in Table 6.2.\n\n\nTable 6.2: Example of a relationship matrix.\n\n\n\nCase 1\nCase 2\nCase 3\n\n\n\n\nCase 1\nValue 1\nValue 2\nValue 3\n\n\nCase 2\nValue 4\nValue 5\nValue 6\n\n\nCase 3\nValue 7\nValue 8\nValue 9\n\n\n\n\nA relationship matrix thus captures exactly that, the relationship between two cases as shown in Table 6.2. So each cell, as the intersection of two cases (the row case and column case) gives us the value of the relationship between the cases. This value could be “friends” (if the two people are friends) or “not friends” (if they are not friends). The value could be the strength of the relationship. For instance each cell could contain the number of times a given case (e.g., a person) messaged another case.\nRelationship matrices are different from attribute value matrices, in that the latter are typically rectangular matrices. In a rectangular matrix, the number of rows (e.g., people) can be different from the number of columns (e.g., attributes). For instance, the typical attribute-value matrix used by social scientists who collect survey data on people are typically rectangular containing many more cases (rows) and columns (attributes). Some networks, like two mode networks represented as bipartite graphs, are best studied using rectangular matrices.\nRelationship matrices have some unique attributes. For instance, all relationship matrices are square matrices. A square matrix is one that has the same number of rows and columns: \\(R = C\\). So the relationship matrix shown in Table 6.2 is \\(3 \\times 3\\). A square matrix with n rows (and thus the same number of columns) is said to be a matrix of order n.\n\n\n6.1.2 Diagonal versus off-diagonal cells\nIn a relationship matrix, we need to distinguish between two types of cells. First, there are the cells that fall along the main diagonal an imaginary line that runs from the uppermost left corner to the lowermost right corner; these are called diagonal cells, the values corresponding to which are shown in italics in Table 6.2. So if we name the matrix in Table 6.2 matrix A, then we can see that any cell a\\(_{ij}\\) in which i = j falls along the main diagonal; these are Values 1, 5, and 9 Table 6.2. Every other cell in which i \\(\\neq\\) j, is an off-diagonal cell.3\nIn reference to the main diagonal, off-diagonal cells are said to be above the main diagonal if the row index for that cell is smaller than the column index (e.g., a\\(_{i < j}\\)). So in Table 6.2, values 2, 3, and 6, corresponding to cells a\\(_{12}\\) a\\(_{13}\\) and a\\(_{23}\\), respectively, are above the main diagonal. In the same way, cells in which the row index is larger than the column index are said to be below the main diagonal (e.g., a\\(_{i > j}\\)). So in Table 6.2, values 4, 7, and 8, corresponding to cells a\\(_{21}\\) a\\(_{31}\\) and a\\(_{32}\\), respectively, are below the main diagonal.\nNote that in a square matrix, the values above and below the main diagonal have a “triangular” arrangement. Accordingly, sometimes we refer to these areas of a square matrix as the upper and lower triangles.\nNote also that if the relationship matrix represents the relationship between the cases, and the cases are people in a social network, then the diagonal cells in a relationship matrix represent the relationship of people with themselves! Now if you have seen M. Night Shyamalan movies about people with split personalities, it is quite possible for people to have a rich set of relationships with themselves. Some of these may even form a social network inside a single head (Martin 2017). But we are not psychiatrists, so we are interested primarily in interpersonal not intrapersonal relations.\n\n\nTable 6.3: Example of a relationship matrix with blocked diagonals.\n\n\n\nCase 1\nCase 2\nCase 3\n\n\n\n\nCase 1\n–\nValue 1\nValue 2\n\n\nCase 2\nValue 3\n–\nValue 4\n\n\nCase 3\nValue 5\nValue 6\n–\n\n\n\n\nThis means that most of the time, we can ignore the diagonal cells in relationship matrices and rewrite them as in Table 6.3, in which values appear only for the off-diagonal cells. So here we can see the relationship between Case 1 and Case 2 is Value 1, and the relationship between Case 2 and Case 1 is Value 3. Wait, would that mean Value 2 and 4 are the same? The answer is maybe. Depends on what type of network tie is being captured, as these were discussed in the lesson on graph theory. If the tie is symmetric (and thus represented in an undirected graph), then the values will have to be the same. But if the asymmetric (and thus represented in a directed graph) then they don’t have to be.\nBy convention, in a relationship matrix, we say that the case located in row i sends (a tie) to the case located in column j, so if the relationship matrix was capturing friendship, we might say that i considers j to be a friend (sends the consideration) and so if i is Case 1 (row 1) and j is Case 2 (column 2), that would be Value 1 (e.g., “Are we friends?” Value 1 = Yes/No). But when i is now Case 2 (row 2) and j is Case 1 (column 1), we are now asking if Case 2 considers Case 1 to be a friend (e.g., “Are we friends?” Value 3 = Yes/No). If friendship is considered an asymmetric tie in this case, then that could be true, or it could not be. For instance, Case 2 can rebuff Case 1’s friendship offer.\nNote that if the tie we recorded in a relationship matrix is symmetric, we can simplify the presentation even further. The reason is that as already noted, if a relationship is symmetric, then the value of the tie that i sends to j is necessarily the same as the value of the tie that j sends to i. This means that, in the relationship matrix, the value of cell a\\(_{ij}\\) has to be the same as the value of the cell a\\(_{ji}\\) for all rows i and columns j in the matrix. This yields a symmetric relationship matrix, like that shown in Table 6.4.\n\n\nTable 6.4: Example of a symmetric relationship matrix with blocked diagonals.\n\n\n\nCase 1\nCase 2\nCase 3\n\n\n\n\nCase 1\n–\nValue 1\nValue 2\n\n\nCase 2\nValue 1\n–\nValue 3\n\n\nCase 3\nValue 2\nValue 3\n–\n\n\n\n\nNote that a symmetric relationship matrix is simpler than its asymmetric counterpart, because now we only have to worry about half of the values. So before, in Table 6.3 we had to worry about six distinct relationship values, but now we only have to worry about three. This means that, in a symmetric matrix, all the network information we need to look at is contained in either the lower triangle or the upper triangle. As we will see, in many applications, we can ignore one of the triangles altogether!\nThere are many types of relationship and attribute-value matrices as the basic principles just stated can be varied to capture different underlying facets of relationships. This lesson will cover various ways different types of networks can be best captured in a matrix form and then manipulated to produce sociologically meaningful results."
  },
  {
    "objectID": "lesson-from-graph-to-matrix.html#adjacency-matrices-for-different-types-of-graphs",
    "href": "lesson-from-graph-to-matrix.html#adjacency-matrices-for-different-types-of-graphs",
    "title": "6  From Graph to Matrix",
    "section": "6.2 Adjacency Matrices for Different Types of Graphs",
    "text": "6.2 Adjacency Matrices for Different Types of Graphs\nThe adjacency matrix is the most important, most commonly used way of representing graphs in network analysis. An adjacency matrix asks if two cases share a relationship or not. As we saw in the lesson on graph theory, two actors have a relationship, they share an edge (are adjacent) in the graph, whereas if they do not share a relationship, they do not share an edge in the graph (are non-adjacent).\n\n6.2.1 The Symmetric Adjacency Matrix\nIf we want to build an adjacency matrix of a network, we simply list all the actors in the rows and columns, and ask if the two share a relationship in order to fill in the values. A great way of understanding this is to start with a graph and to convert it into a matrix. This is done below where the undirected graph from Figure 8.1 is converted into its symmetric adjacency matrix equivalent, shown in Table 6.5.\nThe first step in building the adjacency matrix that represents the graph is to list all the nodes {A, B, C, D, E, F, G, H, I} as both a row and a column entry for each node. Next, one goes sequentially across the rows and columns, asking the question “does actor i have the relationship I am examining with actor j?” If the question asked is about the absence or presence of a relationship, 0’s and 1’s are used. If A has a relationship with B, the value 1 is marked. Otherwise, 0.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n–\n1\n1\n1\n1\n0\n0\n0\n0\n\n\nB\n1\n–\n1\n1\n0\n0\n0\n0\n0\n\n\nC\n1\n1\n–\n1\n1\n0\n0\n0\n0\n\n\nD\n1\n1\n1\n–\n1\n1\n0\n0\n0\n\n\nE\n1\n0\n1\n1\n–\n0\n0\n0\n0\n\n\nF\n0\n0\n0\n1\n0\n–\n1\n1\n1\n\n\nG\n0\n0\n0\n0\n0\n1\n–\n1\n1\n\n\nH\n0\n0\n0\n0\n0\n1\n1\n–\n1\n\n\nI\n0\n0\n0\n0\n0\n1\n1\n1\n–\n\n\n\nTable 6.5: Symmetric adjacency matrix corresponding to an undirected graph.\n\n\nAs we can see in Table 6.5, A indeed has a relationship with B, so the corresponding cell is marked 1. In fact, A has a relationship with B, C, D, and E and has 1’s in each of the cells corresponding to these actors, but not with F, G, H, or I and so 0’s are in these cells.\nBut what do we do about the cells where we are theoretically supposed to ask if A has a relationship with A? As we have seen, for most sociological applications, it makes sense to just put a dash there, thus blocking the diagonals. It’s not sociologically meaningful for A to have a relationship with itself. For example, asking “Is A friends with A?” does not make much sense, but there are rare cases when it does, such as when A is a group of people and not an individual, and the relationship under examination might occur both within and between groups. As we saw before, these are called reflexive-ties or loops. But if the network is represented as a simple graph it should contain no loops.\nAfter completing the first row, we ask does actor B have a relationship with actor A? Well yes, it does. In fact, we can know without even looking because if you recall, this network is defined ahead of time as reciprocal, meaning if A is friends with B, B is friends with A. We can remember this because the graph we are using is undirected. This means that the resulting matrix is going to be symmetric. Symmetric matrices are those that, when flipped along the diagonal (as shown in Figure Table 6.5, the two sides of the matrix will be mirror images of each other.\n\n\n6.2.2 Applying Your Knowledge: Calculating Average Nearest Neighbor Degree from the Adjacency Matrix\nWe can calculate the node metric known as average nearest neighbor degree (\\(k^{nn}_i\\)), covered in the graph metrics lesson, from the graph’s adjacency matrix in a straightforward way.\nLet \\(k_i\\) be the sum of the i\\(^{th}\\) row in the graph adjacency matrix (\\(\\sum_j a_{ij}\\)), which gives us node i’s degree, which we refer to as \\(k_i\\). So for the other nodes in the network j, l, m, and so forth, the same formula gives us their degrees, which we write as \\(k_j\\), \\(k_l\\), \\(k_m\\), and so on. We just go to the corresponding row in the matrix and sum across the columns. Taking the row sums of the entire adjacency matrix, then this gives us the degree set of the entire graph.\nOnce we have all the degrees for each node in the graph, the calculation of each node’s average nearest neighbor degree simplifies to #Equation 6.1:\n\\[  \n  \\bar{k}_{nn(i)} = \\frac{1}{k_i} \\sum_i a_{ij} k_j\n\\tag{6.1}\\]\nWhere \\(a_{ij}\\) are cells in the adjacency matrix, so \\(a_{ij} = 1\\) if nodes i and j are neighbors and equals zero otherwise. It is easy to see that this formula will first compute the sum of degrees of the neighbors of node i only (because the degrees of the nodes that are not i’s neighbors will be set to zero when each \\(k_{j}\\) is multiplied by \\(a_{ij}\\)) and then it will divide this total by i’s degree (\\(k_i\\)).\n\n\n6.2.3 The Asymmetric Adjacency Matrix\nConversely, a directed graph describing a network of asymmetric or anti-symmetric ties will create an asymmetric matrix. Saying a matrix is asymmetric means that the values contained in the upper and lower triangles of the matrix do not mirror each other. In other words, In an asymmetric matrix the cell values are not necessarily the same (the relationship is not necessarily equivalent) between every pair of cases.\nFigure 8.2 shows an example directed graph. The corresponding asymmetric adjacency matrix is shown in Table 6.6. Note that while some relationships (such as between node A and B) are reciprocated, not all connections in the network are reciprocated. Node G sends ties to D and F, but does not receive any ties back. In the resulting matrix, A to B and B to A each have a 1 listed for the value, while G to D and G to F also have a value of 1. However, the cells corresponding to F to G and D to G each have a value of 0 because the ties are unreciprocated. These unreciprocated ties make the resulting matrix asymmetric. The two halves across the diagonal are no longer mirror images, but contain different entries.\nWhy are the ties are not reciprocated? You might remember from our lessons on types of ties and types of graphs, but it is because of the type of data that the graph and matrix are representing. For example, the matrix in Table 6.6 and graph shown in Figure 8.2 could represent a intramural basketball club where they ask everyone in the club who they like to have as a teammate. Not everyone could agree that they like to have one another as teammates, and the matrix and graph in Figure 8.2 would represent that. In this case, a node like G and E look really lonely since they have nobody who wants to play with them. However, if the tie were to be about advice, such that actually G gives advice to D and F, but does not take their advice back, G (and E) now look like respected figures in the network.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n1\n0\n0\n0\n1\n0\n\n\nB\n1\n–\n0\n1\n0\n0\n0\n\n\nC\n0\n1\n–\n0\n0\n0\n0\n\n\nD\n0\n1\n0\n–\n0\n0\n0\n\n\nE\n0\n0\n1\n1\n–\n0\n0\n\n\nF\n1\n0\n0\n0\n0\n–\n0\n\n\nG\n0\n0\n0\n1\n0\n1\n–\n\n\n\nTable 6.6: Asymmetric adjacency matrix corresponding to an directed graph.\n\n\n\n\n6.2.4 The Valued Adjacency Matrix\nRecall that weighted networks, in which the edges connecting actors have some kind of value range beyond zero one (e.g., representing the intensity of the relationship or the amount of interaction) are best represented using weighted graphs. Can we translate weighted graphs into matrix form? Yes! We can use square valued matrices to represent represent weighted graphs. A valued matrix is simply a type of square adjacency matrix where cell values could be any number. From the information on tie weights in the graph shown as Figure , we use the weight as an indication of the strength of the tie between the two actors. Thus, a tie is not simply absent or present, but falls along some scale ranging from absence to “maximum intensity,” which is a function of how the researcher thinks about the tie.\n\n\n\n\n\nFigure 6.1: A weighted graph.\n\n\n\n\nFor example, your best friend might be a tie strength of 1.0, while people you are no very close to are closer to zero. However, in the middle there are all types of people who fall along the spectrum between your best friend and someone you have never seen before. It might make sense if you are trying to study social influence that people’s best friend might have more influence on their choices than their 5th closest friend. What the weights for your 5th closest friend in this weighting system is a matter of research design which can be debated, but once the weights are determined, it is simple to incorporate them into a matrix.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n0\n0\n0\n1\n0\n\n\nB\n0\n–\n0.82\n0\n0\n0\n\n\nC\n0.22\n0.71\n–\n0\n0\n0.7\n\n\nD\n0\n0.33\n0\n–\n0.98\n0\n\n\nE\n0.58\n0.85\n0\n0.96\n–\n0.87\n\n\nF\n0.31\n0.39\n0.42\n0\n0.84\n–\n\n\n\nTable 6.7: Adjacency matrix for a weighted graph\n\n\nIn the Figure 6.1 case, the theoretical maximum is represented by a weight of 1.0. This is shown in Table Table 6.7. Note that we follow the same procedure as that used for constructing the asymmetric adjacency matrix, only that this time, we check on the weight of the edge that actor i sends to actor j. When there is no edge between actors, we put a zero in the corresponding cell of the valued adjacency matrix as before. For instance, to fill the row corresponding to actor D, we see that they do not send a tie to actor A, so we put a zero in the cell corresponding to row 4 and column 1). We also observe that they indeed send a tie to actor B and the corresponding weight of this tie is 0.33, so we input that number into cell corresponding to row 4 and column 2 in the table. We continue until all the entries for actor D (row 4 of the valued adjacency matrix) are filled and keep on going row by row until we have covered all the actors in the network.\n\n\n6.2.5 The Signed Adjacency Matrix\nSigned graphs like those discussed in the graph theory lesson, can also be represented using signed matrices. Signed matrices are just like valued matrices, with the stipulation that values are restricted to the couplet of -1 (to indicate a negative tie) and +1 to indicate a positive tie. For instance, the valued adjacency matrix for the signed graph shown in Figure 14.3 is shown in Table 6.8.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\n\n\n\n\nA\n–\n-1\n-1\n1\n1\n\n\nB\n1\n–\n1\n-1\n-1\n\n\nC\n-1\n-1\n–\n-1\n1\n\n\nD\n1\n-1\n1\n–\n1\n\n\nE\n-1\n-1\n-1\n-1\n–\n\n\n\nTable 6.8: Adjacency matrix for a signed graph.\n\n\nAs before each cell in the matrix encodes the sign of the relationship that goes from the row node to the column node. So we see that node A sends a positive ties to nodes D and E and sends negative ties to nodes B and C. This node has an equal number of negative and positive links. This contrasts to the row representing node E, which shows that they send negative links to all the other nodes in the graph. Perhaps they are a difficult person or some kind of downer. Reading across the rows tell us something about what each actors “gives off” in the network of valenced relations. This could be positivity or negativity. Reading across the columns, on the other hand, tell us what each actors receives from others in the network. For instance, node B receives negative links from every other node in the graph, making them the only universally disliked actor in the system. Meanwhile, while E gives off all negative links, they receive mostly positive links except for the one coming from node B.\nTable 6.9 summarizes the linkage between the different types of graphs and the different types of matrices used to represent them.\n\n\nTable 6.9: Specific Terms Referring to Graphs and Matrices based on the type of tie and associated network we are studying. Keep them straight!\n\n\nType of Tie\nGraph\nAdjacency Matrix\n\n\n\n\nSymmetric\nUndirected\nSymmetric\n\n\nAsymmetric\nDirected\nAsymmetric\n\n\nWeighted\nWeighted\nValued\n\n\nValenced\nSigned\nSigned"
  },
  {
    "objectID": "lesson-from-graph-to-matrix.html#references",
    "href": "lesson-from-graph-to-matrix.html#references",
    "title": "6  From Graph to Matrix",
    "section": "References",
    "text": "References\n\n\n\n\nForsyth, Elaine, and Leo Katz. 1946. “A Matrix Approach to the Analysis of Sociometric Data: Preliminary Report.” Sociometry 9 (4): 340–47.\n\n\nMartin, John Levi. 2017. “The Structure of Node and Edge Generation in a Delusional Social Network.” Journal of Social Structure 18 (1): 1–21."
  },
  {
    "objectID": "lesson-indirect-connections.html#sec-paths",
    "href": "lesson-indirect-connections.html#sec-paths",
    "title": "7  Indirect Connections",
    "section": "7.1 Paths",
    "text": "7.1 Paths\nTake for instance, the undirected graph shown in Figure 7.1. In the Figure, nodes A and B are not adjacent (they are part of a null dyad). However, A can still reach B using a graph-theoretic path, written \\(A-B\\). For instance, if A wanted to pass along a message to B without it having to go through the same person twice, they could tell D (using the edge \\(A \\leftrightarrow D\\)), who could tell E (using the edge \\(D \\leftrightarrow E\\)) who could the tell B (using the \\(E \\leftrightarrow B\\) edge). As noted above, this sequence of edges, namely \\(A \\leftrightarrow D, D \\leftrightarrow E, E \\leftrightarrow B\\) defines a path between nodes A and B.\nTo define a path, we could also write the names of the edges separated by commas while omitting the arrows as in: \\(A-B = \\{AD, DE, EB\\}\\). In a path, the first node listed in the sequence is called the origin node and the last node listed is called the destination node. The nodes “in between” the origin and destination nodes in the path are called the inner nodes. Together, the origin and destination nodes are referred to as the end nodes of the path. So in the path \\(A-B = \\{AD, DE, EB\\}\\), node A is the origin node, node B is the destination node, nodes D and and E are the inner nodes, and both nodes A and B are the end nodes.\nSo what is a path?\nIn a graph, a path between two nodes A and B is a sequence of nodes and edges, such that the sequence begins with node A and ends with node B and only goes through each of the inner nodes in the path once.\nNote that every path is actually a subgraph of the original graph as defined in ?sec-graph-subgraphs! Except that the subgraph that defines a path will always contain a sequence of vertices and edges connected in a line."
  },
  {
    "objectID": "lesson-indirect-connections.html#properties-of-paths",
    "href": "lesson-indirect-connections.html#properties-of-paths",
    "title": "7  Indirect Connections",
    "section": "7.2 Properties of Paths",
    "text": "7.2 Properties of Paths\nIn graph theory, paths between pairs of nodes have some unique properties:\n\nFirst, as mentioned in the definition, paths do not repeat nodes. This means that each of inner nodes in the path (e.g., the nodes that are not the origin and destination ones) only appears exactly twice when we list the edges that make up the path, like nodes D and E in the AB in the path listing \\((AD, DE, EB)\\). The origin and destination nodes, in contrast, appear exactly once.1\nSecond, while it might not seem immediately obvious, because paths do not repeat nodes, they also do not repeat edges. As we will see later, this property helps differentiate paths from other less restricted types of edge sequences we may define in a graph featuring two nodes at the ends of it.\n\nThird, paths are characterized by their length. The length of a path (\\(\\mathcal{l}_{ij}\\), where \\(i\\) is the origin node and \\(j\\) is the destination node) is given by the number of edges included in it. So the length of the path \\((AD, DE, EB)\\) is \\(\\mathcal{l}_{AB} = 3\\) because there are three edges in the path.2\nFinally, there may be multiple paths connecting the same pair of nodes. For instance, In Figure 7.1 node A can also reach node B via the path \\((AC, CE, EB)\\) which is distinct from the one shown in red the Figure, but which also counts as a proper graph theoretic path (intervening nodes only appear twice in the listing, and the end nodes only appear once).\n\nThis leads us to the graph theory definition of connectivity for pairs of nodes:\n\nIn a graph, two nodes A and B are connected if there is at least one path (of any length), featuring node A as the origin node and node B as the destination node. Otherwise, the two nodes A andB are disconnected.\nWhen a node can indirectly connect to another node via a path, we say that that the origin node can reach the destination node, or that the destination node is reachable by the origin node.\n\nNote that once we understand the graph theory notion of connectivity, it becomes clear that the concept of adjacency is a special (limiting) case of connectivity: Two nodes i and j are adjacent when there is a path of \\(\\mathcal{l_{ij}} = 1\\) between them!\nIn a simple graph with no isolates, like that shown in Figure 7.1, it is easy to see that all node pairs are connected via a path of some length."
  },
  {
    "objectID": "lesson-indirect-connections.html#connected-and-disconnected-graphs",
    "href": "lesson-indirect-connections.html#connected-and-disconnected-graphs",
    "title": "7  Indirect Connections",
    "section": "7.3 Connected and Disconnected Graphs",
    "text": "7.3 Connected and Disconnected Graphs\nLook at Figure 7.1 again. Are there any disconnected pairs of nodes in the graph? The answer is no. If you pick any pair of nodes, they are either directly connected, or indirectly connected to one another via a path of some length. So how can we get two nodes to be disconnected in a simple undirected graph? The answer is that there has to be some kind of “gap” splitting the graph into two or more pieces, so that some set of nodes can no longer reach some of the other ones.\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) A disconnected graph.\n\n\n\n\nFigure 7.2: A connected graph versus a disconnected graph.\n\n\nFor instance, if you compare the undirected graphs Figure 7.2 (a) and Figure 7.2 (b) you can appreciate the difference between a connected graph and a disconnected graph. The difference between Figure 7.2 (a) and Figure 7.2 (b) is that the Figure 7.2 (b) graph is a subgraph of the Figure 7.2 (a) graph, in which the \\(DF\\) edge has been removed (an edge-deleted subgraph according to what we discussed in Chapter 4): \\(G(b) = G(a) - DF\\). Removing this edge has resulted in \\(G(b)\\) being disconnected. There is no way we can trace a path connecting any of the vertices in the \\(\\{F, G, H, I\\}\\) subset with any of the vertices in the \\(\\{A, B, C, D, E\\}\\) subset. This is different from Figure 7.2 (a) where we can trace a finite path between any two vertices, even if it is very long. Thus, we can come up with a formal definition of graph connectivity:\n\nA graph is connected if there exists a (finite) path between each pair of nodes.\nA graph is disconnected if there does not exist a path connecting every pair of nodes"
  },
  {
    "objectID": "lesson-indirect-connections.html#components-and-the-giant-component",
    "href": "lesson-indirect-connections.html#components-and-the-giant-component",
    "title": "7  Indirect Connections",
    "section": "7.4 Components and the Giant Component",
    "text": "7.4 Components and the Giant Component\nWhen a graph goes from connected (like Figure 7.2 (a)) to disconnected (like Figure 7.2 (b)) it is split into a number of subgraphs that are themselves connected. For instance, in Figure 7.2 (b) the set nodes \\(\\{A, B, C, D, E\\}\\) form a connected subgraph of the larger disconnected graph. A connected subgraph that is part of a larger disconnected graph is called a component. In Figure 7.2 (b) there are two components, one formed by the connected subgraph with node set \\(\\{A, B, C, D, E\\}\\), and the other formed by the connected subgraph with node set \\(\\{F, G, H, I\\}\\).\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) A disconnected graph with three components.\n\n\n\n\n\n\n\n(c) A disconnected graph with a giant component.\n\n\n\n\nFigure 7.3: A series of connected and disconnected graphs displaying components and the giant component.\n\n\nIt is possible to split a graph into multiple components not just two. For instance, take a look at Figure 7.3 (a). If we were to generate a subgraph from Figure 7.3 (a) by deleting the red and purple edges, we would end up with the Figure 7.3 (b); this graph is disconnected, and it features three separate components (connected subgraphs). If were to delete just the red edges from Figure 7.3 (a) and keep the purple edge, we would end up with the graph shown in Figure 7.3 (c). Like the Figure 7.3 (b), Figure 7.3 (c) is also disconnected, but it is split into two not three components.\nNote that one of the connected components in Figure 7.3 (c) is way larger than the other one. The bigger connected component of graph Figure 7.3 (c) is of order ten, but the smaller component is only of order four. When a disconnected graph is split into multiple components, the component containing the largest number of nodes (the connected subgraph of the highest order) is called the giant component."
  },
  {
    "objectID": "lesson-indirect-connections.html#sec-bridges",
    "href": "lesson-indirect-connections.html#sec-bridges",
    "title": "7  Indirect Connections",
    "section": "7.5 Bridges",
    "text": "7.5 Bridges\nWe went from the connected graph shown in Figure 7.3 (a) to the disconnected graphs in Figure 7.3 (b) and Figure 7.3 (c) by removing or deleting particular edges (like the red and purple ones in Figure 7.3 (a)). These edges are clearly more important than the other blue ones, because they are responsible for keeping the whole graph together as a connected structure. Edges whose removal results in a graph going from being connected to disconnected are so important that they have a special name: bridges.\nThus, in a graph the bridges are (smallest in terms of cardinality) set of edges whose deletion results in the graph becoming disconnected. For instance, in Figure 7.2 (a) the \\(DF\\) edge is a bridge. In Figure 7.3 (a), on the other hand, any two of the threesome formed by the two red edges and the purple edge count as bridges. Deleting any two of these three edges disconnects the graph.\nNote that in contrast to Figure 7.2 (a), Figure 7.3 (a) doesn’t have any one edge that is the bridge (the cardinality of the set of bridges is one). Instead, it takes deleting at least two edges from the red and purple set to disconnect it. For instance, if we were to remove just the purple edge, Figure 7.3 (a) would still be connected, and it would look like Figure 7.4 (a), which is a connected graph. The same goes for removing just one of the red edges. If were to do that, we would end up with the graph shown in Figure 7.4 (b), which is also a connected graph.\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) Another connected graph.\n\n\n\n\nFigure 7.4: Two connected graphs."
  },
  {
    "objectID": "lesson-indirect-connections.html#shortest-paths",
    "href": "lesson-indirect-connections.html#shortest-paths",
    "title": "7  Indirect Connections",
    "section": "7.6 Shortest Paths",
    "text": "7.6 Shortest Paths\nConsider the multiple ways node B can reach node C via a path in Figure 7.1. One possibility is the path defined by the edge sequence \\((BE, ED, DA, AC)\\). Another possibility is the path defined by the sequence \\((BE, ED, DC)\\). Yet another possibility is the path given by the edge sequence \\((BE, EC)\\). What’s the difference between these paths? Well, for the first one, \\(\\mathcal{l}_{BC}^{(1)} = 4\\), for the second one \\(\\mathcal{l}_{BC}^{(2)} = 3\\) and for the last one \\(\\mathcal{l}_{BC}^{(3)} = 2\\). The three paths are of different length, even though all three are proper graph-theoretic paths (they do not repeat inner nodes) and even though all three feature the same actors as the end nodes.3\nLet’s say B was a spy who needed to send an urgent message to C even though they don’t know C directly. If B wanted the message to get to C in the fastest way, they would use the shortest path between them, which in this case is \\((BE, EC)\\). For any two nodes in the network, the shortest path is the smallest existing path (in terms of path length) that has those nodes as the end nodes.^[This means that for every pair of connected actors in the network, we can define a shortest path between them.4\nSometimes, as with actors A and B in Figure 7.1), there will be multiple shortest paths between two pairs of nodes, because we end up with two or more paths that are “tied” in length (and thus all count as “shortest” paths). So for nodes A and B in Figure 7.1, the two shortest paths connecting them are of length 3: \\((AC, CE, EB)\\) and \\((AD, DE, EB)\\).\nObserve that, in Figure 7.1 if B really wanted to reach C via the shortest path, they will always have to go through node E. When this is the case, we say that E stands in the shortest path between B and C. So B is highly dependent on E to reach C. This makes E’s position in the network particular important for B, because they play the role of go-between or broker between B and the other actors in the network.\nBecause of this connection to communication efficiency and inter-mediation, shortest paths figure prominently in various measures of node position, called centrality measures we will deal with in Chapter 20. One such measure, called betweenness centrality is based on counting the number of shortest paths between every other pair of nodes that a given node stands on.\n\n\n\n\n\nFigure 7.5: An undirected graph showing a cyle beginning and ending with node A (red edges).\n\n\n\n\nThe length of the shortest path between two pair of non-adjacent vertices in the network can be thought of as the “degrees of separation” between them. This is also called the geodesic distance between two nodes. For instance, the shortest path between nodes A and B is of length three, so A is three degrees of separation away from B. As we saw in this example, there does not have to be only one shortest path between two nodes. Actors in the network can be connected via multiple distinct shortest paths as are A and B in Figure 7.1)."
  },
  {
    "objectID": "lesson-indirect-connections.html#sec-cycles",
    "href": "lesson-indirect-connections.html#sec-cycles",
    "title": "7  Indirect Connections",
    "section": "7.7 Cycles",
    "text": "7.7 Cycles\nConsider the edge sequence \\((AD, DE, EC, CA)\\) highlighted in red in Figure 7.5). What’s so special about it? Well, it looks like a path, because the inner nodes are only listed twice and so all the edges are unique. However, both the origin and destination nodes are the same!\nSomething like this sometimes happens with gossip. You start a rumor about someone by telling somebody else, and then a third person tells you the rumor that you started as if it was news to you! In this case, the rumor has traveled in the network via what is called a cycle.\nA cycle is path,of length three or larger, featuring the same node in both the origin and destination slots.\nSome directed graphs, are distinctive because they don’t have any cycles. It doesn’t matter how hard you try, or how long you stare at them, there is no way you will find a directed path of minimum length three that begins and ends with the same node. Directed graphs completely lacking in cycles are called directed acyclic graphs (DAGs). Note that tree graphs made up of anti-symmetric links, such as the one shown in Figure 8.7 are DAGs."
  },
  {
    "objectID": "lesson-indirect-connections.html#trails-and-walks",
    "href": "lesson-indirect-connections.html#trails-and-walks",
    "title": "7  Indirect Connections",
    "section": "7.8 Trails and Walks",
    "text": "7.8 Trails and Walks\nNot every sequence of nodes and edges that begins with one node and ends in another counts as a proper graph-theoretic path between two nodes.\nFor instance, looking at Figure 7.1, we could imagine a sequence of edges that started with one node and ended in another one without repeating edges, but repeating nodes. For instance, the sequence \\((CA, AD, DC, CE, ED)\\) has node C as the origin node and node D as the destination node, and all the edges in the sequence are unique. However, the origin node C and the destination node D also appear in the intervening chain, which means that they are listed three times. This fails the path test for this edge sequence. Sequences like this, featuring all unique edges but repeated nodes, are called trails.\nIn the same way, we could imagine some kind of message traveling across the edge sequence \\((AD, DC, CE, ED, DC, EC, EB)\\). This sequence has A as the origin node and has B as the destination node. However, the sequence goes through nodes C, D, and E twice; moreover, the edge DC appears twice, as does the edge EC. This means nodes C, D, and E appear four times in the listing which also fails the “is this a path?” test for this sequence.\nArbitrary sequences of nodes and edges that begin with one node and end in another node but that feature both repeated nodes and edges along the way are called walks.\nWalks, trails, and paths, form a hierarchy of increasingly less restricted “travels” from one node to another via edges in a graph. Walks are the least restricted (they can include both repeated nodes and edges) and paths are the most restricted (they cannot include repeated nodes, which by implication also means they can’t include repeated links). Trails are in between. They are forbidden from repeating edges but can use multiple nodes more than once.\nThis means all paths are trails, and are trails are walks, but not all walks are trails, and not all trails are paths!"
  },
  {
    "objectID": "lesson-indirect-connections.html#keeping-walks-and-trails-apart",
    "href": "lesson-indirect-connections.html#keeping-walks-and-trails-apart",
    "title": "7  Indirect Connections",
    "section": "7.9 Keeping Walks and Trails Apart",
    "text": "7.9 Keeping Walks and Trails Apart\nOne way to keep these distinctions straight is by thinking about different types of things that flow through a network and how they may use paths, trails, or walks (Borgatti 2005). Take for instance a virus. A virus doesn’t decide where to go. It just gets transmitted from person to person every time pairs of people (or multiple pairs if it’s a mass gathering) come into contact with one another. The same person can become exposed to the virus multiple times via different links (so the chain repeats nodes). In the same way, the same link (your best friend) can expose you to the virus multiple times (so the chain repeats edges). This means that viruses form transmission chains in social networks that look like walks: they repeat both nodes and edges. In this way, viruses are very much like money. If you buy something with a dollar, and the person you gave the dollar to buys something with it, the dollar can come back to you after flowing through the network for a while, so money also travels through networks via walks because it can repeat both nodes and edges.\nHow about trails? We already mentioned the idea of “six degrees of separation.” In the 1960s, the social psychologist Stanley Milgram (1967) designed an experiment where he sent almost two hundred packages containing a letter to a bunch of random people in Nebraska. The letter had the name of a stockbroker who lived in Boston and instructions that read: “If you know this person and where they live, send them this package; if you don’t know this person, forward it to someone you know who you think might know this person.” This is called the small world experiment. These letters thus began to flow through indirect connections in social networks. Any one “chain” of letters however would flow from one link in the network to another. Theoretically it could come back to a person who had already sent the letter (if one of the people forward in the chain didn’t know that person had sent the letter before), but no one would forward the letter back to the same person they had forwarded the letter before. This means that the packages in the small world experiment traveled via trails. They could repeat nodes (e.g., people) but not edges. Gossip spreads through the network via trails too. You can hear the same piece of gossip from different friends (meaning that you are a repeated node in the trail), but you won’t hear the same piece of gossip from the same friend (meaning that edges are not repeated), unless they had memory loss!\nSo the differences between the ways viruses and gossip travel via social networks should help you keep walks and trails apart."
  },
  {
    "objectID": "lesson-indirect-connections.html#local-bridges",
    "href": "lesson-indirect-connections.html#local-bridges",
    "title": "7  Indirect Connections",
    "section": "7.10 Local Bridges",
    "text": "7.10 Local Bridges\nIn a classic paper on “The Strength of Weak Ties,” Mark Granovetter (1973) developed the concept of a local bridge. Recall from section Section 7.5, that a bridge is an edge (or set of edges) that if removed would completely disconnect the graph. There is another way of thinking of bridges in the context of shortest paths and this is with respect to what happens to the connectivity between particular pairs of nodes in the graph when a specific edge is removed. From this perspective, a bridge is an edge (or set of edges) that if removed would increase the length of the shortest paths between two sets of nodes from a particular number to infinity since, in a disconnected graph, the length of the path between nodes that cannot reach one another is indeed infinity! (\\(\\infty\\)).\nBut of course, between a small number and infinity there’s a lot of in-between. That’s what the concept of a local bridge is intended to capture. A local bridge is an edge that if removed from the graph, would increase the length of the shortest path between a particular pair of nodes to a number that is less than infinity. This number is called the degree of the local bridge in question. Because a local bridge is always defined with respect to a particular pair of nodes, it is a triplet, involving two nodes and one edge.\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) Another connected graph.\n\n\n\n\nFigure 7.6: Two connected graphs.\n\n\nFor instance, in Figure 7.6 (a), with respect to nodes \\(A\\) and \\(H\\), the edge \\(HK\\) (pictured in purple) is a local bridge of degree 4. The reason for that is that, as shown in Figure 7.6 (b), when the edge \\(HK\\) is removed from the graph, the shortest path_ between nodes \\(H\\) and \\(K\\) increases from \\(l_{HK} =1\\) (\\(H\\) and \\(K\\) are adjacent in Figure 7.6 (a)) to \\(l_{HK} = 4\\), as given by the edge sequence \\(\\{GH, AG, AD, DK\\}\\) (pictured in red). Note that from the perspective of nodes \\(D\\) and \\(H\\), the purple \\(HK\\) edge is a local bridge of degree three, because \\(l_{DH} = 2\\) in Figure 7.6 (a) and \\(l_{DH} = 3\\) when the edge \\(HK\\) is removed from the graph in Figure 7.6 (b)."
  },
  {
    "objectID": "lesson-indirect-connections.html#references",
    "href": "lesson-indirect-connections.html#references",
    "title": "7  Indirect Connections",
    "section": "References",
    "text": "References\n\n\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nGranovetter, Mark S. 1973. “The Strength of Weak Ties.” American Journal of Sociology 78 (6): 1360–80.\n\n\nMilgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67."
  },
  {
    "objectID": "lesson-ties-and-graphs.html#symmetric-ties-and-undirected-graphs",
    "href": "lesson-ties-and-graphs.html#symmetric-ties-and-undirected-graphs",
    "title": "8  Types of Ties and Their Graphs",
    "section": "8.1 Symmetric Ties and Undirected Graphs",
    "text": "8.1 Symmetric Ties and Undirected Graphs\nNodes and edges are indeed the building blocks of a graph. However, types of relationships that the edges represent can change both how we understand the network conceptually and also what mathematical techniques we can apply to the graph when we compute graph metrics (the subject of a future lesson). The basic idea is that when we do network analysis, we want to map our understanding of the nature of the social relationships we are studying to the types of graphs we use to represent the network formed by the concatenation of those relationships.\n\n\n\n\n\nFigure 8.1: A undirected graph\n\n\n\n\nLet us assume that Figure 8.1 represents a network of people who spend time together. One way of building this network would be to ask people on your dorm room floor who are the people that they spend some amount of time (e.g., more than an hour a week) hanging out with. By definition the relation “spending time together” lacks any inherent directionality. Mutuality (or reciprocity) is built in by construction. It would be nonsensical for a person (say A) to claim that they spend time with another person (say B) and for B to say that they do not spend time with A. In social network analysis these types of ties are called symmetric ties.\nAccordingly, two people being in the same place at the same time (co-location), even if they do not one another, is an example of a symmetric tie. You also have the symmetric tie “being in the same class as” every other student that is also taking your Social Networks seminar this term. Note that, in this sense, all co-memberships (e.g., being in the same club or organization or being part of the same family) create symmetric ties among all actors involved (we will revisit this topic when talking about two-mode networks in another lesson). If I am a member of your family, you are also my family member; if we are both members of the soccer club, we are considered teammates. Social networks composed of symmetric ties are represented using undirected graphs like the one shown in Figure 8.1. When an undirected graph has no loops (edges connecting a node to itself), and there is only one edge connecting adjacent vertices to one another (the graph has no multiedges), it is called a simple undirected graph.\nNetworks composed of symmetric ties have some interesting properties. If we know that the relationship (R) linking two nodes A and B is symmetric, then only a single edge exists that links them, and it does not matter whether we call this edge AB or BA. The order does not matter. In this way, we can formally define as symmetric tie as one that lacks directionality; if a tie is symmetric, then if we know that A is related to B (the AB edge is part of the edge set of the graph), then we know by necessity that B is related to A.\nCan you think of other examples of symmetric ties? Is friendship, as culturally defined in the contemporary world, a symmetric tie?"
  },
  {
    "objectID": "lesson-ties-and-graphs.html#asymmetric-ties-and-directed-graphs",
    "href": "lesson-ties-and-graphs.html#asymmetric-ties-and-directed-graphs",
    "title": "8  Types of Ties and Their Graphs",
    "section": "8.2 Asymmetric Ties and Directed Graphs",
    "text": "8.2 Asymmetric Ties and Directed Graphs\nIn contrast to spending time together, being members of the same family, or being in the same place at the same time, some social ties allow for inherent directionality. Edges in these graphs are are called asymmetric ties. That is, one member of the pair can claim to have a particular type of social relationship with the other, but it is possible (although not necessary) that the other person fails to have the same relationship with the first.\nHelping or social support relations, are like this. For instance, you can help someone with their homework, or given them personal advice, but this does not necessarily mean that that person will return the favor. They may, or they may not. The point is that, in contrast to symmetric tie, mutuality or reciprocity is not built in by definition, but must happen as an empirical event in the world. We need to ask the other person to find out (or check their email logs). Can you think of other examples of asymmetric social ties?\n\n\n\n\n\nFigure 8.2: A directed graph\n\n\n\n\nReciprocity is an important concept in social network analysis. Some have said it is perhaps the most important concept for understanding human society (Gouldner 1960), which may be a bit of an exaggeration. Only asymmetric ties may have the property of being non-reciprocal or having more or less reciprocity. If I think you are my friend, I very much hope that you also think you are my friend. That said, sociologists have found that in many natural social settings this is not the case. Sometimes people think they are friends with others, but those other people disagree (Carley and Krackhardt 1996). For this reason, sociologists typically ask: if I do you a favor, would you do me a favor in the future? Additionally, sociologists often ask: if I treat you with respect, will you also treat me with respect? If I text you, will you text me back? If this is true, we have a level of reciprocity in our relationship.\nFor some ties, such advice or support, or friendship relations, reciprocity is all or none; it either exists or it does not. For instance, the friendship offer you extend to someone may be reciprocated (or not). In the same way, you can like someone and they may like you back (or not), like the notes you passed around in middle school. For other ties, such as communication ties (e.g., those defined by the amount of texting, or calling), reciprocity is a matter of degree, there may be more or less. For instance, you can text someone 10 times a day, but they may text you back only half of those instances. In all cases, reciprocity is at a maximum when the content of the relationship is equally exchanged between actors.\nCan you think of relationships in your life characterized by more or less reciprocity?\nJust like symmetric ties are represented using a particular type of graph (namely, an undirected graph), social networks composed of asymmetric ties are best represented by a type of graph called a directed graph (or digraphs for short). Figure 8.2 shows the point and line diagram picture of a digraph. What were simple lines for in the undirected graph shown in Figure 8.1 have been replaced with arrows indicating directionality. A node sends a relationship to the node that the arrow points to, which in turn receives the relationship. In a digraph, up to two directed arrows may link nodes going in both directions. When an undirected graph has no loops (edges connecting a node to itself), and there is only one edge connecting a sender node to a receiving node (the graph has no multiedges) like Figure 8.2, it is called a simple directed graph.\nIn a directed graph, for every edge, there is a source node and a destination node. So in the case of “A helps B” the source node is A and the destination node is B. In the case of “B helps A” the source node is B and the destination node is A. This means that in a directed graph, in contrast to a undirected one, the order in which you list the nodes when you name the edges matters. Thus, the edge AB is a different one from the edge BA. The first one may exist but the second one may not exist (edges in a directed graph are sometimes also called arcs). For instance, if Figure 8.2 were an advice network (Cross, Borgatti, and Parker 2001), on the other hand, we could say that H seeks advice from D, but D does not seek advice from H. This may be because D is higher in the office hierarchy or is more experienced than H, in which case lack of reciprocity may be indicative of an authority relationship between the two nodes.\nOne must always be careful when examining a directed network to make sure one properly understands the direction of the underlying social relationships!\n\n\n\n\n\nFigure 8.3: A symmetric directed graph\n\n\n\n\nThere are also some special types of directed graphs. For instance, if in directed graph, every pair of adjacent nodes is connected via a bi-directional, reciprocal relationship, as in Figure 8.3, then it is called as symmetric directed graph.\n\n8.2.1 Node Neighborhoods in Directed Graphs\nJust like in undirected (simple) graphs, each node in a directed graph has a node neighborhood. However, because now each node can be the source or destination for a asymmetric edges, this means that we have to differentiate the neighborhood of a node depending on whether the node is the sender or the recipient of a given link.\nSo, we say that a node j is an an in-neighbor of a node i if there is a directed link with j as the source and i as the destination node. For instance, in Figure 8.2, E is an in-neighbor of C, because there’s a asymmetric edge with E as the source and C as the destination.\nIn the same way, we say that a node i is an out-neighbor of a node j if there is a directed link with i as the source and j as the destination. For instance, in Figure 8.2, F is an in-neighbor of G, because there’s a asymmetric edge with G as the source and F as the destination\nFor each node, the full set of in-neighbors forms the in-neighborhood of that node. This is written \\(N^{in}(v)\\), where \\(v\\) is the label corresponding to the node. For instance, in Figure 8.2, the node set \\(N^{in}(D) = \\{B, E, G\\}\\) is the in-neighborhood of node D.\nIn the same way, the full set of in-neighbors defines the out-neighborhood of that node. This is written \\(N^{out}(v)\\), where \\(v\\) is the label corresponding to the node. For instance, in Figure 8.2, the node set \\(N^{out}(B) = \\{A, C, D\\}\\) is the out-neighborhood of node B.\nNote that typically, the set of in-neighbors and out-neighbors of a given node will not be exactly the same, and sometimes the two sets will be completely disjoint (they won’t share any members).\nNodes will only show up in both the in and out-neighborhood set when there are reciprocal or mutual ties between the nodes. For instance, in Figure 8.2, the out-neighborhood of node F is \\(\\{A\\}\\) and the in-neighborhood is \\(\\{A, G\\}\\). Here node A shows up in both the in and out-neighborhood sets because A has a reciprocal tie with F.\n\n\n8.2.2 Node Degree in Directed Graphs\nBecause in a directed graph, each node has two distinct set of neighbors, we can compute two versions of degree for the same node.\n\nin a directed graph, for any node i, we can count the number of edges that have a given node \\(v\\) as their destination node. This is also the cardinality of the in-neighborhood set of that node. This is called a node’s indegree and it is written \\(k^{in}_i\\), where i is the label corresponding to that node.\nAdditionally, in a directed graph, for any node i, we can count the number of edges that have a given node \\(i\\) as their source node. This is also the cardinality of the out-neighborhood set of that node. This is called that node’s outdegree and it is written as \\(k^{out}_i\\), , where i is the label corresponding to that node.\n\nFor instance, in Figure 8.2, \\(k^{out}_B = 3\\) and \\(k^{in}_B = 2\\). Node B has three outgoing ties (from nodes A, C, and D) and three incoming ties (from nodes A and D).\nCan you calculate what the indegree and outdegree of node D in Figure 8.2 is?\nThe graph theoretic ideas of indegree and outdegree have clear sociological interpretations. In a social network, for instance, a node having a large outdegree could indicate a sociable person (a person that likes to connect with others), while having a large indegree can indicate a popular person (e.g., a person lots of other people want to be friends with). In a later lesson we will see how to use a directed graph’s asymmetric adjacency matrix to readily compute the outdegree and indegree in real social networks.\n\n\n8.2.3 Functional Graphs\nThere is a special kind of digraph that is built by imposing the restriction that each node can only have a single out-neighbor, meaning that the outdegree of each node is equal to one. This is called a functional graph and has applications in the anthropological analysis of kinship exchange systems in less differentiated societies. For instance, ?fig-fund\n\n\n\n\n\nFigure 8.4: A symmetric directed graph\n\n\n\n\n\n\n8.2.4 Types of Nodes in Directed Graphs\nThis means that in a directed graph, there will typically be three types of (non-isolate) nodes (Harary, Norman, and Cartwright 1965):\n\nFirst, there will be nodes that receive ties but don’t send them. These are called receivers (like node C in Figure 8.2). For receiver nodes \\(k_{in} > 0\\) and \\(k_{out} = 0\\).\nSecond, there will be nodes that receive ties and also send out ties. These are called carriers (like nodes A and B in Figure 8.2. For carrier nodes, \\(k_{in} > 0\\) and \\(k_{out} > 0\\).\nFinally, there will be nodes that send ties but don’t receive them. These are called transmitters (like nodes E and G in Figure 8.2). For transmitter nodes, \\(k_{in} = 0\\) and \\(k_{out} > 0\\)."
  },
  {
    "objectID": "lesson-ties-and-graphs.html#sec-oriented",
    "href": "lesson-ties-and-graphs.html#sec-oriented",
    "title": "8  Types of Ties and Their Graphs",
    "section": "8.3 Anti-Symmetric Ties and Oriented Graphs",
    "text": "8.3 Anti-Symmetric Ties and Oriented Graphs\nThere is a particular type of directed relationship that has the property of only going in one direction. These are called anti-symmetric ties. Like asymmetric ties, anti-symmetric ties have a directionality (and thus source and destination nodes), but reciprocity is forbidden by definition. That means that if A is anti-symmetrically connected to B, then B cannot send the same type of tie back to A (although B may be connected, and typically is, to A via some other type of tie in a different network).\nA common example of anti-symmetric ties in political sociology are patron-client ties (Martin 2009). Patrons can have many clients, but it is impossible for client of a patron to also be a patron to the same person. Other types of anti-symmetric ties include authority relations such as hierarchical ties at work, or cross-generational relations like kinship ties in families.\nIn the first (authority relations) case, your boss is your boss, while you cannot be your boss’ boss. The same goes for armies and other command and control structures, giving orders to is an anti-symmetric relation. An officer who gives orders to another officer (and thus commands them) cannot by definition also receive orders from them. Thus, Figure 8.7 could be a network in which the anti-symmetric links are directed “gives orders to” (in an army or an office) relations, where the source node directs commands toward the destination node. So A is the top boss and commands B, C, and D. Node B, in their turn, gives orders to E, who is at the lowest level of the command and control structure, not commanding anybody in turn.\n\n\n\n\n\nFigure 8.5: An oriented directed graph\n\n\n\n\nIn the second (cross-generational) case, your parents are your parents (but you can only be a son or daughter to your parents), and your grandparents are their parents, and so forth. “Being the parent of” thus counts as an anti-symmetric relation as we define here; it only goes way (from parents to children) but it cannot come back from children towards parents. Teacher-student, coach-athlete, buyer-seller, parent-child are all examples of anti-symmetric relationships that can be depicted as tree graphs.\nYet another set of relations that can only be represented using anti-symmetric links are dominance relations. These include negative interactions like “bullying”—or “pecking” if we are talking about chickens Chase (1982). Obviously if somebody bullies somebody else then they cannot be bullied back at the same time (although it could happen that a person dominated by a bully can become dominated by the person they used to bully, as when George McFly stood up to Biff in Back to the Future, thus reversing the direction of the one-way anti-symmetric link going from \\(A\\) to \\(B\\) so that it goes from \\(B\\) to \\(A\\)).1\n\n\n\n\n\nFigure 8.6: An oriented directed graph drawn as a tree.\n\n\n\n\nWhat happens if we build a network composed of pure anti-symmetric ties? The resulting graph is a special type of directed graph called an oriented graph, like the one in Figure 8.5. An oriented graph is kind of the opposite of the kind of symmetric directed graph depicted in Figure 8.3. Whereas in Figure 8.3 every connected dyad is linked by a bi-directional reciprocal link, in the oriented graph shown in Figure 8.5, every connected dyad is linked only by a one-way non-reciprocal link, the link that would turn the asymmetrically connected dyads in Figure 8.5 into a reciprocal dyad is forbidden by the nature of the tie.\n\n\n\n\n\nFigure 8.7: A tree graph with thirteen nodes.\n\n\n\n\nAs we will see in Chapter 7, when considering connections between people that involve intermediaries, there is a special pattern called a cycle. This involves a node sending a directed tie to another node, which sends a directed tie to a third node. What makes it a cycle is that third node then sends a directed tie to the first node we began with. Recall from Section 10.3, that the three-person subgraph AAA(C) shown in Figure 10.17 forms a cycle. For instance, in Figure 8.5, the sequence of directed edges \\(ED, DG, GE\\) forms a cycle starting and ending in node \\(E\\). Can you spot any other cycles in Figure 8.5?\nOne feature of a network composed of only anti-symmetric relations is that its corresponding graph can always be drawn from top to bottom, starting (at the top) with the node that only sends but does receive any ties and ending (at the bottom) with nodes that only receive, but do not send, ties. This is called a tree layout. For instance, Figure 8.6 shows the same graph as in Figure 8.5, but laid out as a tree.\nFor instance, if Figure 8.6 was a network composed of dominance interactions between people like “bullying” or “beating in a fight,” then the tree layout of the oriented graph would reveal the relevant dominance hierarchy between people. So we would conclude that node \\(D\\) being a transmitter node as defined in Section 8.2.4, that is, a node that is not the receiver of any dominance interactions but only a sender of them, is the “strongest.” No one dominates \\(D\\).\nNote that if you are dominated by somebody else in a dominance hierarchy, while you may not be able to dominate the person who dominates you back (given the anti-symmetric constraint), you can always dominate somebody else. This dynamic creates chains of negative interactions, which is exactly how bullying networks work when observed by sociologists. Thus, in Figure 8.6, while \\(C\\) gets dominated by \\(D\\), they in turn bully \\(F\\) and \\(E\\). This creates dominance chains—technically known as a directed paths as we will see in Chapter 13—going from \\(A\\) to \\(C\\) to \\(F\\) and \\(E\\). Also note that in a dominance hierarchy, transmitter nodes like \\(\\{A, B, C, G\\}\\) in Figure 8.6 are both bullies and bullied, while receiver nodes are in the worst possible position only receiving negative interactions.\n\n8.3.1 Tree Graphs\nAlthough their names a similar, a tree graph is not the same as an oriented graph drawn as a tree. Instead, a tree graph is a special kind of oriented graph in its own right, obeying a few specific restrictions:\n\nEvery node is the source of only one directed edge and/or the destination of only one directed edge.\nA node can only send a tie to a node that does not receive ties from any other node.\nIf a node sends an anti-symmetric link to a second node, and that node sends an anti-symmetric link to a third node, this last node cannot send an anti-symmetric link back to the first node.\n\nAn oriented graph obeying these three restrictions, and thus counting as a tree graph, is shown in Figure 8.7. As per conditions (1) and (2), the indegrees of each node in the graph are restricted to be either zero or one. Transmitter nodes have indegree zero but oudegree equal to one or more, like \\(A\\) in Figure 8.7 (\\(k_{in}(A) = 0, k_{out}(A) = 3\\)). Receiver nodes have indegree one but outdegree zero like \\(\\{E, F, G, H, I, J, K, L, M\\}\\) in Figure 8.7. Finally carrier nodes indegree equal to one and outdegree of one or more like \\(\\{B, C, D\\}\\) in Figure 8.7.\nAs noted, your family tree is an example of a tree graph composed of anti-symmetric kin ties going from people in an older generation to those in a younger generation. For instance, A could be your grandmother, and B, C, and D could be her three daughters. If B was you mom, then you could be E (along with your siblings F and G) and your cousins H, I, J, K, L, M.\nGiven condition (3), a key property of tree graphs composed of anti-symmetric ties is that they can’t have any cycles. No node that sends a tie to another node can receive a tie back from a third node their neighbor points to! This is the reason why tree graphs are said to to be “acyclic” (they lack cycles).\nIn this case, tree graphs are a special case of a directed graph without cycles. These special type of directed graphs are also called directed acyclic graphs which is sometimes abbreviated as “DAG.” DAGs have applications in certain fields of statistics, causal analysis, and machine learning and have been made famous by Judea Pearl, a Professor of Computer science right here at UCLA."
  },
  {
    "objectID": "lesson-ties-and-graphs.html#references",
    "href": "lesson-ties-and-graphs.html#references",
    "title": "8  Types of Ties and Their Graphs",
    "section": "References",
    "text": "References\n\n\n\n\nCarley, Kathleen M, and David Krackhardt. 1996. “Cognitive Inconsistencies and Non-Symmetric Friendship.” Social Networks 18 (1): 1–27.\n\n\nChase, Ivan D. 1982. “Behavioral Sequences During Dominance Hierarchy Formation in Chickens.” Science 216 (4544): 439–40.\n\n\nCross, Rob, Stephen P Borgatti, and Andrew Parker. 2001. “Beyond Answers: Dimensions of the Advice Network.” Social Networks 23 (3): 215–35.\n\n\nGouldner, Alvin W. 1960. “The Norm of Reciprocity: A Preliminary Statement.” American Sociological Review, 161–78.\n\n\nHarary, Frank, Robert Z Norman, and Dorwin Cartwright. 1965. Structural Models: An Introduction to the Theory of Directed Graphs. Wiley.\n\n\nMartin, John Levi. 2009. Social Structures. Princeton University Press."
  },
  {
    "objectID": "lesson-tree-graphs.html#properties-of-tree-graphs",
    "href": "lesson-tree-graphs.html#properties-of-tree-graphs",
    "title": "9  Tree Graphs",
    "section": "9.1 Properties of Tree Graphs",
    "text": "9.1 Properties of Tree Graphs\nTree graphs have the interesting property that their number of edges \\(m\\) is always equal to their number of nodes \\(n\\) minus one. Thus, if a graph is a tree graph or order five, we know it must contain four edges (it must be of size four). For instance, in Figure 9.1, all the graphs have four links. If we were to add a fifth link connecting any pair of nodes to any of the graphs, it would create a cycle (of some length) and thus the graph would no longer count as a tree graph! Try it for yourself and see.\nIn equation form, if \\(G(n, m)\\) is a tree graph, then:\n\\[\n  m = n - 1\n\\tag{9.1}\\]\nUsing simple algebra, we can also solve for the order of a tree graph if we know the size:\n\\[\n  n = m + 1\n\\tag{9.2}\\]\nThis equation says that the order of a tree graph is equal to the number of edges plus one.\nNote that from Equation 11.4 in Section 11.5 that the sum of the degrees of an undirected graph equals twice the number of edges (\\(2m\\)). Applying this reasoning, we can see that there is a special formula for the sum of degrees of an undirected tree graph. The reason is that if we know that:\n\\[\n\\sum_i k_i = 2m\n\\tag{9.3}\\]\nAnd we also know that \\(m = n - 1\\) as per Equation 9.1, then substituting for \\(m\\) in Equation 9.4, gives us:\n\\[\n\\sum_i k_i = 2(n-1) = 2n - 2\n\\tag{9.4}\\]\nThus in a tree graph, the sum of degrees will always equal to twice the number of nodes minus two!\nTree graphs have four other unique properties.\n\nFirst, if \\(G\\) is a tree graph, then every node \\(V\\) in \\(G\\) is linked to every other node via a single path.\nSecond, the one path connecting each pair of nodes is unique, that is, a sequence of nodes of and edges that is distinctive for that node pair and does not repeat for any other pair.\nThird, removing even a single edge of a tree graph disconnects the graph. Every edge of a tree graph thus counts as a bridge as discussed in Chapter 7.\nFourth, following (3), if we disconnect a tree graph by removing an edge, the resulting connected components are also trees.\n\nA disconnected graph whose components are trees is called (you guessed it) a forest. Figure 9.2 is a forest.\n\n\n\n\n\n\n\n(a)\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\n\n\n\n(c)\n\n\n\n\nFigure 9.1: Tree graphs with five nodes\n\n\n\n\n\n\n\nFigure 9.2: A Forest with ten nodes and two components\n\n\n\n\nAs Figure 9.1 shows, tree graphs come in different configurations, some of which are of particular note."
  },
  {
    "objectID": "lesson-tree-graphs.html#the-line-graph",
    "href": "lesson-tree-graphs.html#the-line-graph",
    "title": "9  Tree Graphs",
    "section": "9.2 The Line Graph",
    "text": "9.2 The Line Graph\nNote for instance, that Figure 9.1(a) is just a straight path between two nodes. This graph counts as a tree as it is both connected and has no cycles. A graph like Figure 9.1(a) which are just one long path featuring some set of nodes is also called the line graph. Line graphs are distinguished by their order and can be referred as \\(L_n\\) where \\(n\\) is the number of nodes. Thus, \\(L_5\\) is the line graph shown in Figure 9.1(a); a line graph with five nodes.\nWhat are line graphs useful for? Well, they can be used to model the social phenomenon known as a the “telephone game.” We can set up people in a line graph in a laboratory, have nodes pass a message, piece of gossip or a story along the line and see how different the original message relayed by \\(A\\) is by the time it gets to \\(E\\). Obviously, the longer the number of edges in the line, the more distortion we should expect at the other end."
  },
  {
    "objectID": "lesson-tree-graphs.html#the-star-graph",
    "href": "lesson-tree-graphs.html#the-star-graph",
    "title": "9  Tree Graphs",
    "section": "9.3 The Star Graph",
    "text": "9.3 The Star Graph\nNote also the graph that is just a central node connected to all the other end-point nodes who are not themselves connected to one another, like Figure 9.1(d) also counts as a tree. This graph is sometimes called the star graph. Like line graphs, we can refer to star graphs by using a letter and a number indicating their order like \\(S_n\\). Thus, \\(S_5\\) is the star graph with five nodes as in Figure 9.1(d).\nStar graphs are useful for modelling centralized systems like the airport network we saw in Chapter 1. Such hub-spoke systems feature a central node (the hub) connected to a bunch of end-point nodes not connected to one another (the spokes), as when a big airport (like LAX) sends flights to smaller regional airports. In this system, LAX is the “hub” at the center of the star and the smaller airports are the “spokes” at the end."
  },
  {
    "objectID": "lesson-tree-graphs.html#caterpillar-graphs",
    "href": "lesson-tree-graphs.html#caterpillar-graphs",
    "title": "9  Tree Graphs",
    "section": "9.4 Caterpillar Graphs",
    "text": "9.4 Caterpillar Graphs\nTree graphs with an even number of nodes can sometimes be drawn like the ones in Figure 9.3. This is a line graph on top, with edges sticking out of each node in the line graph connecting to an equal number (half the other nodes in the graph) of end point nodes.\nBecause of the shape they form, these tree graphs are sometimes called caterpillar graphs, with the line graph at the top playing the role of the caterpillar’s “body” and the edges incident to the end point nodes the role of the “legs.” Figure 9.3(a) shows a caterpillar graph of order six and Figure 9.3(b) shows a caterpillar graph of order eight.\n\n\n\n\n\n\n\n(a) Six nodes\n\n\n\n\n\n\n\n(b) Eight nodes\n\n\n\n\nFigure 9.3: Caterpillar graphs\n\n\nLike other graphs, tree graphs can be directed and undirected. The graphs shown in Figure 9.1 and Figure 9.3 are undirected trees.\n\n\n\n\nBenjamin, Arthur, Gary Chartrand, and Ping Zhang. 2017. The Fascinating World of Graph Theory. Princeton University Press."
  },
  {
    "objectID": "lesson-dyads-and-triads.html#sec-dyads",
    "href": "lesson-dyads-and-triads.html#sec-dyads",
    "title": "10  Dyads and Triads",
    "section": "10.1 Dyads",
    "text": "10.1 Dyads\nIn a graph, every pair of nodes, whether joined by an edge or not, is referred to as a dyad. Essentially, a dyad is any subgraph of order two of a larger graph. Generally researchers only refer to dyads when describing features of the network, although it it important to remember that ties that do not exist, but could exist, may be socially meaningful.\nAdditionally, when may consider the relationship among sets of three actors, we describe this as a triad. Triads have very important sociological properties that we will explore in other lessons. Dyads, triads and larger motifs constitute the (lego-like) building blocks of social network.s For now however, the terms provide a language that we can use to describe parts of the graph.\n\n\n\n\n\nFigure 10.1: A connected dyad.\n\n\n\n\n\n\n\n\n\nFigure 10.2: A null dyad.\n\n\n\n\n\n10.1.1 Types of Undirected Dyads\nUsing Figure 8.1 as the reference graph \\((G)\\), we can define a subgraph \\((G')\\) containing only nodes A and B. This is shown in Figure 10.1. In the same way, we could define a subgraph containing only nodes D and E. This is shown in Figure 10.2). Each figure portrays the two types of dyads that can exist in a undirected graph. Either a dyad with two nodes share an edge, or they don’t.\nThe dyad shown in Figure 10.1, with two nodes linked by an edge, is called a connected dyad, while the dyad shown in Figure 10.2 is called a null dyad. Since there can only be two types of dyads an undirected graph, every pair of actors is either part of a connected or a null dyad. Both types of dyads are defined by subgraphs of the same order (two), but they are different in size. The null dyad is size zero, and the connected dyad is size one.\n\n\n10.1.2 Types of Directed Dyads\n\n\n\n\n\nFigure 10.3: An Asymmetric dyad.\n\n\n\n\n\n\n\n\n\nFigure 10.4: A mutual dyad.\n\n\n\n\nWhile in an undirected graph there can only be two types of dyads, in a directed graph like the one shown in Figure 8.2, there can three different kinds of dyads. The reason for this is that in a directed graph, connected dyads can be of two kinds. On the one hand, we can have a dyad like that of actors C and B in Figure 10.3, who are described as an asymmetrically connected dyad (or asymmetric dyad for short).1 This is because while C sends a tie to B, B does not send a tie back to C.\nIn contrast, actors A and B in Figure 8.2 can be described as a mutually connected dyad (or mutual dyad for short), their subgraph is shown in Figure 10.4. Actors A and B are part of a mutual dyad because A sends a tie to B, and B sends a tie back to A.2 Thus, in a directed graph representing a network of asymmetric ties, every pair of actors in the network can be classified as belonging to one of three types of dyads: Mutual, Asymmetric, and Null. An easy way to remember this is that this classification spells “MAN.”\nIt is important to keep distinct the notion of asymmetry when used to refer to types of edges in a graph, from when it is used to refer to types of dyads. For instance, a mutual dyad is composed of a pair of asymmetric edges! When used to refer to edges, the notion of asymmetry implies only directionality of the single edge, while when used to refer to dyads the notion of asymmetry implies non-reciprocity of the whole dyad.\nA lot of the time we collect social network information that has a directed basis. For instance, we ask people whether the “know” someone, or whether they consider somebody a “friend.” These type of network data are called nominations, and they are very common in social network analysis. For instance may ask an individual \\(E\\) (let’s call her Jennifer) whether they nominate another individual \\(C\\) (in their school, classroom, dorm, office) as a “friend” (let’s call her Mariah), and they say “yes I know Mariah,” or “Mariah is my friend.” As with the “I don’t know her” meme,3 sometimes it happens Jennifer says they know Mariah, or “nominates” Mariah as a friend, but Mariah says they don’t know Jennifer, or fail to nominate Jennifer as a friend back! This creates a situation in which most social network data sets, end up being composed of directed dyads with asymmetric friendship or acquaintance relations, even if the researcher thought they would end up with symmetric ties (Carley and Krackhardt 1996)."
  },
  {
    "objectID": "lesson-dyads-and-triads.html#triads",
    "href": "lesson-dyads-and-triads.html#triads",
    "title": "10  Dyads and Triads",
    "section": "10.2 Triads",
    "text": "10.2 Triads\n\n10.2.1 Types of Undirected Triads\n\n\n\n\n\nFigure 10.5: A null triad.\n\n\n\n\nWe could do the same thing we did with dyads (subgraphs of order two) with the different subgraphs of order three in an undirected graph. These are called undirected triads.\nTake for instance, the subgraph defined by nodes B, C, and F in the graph shown in Figure 8.1. This is shown in Figure 10.5. It shows three people who are not connected to one another! Like strangers in a park sitting on three different benches. This is called the null triad.\n\n\n\n\n\nFigure 10.6: A disconnected triad\n\n\n\n\nNow let’s define a subgraph using nodes A, C, and E. The resulting triad is shown in Figure 10.6. Now this looks like a pair of friends A and C, in the same room with a stranger (E) whom they are not acquainted with. You may have experienced this before at a social gathering. This is called the disconnected triad. It is disconnected, because the subgraph formed by the three nodes is disconnected, as defined earlier; there is no way that either A or C can reach E, given that E is an isolate in the subgraph.\n\n\n\n\n\nFigure 10.7: An open triad.\n\n\n\n\nWe can continue. Let’s define a subgraph from the larger graph in shown in Figure 8.1, but this time we will pick nodes A, B, and C. What comes out? This is shown in Figure 10.7. This time, there is one person, node A, who is acquainted with two other people, nodes B and C, but they don’t seem to know one another. It’s like when you have friends from work and friends from school who have never met. This is called the open triad, because even though the subgraph is connected (there are no isolate nodes like in Figure 10.6, there is a “open hole” in the triad separating nodes B and C. Perhaps A should introduce their friends to one another!\n\n\n\n\n\nFigure 10.8: A closed triad.\n\n\n\n\nOne last one. Let’s define a subgraph from Figure 8.1, but this time let’s pick nodes A, D, and F. The result is shown in Figure 10.8. Now we have three friends all of whom know one another! So there are three distinct pairs of relations in the triad: AD, AF and DF. It’s like that group of three friends that always seems to hang out together. This is called the closed triad, because there is no room to add more links to it. It is also called the closed triad because it is the configuration you get when you add a final link to the open triad (thus “closing” it).\n\n\n\n\n\nFigure 10.9: The four types of undirected triads.\n\n\n\n\nAs shown in Figure 10.9, in an undirected graph, there can only be these four types of triads. So every threesome of actors is part of a null, disconnected, open, or closed triad. All four triads are subgraphs of the same order (three), but they are different in size. The null triad is size zero, the disconnected triad is size one, the open triad is size two, and the closed triad is size three.\nDyads, triads, and subgraphs of higher order (called network motifs) are the building blocks of larger network structures in society (Milo et al. 2002)."
  },
  {
    "objectID": "lesson-dyads-and-triads.html#sec-dirtriads",
    "href": "lesson-dyads-and-triads.html#sec-dirtriads",
    "title": "10  Dyads and Triads",
    "section": "10.3 Advanced: Types of Directed Triads",
    "text": "10.3 Advanced: Types of Directed Triads\n\n\n\n\n\nFigure 10.10: A disconnected directed triad with one asymmetric link.\n\n\n\n\n\n\n\n\n\nFigure 10.11: A disconnected directed triad with one mutual link.\n\n\n\n\n\n\n\n\n\nFigure 10.12: An open directed triad with a directed line configuration.\n\n\n\n\n\n\n\n\n\nFigure 10.13: An open directed triad with an out-star configuration.\n\n\n\n\n\n\n\n\n\nFigure 10.14: An open directed triad with an in-star configuration.\n\n\n\n\n\n\n\n\n\nFigure 10.15: Open directed triads with one mutual.\n\n\n\n\n\n\n\n\n\nFigure 10.16: An open directed triad with two mutuals.\n\n\n\n\n\n\n\n\n\nFigure 10.17: A closed directed triad with a cycle configuration.\n\n\n\n\n\n\n\n\n\nFigure 10.18: A closed directed triad with an in/out star configuration.\n\n\n\n\n\n\n\n\n\nFigure 10.19: Closed directed triads with one mutual.\n\n\n\n\n\n\n\n\n\nFigure 10.20: Closed directed triads with two mutuals.\n\n\n\n\nJust in the same way we can enumerate all the directed triads that exist in a directed graph, we can do the same for subgraphs of order three, namely triads. However, now that we are talking about threesomes, things get more complicated because we have a larger number of combinations to deal with. Let us go through them.\nFirst, it is useful to think about what we are dealing with. First, with a subgraph of order three we have three “slots” in the structure to consider. Each of these slots is a directed dyad. Thus, a triad can also be thought of as a concatenation of three directed dyads. That means that in a directed dyad, each of the slots can only be in on of three states (just like regular directed dyads): It can be mutual (M), asymmetric (A), or null (N)!\n\n10.3.1 The Null Triad\nSo we know that right off the bat, one of the configurations is just going to be composed of three null dyads. Let’s call it \\(NNN\\), and is going to look like just like the null dyad in the directed case (see Figure 10.5.\n\n\n10.3.2 Disconnected Directed Triads\nAnother set of configurations is going to be composed disconnected directed triads that are going to feature one connected dyad and two null dyads. Let’s call them \\(NNC\\). However, while there was only one such configuration in the undirected triad case, there’s going to be two in the directed triad case, because there are two kinds of directed connected dyads (asymmetric and mutual). So, one of the disconnected directed triads is going to have two null dyads and one asymmetric connected dyad. It looks like Figure 10.10. The other one is going to have two null dyads and a mutual connected dyad. It’s going to look like Figure 10.11. That’s it for the two null dyad configurations. We have collected a total of three directed triadic configurations so far!\n\n\n10.3.3 Open Directed Triads\nNow let’s think about directed triadic configurations featuring two asymmetric directed edges and one null dyad. These are directed versions of the open triad we considered in the undirected case. Here things get a bit interesting because edge directionality generates distinct configurations even when the number of links within the triad is the same (in this case two). So let’s call this configuration set \\(AAN(*)\\), where the \\(*\\) will be substituted with a letter to distinguish between the different arrangements.\n\nIn one set up, we have a person who directs an asymmetric edge to another person, and this person directs an asymmetric edge to a third. This is called the directed line, so let’s call this triad \\(NNA(L)\\), where “L” stands for “Line.” It looks like Figure 10.12. Think of when someone tells you a secret and then you tell someone else.\nIn another set up, we have a person who directs two asymmetric edges at two people at the same time. This is called the out star, so let’ call this triad \\(NNA(O)\\), where “O” stands for “out-star.” It looks like Figure 10.13. Think of you sending a text to two of your friends on a groupchat.\nIn yet another set up, we have two people who direct two asymmetric edges to a third person at the same time. This is called (you guessed it) the in star, so let’ call this triad \\(NNA(I)\\), where “I” stands for “in-star.” It looks like Figure 10.14. Think of two of your friends performing an intervention on you.\n\nWith these three additional entries, our total set of directed triadic configurations has grown to six!\nNow, let’s consider cases where: (a) the triad is both connected and open, and (b) there is one mutual dyad in the triad. There are two of these cases (let’s call them \\(MAN1\\), and \\(MAN2\\)). These are less interesting because only difference is whether the asymmetric edge is going in one direction or the other direction. These cases are shown in Figure 10.15.\nFinally, let’s consider the case where: (a) the triad is both connected and open, and (b) there are two mutual dyad in the triad. There is only one case like this (let’s call it \\(MMN\\)), and is shown in Figure 10.16.\nNotably, these three additional configurations brings our total to nine (with six of these being versions of open directed triads)\n\n\n10.3.4 Closed Directed Triads\nNow, let’s consider the cases of closed directed triads. These are triads featuring three connected dyads. Once again, there was only one option in the undirected case, but there’s a much larger number in the directed case because dyads can be connected in two ways (asymmetric and mutual). Let us take the configurations featuring three asymmetric links and called the \\(AAA(*)\\), where the \\(*\\) will be substituted with a letter to distinguish between the different arrangements.\n\nIn one setup, we have a person who directs an asymmetric edge to another person, this person directs an asymmetric edge to a third, and this third person directs an asymmetric edge back to the first person! As we will see later, this is called a cycle, so let’s call this triad \\(AAA(C)\\), where “C” stands for “cycle.” It looks like Figure 10.17.\nIn another setup, we have a person who directs two asymmetric edges to two other people, who are themselves connected by an asymmetric edge. Let’s call this triad \\(AAA(I/O)\\). Why this weird name? Well if you look at Figure 10.18, this triad combines both the in and out start configurations! In the Figure, node A is the out-star and node C is the in-star. Hence \\(AAA(I/O)\\), where “I/O” stands for “in/out.”\n\nSo these two configurations brings our total to eleven so far! We are almost getting there.\nNow let’s consider directed triadic configurations that are: (a) closed and (b) feature exactly one mutual dyad. Since the triad is closed, we therefore know that the other two non-mutual dyads will be joined by an asymmetric link (no null dyads). Let’s call this generic set \\(MAA(*)\\) triads, where the \\(*\\) will be substituted with a letter to distinguish between the different arrangements.\nLet’s ignore the mutual dyad and focus on the nodes joined by asymmetric links (\\(AA\\)). We know from our consideration of open triads containing asymmetric links (Figure 10.12, Figure 10.13, and Figure 10.14 above) that there are only three ways nodes in a triad can be connected via asymmetric links: 1) the line, 2), the out-star, and the in-star. So that’s exactly how many triads of these type are there. We can call them \\(MAA(L)\\), \\(MAA(O)\\), and \\(MM(I)\\) and they are shown in Figure 10.19.\nThese three additional configurations bring our total number of directed triadic configurations to fourteen! Are we done counting directed triad types?\nWe aren’t. We still have to consider a (final) set of triadic configurations, namely, closed triads containing at least two mutual dyads. Now, since these triads will all have two mutuals and are closed, their only distinguishing feature will be the nature of the third connected dyad. Since connected dyads can only take on of two states (Asymmetric or Mutual), then we know there will also be two subtypes: \\(MMA\\) (two mutual dyads with a third asymmetric dyad), and \\(MMM\\) (the triad with all mutual dyads!). These are shown in Figure 10.20. So, with these two triads, the true (and final) number of directed triadic configurations is sixteen!\n\n\n\n\n\nFigure 10.21: All Sixteen Directed Triad Configurations\n\n\n\n\nThe full sixteen-member set of possible directed triadic subgraphs is shown in Figure 10.21."
  },
  {
    "objectID": "lesson-dyads-and-triads.html#references",
    "href": "lesson-dyads-and-triads.html#references",
    "title": "10  Dyads and Triads",
    "section": "References",
    "text": "References\n\n\n\n\nCarley, Kathleen M, and David Krackhardt. 1996. “Cognitive Inconsistencies and Non-Symmetric Friendship.” Social Networks 18 (1): 1–27.\n\n\nMilo, Ron, Shai Shen-Orr, Shalev Itzkovitz, Nadav Kashtan, Dmitri Chklovskii, and Uri Alon. 2002. “Network Motifs: Simple Building Blocks of Complex Networks.” Science 298 (5594): 824–27."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#the-graph-degree-set",
    "href": "lesson-degree-based-graph-metrics.html#the-graph-degree-set",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.1 The Graph Degree Set",
    "text": "11.1 The Graph Degree Set\nComputing the degree of each node in the network gives us a vector (called k), containing the degree of each node. The vector is of “length” n where this is the number of nodes in the network. This is called the graph’s degree set, written k.\nWhat is a vector? A vector is a sequence of numbers. Thus, \\((1, 2, 3, 4)\\) is a vector, and so is \\((0, 1, 1, 0, 1, 1, 1)\\) and \\((0.23, 0.39, 0.89)\\). The length of a vector is the numbers of elements in it. Thus, the length of the vector \\((1, 2, 3, 4)\\) is 4 and the the length of the vector \\((0, 1, 1, 0, 1, 1, 1)\\) is 7. When vectors are considered as sets of numbers, the length of the vector is equivalent to the cardinality of the set defined by the vector.\nAs you may have already figured out, there are as many members of this set as there are nodes in the graph, so the cardinality of the degree set is the same as that of graph’s node set \\(|\\mathbf{k}| = |V|\\).\n\n\n\n\n\nFigure 11.1: A undirected graph\n\n\n\n\nLet’s consider the graph shown in Figure 11.1 again. The graph’s degree set k is shown in Table 11.1.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 11.1: The degree set of an undirected graph."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#the-graph-degree-sequence",
    "href": "lesson-degree-based-graph-metrics.html#the-graph-degree-sequence",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.2 The Graph Degree Sequence",
    "text": "11.2 The Graph Degree Sequence\nWhen we list the members of the degree set (each node’s degree) in decreasing order from bigger to smaller, this is called the graph’s degree sequence, and it is written d. Every graph has its own degree sequence, but graphs with very different structure (in terms of other graph metrics) can have the same degree sequence (Kim et al. 2009).\nIt is easy to see that if we order the values of the degree set from higher to lower, we would obtain the following degree sequence:\n\n\n\n\n\n\n\n5\n4\n4\n4\n3\n3\n3\n3\n3\n\n\n\nTable 11.2: The degree sequence of an undirected graph."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#minimum-and-maximum-degree",
    "href": "lesson-degree-based-graph-metrics.html#minimum-and-maximum-degree",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.3 Minimum and Maximum Degree",
    "text": "11.3 Minimum and Maximum Degree\nWhen studying a network, we are usually interested in the range of connectivity of people. What is the most connections somebody has in the network? What is the smallest set of neighbors someone has? These two graph metrics are called the maximum and minimum degree respectively. And are written \\(k_{max}\\) and \\(k_{min}\\). They are given by taking the maximum or the minimum values of the graph degree set describing the network. Thus if \\(\\mathbf{k}\\) is the vector containing each node’s degree, then the minimum and maximum degrees are given by:\n\\[\n  k_{max} = max(\\mathbf{k})\n\\tag{11.1}\\]\n\\[\n  k_{min} = min(\\mathbf{k})\n\\tag{11.2}\\]\nWhere \\(max(\\mathbf{k})\\) says “give me the largest number in the vector k, and \\(min(\\mathbf{k})\\) says”give me the smallest number in the vector k. To go back to our running example of the graph shown in Figure 11.1, it is easy to see from the degree set shown in Table 11.1, that for this network, \\(k_{max} = 5\\), and \\(k_{min} = 3\\)."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#degree-range",
    "href": "lesson-degree-based-graph-metrics.html#degree-range",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.4 Degree Range",
    "text": "11.4 Degree Range\nThe arithmetic difference between \\(max(\\mathbf{k})\\) and \\(min(\\mathbf{k})\\) gives us a sense of the heterogeneity or gap between the connectivity of the best connected and the least well connected nodes in the graph. This is called the graph’s degree range, and it is written \\(k_r\\):\n\\[\nk_r = k_{max} - k_{min}\n\\tag{11.3}\\]\nIn this case, \\(k_r = 5 - 3 = 2\\), which tell us than in this graph, the difference between the best and most poorly connected nodes is not really that large. Most nodes have a fair number of links to others. In real world networks, like the graph in a social medial platform like Meta, the degree range can be pretty big, since the maximum degree can be in the millions of followers, and the minimum degree can be zero."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#sec-degsum",
    "href": "lesson-degree-based-graph-metrics.html#sec-degsum",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.5 The Sum of Degrees",
    "text": "11.5 The Sum of Degrees\nNote that if we know the degree set of an undirected graph, we can figure out how many total edges there are in the graph. The reason for this is that the degree of each node can be defined in terms of the total number of edges incident on the node (as we noted earlier). So that means that if we know each node’s degree, then we can also know how many total edges there are in the graph.\nThe first step we need to do is compute the sum of degrees of each node in the graph. This quantity is written \\(\\sum_i k_i\\). For the degree set of the graph shown in Figure 11.1, we can see that the sum of the degrees \\(\\sum_i k_i = 32\\) (go ahead use a calculator to add up all the numbers in Table 11.1).\nIf you go back to the graph shown in Figure 11.1 and count the number of lines, you can see that there are 16. So summing up the degrees of each node ended up totaling twice the number of edges shown in the graph: \\(16 \\times 2 = 32\\). The reason for this is that each edge is incident (touches) two nodes, so it makes sense that when you add up the degrees of each node you end up with twice the amount of lines shown in the visual representation of the graph.\nSo the relationship between the sum of the degrees and the number of edges in an undirected graph is one of simple doubling. In equation form, this is written:\n\\[\n  \\sum_i k_i = 2m\n\\tag{11.4}\\]\nWhere m is the cardinality of the graph’s edge set \\(|E|\\). Some people call this the first theorem of graph theory (Benjamin, Chartrand, and Zhang 2015, 20)!\nAnother way to think about why we end up with twice the number of edges, is that in an undirected graph \\(G\\), if node A is a member of the set of neighbors of node B, then node B is necessarily also a member of the set of neighbors of node A. Thus, when computing the degree of each node, the edge AB does double duty, counting for the computation of both node A and node B’s degree scores, when we sum the degrees the edge thus shows up twice. Note that in the directed case we don’t have that problem, in which case \\(\\sum_i k_i = m\\)."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#graphical-versus-non-graphical-degree-sequences",
    "href": "lesson-degree-based-graph-metrics.html#graphical-versus-non-graphical-degree-sequences",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.6 Graphical versus Non-Graphical Degree Sequences",
    "text": "11.6 Graphical versus Non-Graphical Degree Sequences\nEquation 11.4 has an interesting consequence that may not be immediately obvious. Putting our high-school algebra thinking cap on, we can solve for \\(m\\) and this will give us:\n\\[\n  m = \\frac{\\sum_i k_i}{2}\n\\tag{11.5}\\]\nWhich tells us that the number of edges in a graph is the sum of the degrees of each node divided by two. However, this also tell us something else: For any undirected graph, the sum of the degrees of each node will always be an even number. That’s because we know that \\(m\\) is an integer (the number of edges in a graph can’t be a decimal), so that means that the numerator of the Equation 11.5, namely, the sum of degrees \\(\\sum_i k_i\\) is divisible by two, and any number that is divisible by two (like 10, 24, 48, 120, etc.) is an even number. Ergo, the sum of degrees in a graph, \\(\\sum_i k_i\\), is always an even number.\nThis last result, that the sum of degrees of a graph has to be even, has yet another not so obvious consequence. Consider the following two sums:\n\\[\n4 + 3 + 2  + 1 + 2 + 1 + 3 = 16\n\\tag{11.6}\\]\n\\[\n4 + 3 + 3  + 1 + 2 + 1 + 3 = 17\n\\tag{11.7}\\]\nThe numbers in Equation 11.6 add up to sixteen, which is an even number. So we know automatically, that \\(\\{4, 3, 3, 2, 2, 1, 1\\}\\) could be the degree sequence of a possible graph with seven nodes (\\(n = 7\\)) and \\(m = 16 \\div 2 = 8\\) edges. In fact, one such possible graph is shown in Figure 11.2.\nHowever, we also know that since the sum shown in Equation 11.7 is an odd number, it cannot be the sum of degrees of any possible graph whatsoever. No undirected graph with seven nodes can have the degree sequence \\(\\{4, 3, 3, 3, 2, 1, 1\\}\\).\nIn graph theory terms, we say that the degree sequence \\(\\{4, 3, 3, 2, 2, 1, 1\\}\\) is graphical (could be the degree sequence of a possible graph), while the degree sequence \\(\\{4, 3, 3, 3, 2, 1, 1\\}\\) is not graphical.\n\n\n\n\n\nFigure 11.2: An undirected graph with degree sequence k = {4, 3, 3, 2, 2, 1, 1}.\n\n\n\n\nWhat’s the difference between the graphical sequence \\(\\{4, 3, 3, 2, 2, 1, 1\\}\\) and the not graphical sequence \\(\\{4, 3, 3, 3, 2, 1, 1\\}\\)? Time to find out.\nIn a graph, let us call a node with a degree that is an even number an even node. Let’s call a node with a degree that is an odd number (you guessed it) an odd node. You will notice that in the graphical degree sequence, there are four odd nodes \\(\\{3, 3, 1, 1\\}\\) and three even nodes \\(\\{4, 2, 2\\}\\). The not graphical degree sequence has five odd nodes \\(\\{3, 3, 3, 1, 1\\}\\) and two even nodes \\(\\{4, 2\\}\\).\nSo the number of odd nodes in the graphical sequence is even (four) and the number of odd nodes in the not graphical sequence is odd (five). Is this a coincidence? Turns out not!\nIn every graphical degree sequence, the number of odd nodes has to be an even number. That is to say, if a sequence of numbers has an odd number of odd numbers, then it cannot be the degree sequence of any possible graph! We could have checked because any sequence of numbers that has an odd number of odd numbers will sum to an odd number and we already know that a sequence of numbers that sums to an odd number is not graphical.\nThe number of even nodes, on the other hand, as we saw before can be odd and the sequence is still graphical. That’s because any set of even numbers, odd or even, will sum to an even number. So what matters is the number of odd nodes, which has to be even for a degree sequence to describe possible graph (Buckley and Harary 1990, 3). Heady!\nKnowing the link between the sum of degrees and the number of edges, we can begin to calculate some other important graph metrics."
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#the-average-degree",
    "href": "lesson-degree-based-graph-metrics.html#the-average-degree",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.7 The Average Degree",
    "text": "11.7 The Average Degree\n\n11.7.1 Average Degree in Undirected Graphs\nOnce we know what sum of the degrees is, we can compute an important graph metric called the average degree. This gives us a sense of whether there are lots of well-connected nodes in the graph (in which case the average degree is high), or whether the typical node in the graph only has a small number of neighbors (in which case the average degree is low).\nTo get the average degree for a graph, written \\(\\bar{k}\\), all we need to do is compute the sum of degrees (like earlier) and divided it by the total number of nodes in the graph:\n\\[\n  \\bar{k} = \\frac{\\sum_i k_i}{n}\n\\tag{11.8}\\]\nIn an undirected graph G like the one shown in Figure 11.1, the graph average degree is given by twice the total number of symmetric ties (the cardinality of the edge set as defined earlier) divided by the total number of nodes (the cardinality of the node node set). Because of the identity identified in Equation 11.4, where \\(\\sum_i k_i = 2m\\), we can also write the formula for the average degree as follows by switching the numerator:\n\\[\n  \\bar{k} = \\frac{2m}{n}\n\\]\nFor Figure 11.1, the average degree of the graph is:\n\\[\n  \\bar{k} = \\frac{2m}{n} = \\frac{2 \\times 16}{9} = \\frac{32}{9} = 3.6\n\\tag{11.9}\\]\n\n\n11.7.2 Average Degree in Directed Graphs\nWe can use a variation of the same procedure to compute the average degree in directed graphs. Take the directed graph shown in Figure 8.2, which has seven nodes and 11 edges. In this case, there is no need to “double” the number of lines as in the case of an undirected graph.\nWe could compute either the average indegree or the average outdegree of each node and it would give us the same answer, because each link unless does single duty in determining either the outdegree or indegree of each node. Accordingly, the average degree in the directed case is:\n\\[\n  \\bar{k} = \\frac{m}{|V|} = \\frac{m}{n} = \\frac{ 11}{7}=1.6\n\\tag{11.10}\\]\nEven easier!\nAlthough straightforward, the graph average degree provides a powerful tool to analyze the social world. For example, if we have two school clubs of the same size and we ask students who they are friends with in the club, we might get very different average degrees. Let us assume that the average degree in the first network is two, while in the second network it is five.\nThis statistic informs us that people in the second network have more friends within the group than in the first network. If we are interested in why the first group failed and the second group kept meeting, we might understand that the underlying social relations of friendship, which might be theorized as contributing to the clubs survival, were weaker in the first group to begin with than they were in the second group. We are thus able to gain insight into the causes and/or underlying conditions that shape the social world.\n\n\n11.7.3 The Connection Between Density and Average Degree\nThere is an intimate mathematical relationship between the graph density and the graph average degree. Let’s see what it is.\nTake another look at Equation 5.5. It says that density is equal to:\n\\[\n  d(G)^u = \\frac{2m}{n(n-1)}\n\\tag{11.11}\\]\nWe can write the same equation as the product of two different fractions. Like this:\n\\[\n  d(G)^u = \\frac{2m}{n} \\times \\frac{1}{n-1}\n\\tag{11.12}\\]\nNow, take a look at the first (left-hand) fraction of this product. Does it look familiar? It should, because it is the formula for the average degree we wrote down in Equation 11.8!\nSo that means that if we know the average degree and we know the number of nodes in a graph, we also know the density of the graph without knowing anything about the number of edges. Mathematical magic.\nHow do we do that? Well, substituting the fraction \\(\\frac{2m}{n}\\) for its equivalent, the average degree \\(\\bar{k}\\), in Equation 11.12 we get:\n\\[\n  d(G)^u = \\bar{k} \\times \\frac{1}{n-1} =  \\frac{\\bar{k}}{n-1}\n\\tag{11.13}\\]\nSo this formula tells us that the density of a graph is equivalent to its average degree divided by the number of nodes minus one. Pretty simple!"
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#advanced-degree-metrics-topics",
    "href": "lesson-degree-based-graph-metrics.html#advanced-degree-metrics-topics",
    "title": "11  Degree-Based Graph Metrics",
    "section": "11.8 Advanced Degree Metrics Topics",
    "text": "11.8 Advanced Degree Metrics Topics\n\n11.8.1 Degree Variance\nOnce we know the average degree of a graph, it is possible to compute more complex measures of the heterogeneity in connectivity across nodes (e.g., the extent to which there is a very big spread between well-connected and not so well-connected nodes in the graph) beyond the simpler measures of range such as the difference between \\(k_{max}\\) and \\(k_{min}\\).\nOne such measure was proposed by the sociologist and statistician Tom Snijders in a paper written in 1981 (Snijders 1981). It is called the degree variance of the graph. It is written \\(\\mathcal{v}(G)\\) and it is defined as the average squared deviation between the degree degree of each node and the average degree:\n\\[\n  \\mathcal{v}(G) = \\frac{\\sum_i (k_i - \\bar{k})^2}{n}\n\\tag{11.14}\\]\nThe equation says that to compute the degree variance, first we create a vector of the square of the difference between each node’s degree and the average degree of the graph \\((k_i - \\bar{k})^2\\), then we sum all the values in this vector (note that since we are squaring all these values will be a positive number), and divide the result by the total number of nodes \\(n\\).\nTo compute the degree variance of the graph shown in Figure 11.1 using Equation 11.14, we can thus go through the following steps.\n\nFirst we compute the vector of differences between each node’s degree \\(k_i\\) (shown in Table 11.1) and the average degree \\(\\bar{k}\\) (already computed in Equation 11.9). This yields the following vector:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.44\n-0.56\n0.44\n1.44\n-0.56\n0.44\n-0.56\n-0.56\n-0.56\n\n\n\nTable 11.3: Differences between the degree of each node and the average degree.\n\n\n\nThen we square each of these numbers \\((k_i - \\bar{k})^2\\), which results in the following vector of squared deviations:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.2\n0.31\n0.2\n2.09\n0.31\n0.2\n0.31\n0.31\n0.31\n\n\n\nTable 11.4: Squred differences between the degree of each node and the average degree.\n\n\n\nThen we compute the sum of all these numbers \\(\\sum_i (k_i - \\bar{k})^2\\), which is 4.2.\nFinally, we divide this sum by the number of nodes in the graph, which gives us:\n\n\\[\n  \\mathcal{v}(G) = \\frac{4.2}{9} = 0.47\n\\]\n\n\n\n\n\nFigure 11.3: A 3-regular undirected graph of order 10\n\n\n\n\nThe degree variance is supposed to capture the extent to which there is inequality in the connectivity of nodes in a graph. Inequality exists when a few nodes have a lot of connections and most nodes only have a few. The more inequality, the higher the degree variance. This means that in graphs where there is very little variation in the connectivity of each node (all nodes have roughly the same number of connections), the degree variance should be at a minimum.\n\n\n11.8.2 Regular Graphs\nThe most extreme version of the low inequality case is shown in Figure 11.3. This graph has ten nodes, and every node has the same degree \\(k = 3\\), which means that the average degree is also the same number \\(\\bar{k} = 3\\). This is called a regular graph.\nBecause in a regular graph, every node has the same degree, we refer to regular by their node degree (\\(k\\)) and their order (\\(n\\)) as k-regular graphs of order n. So, the example shown in Figure 11.3, where every node has degree equal to three (\\(k = 3\\)) and there are ten nodes (\\(n = 10\\)) is a three-regular graph of order ten. Because \\(k - \\bar{k} = 3 - 3 = 0\\) for all nodes in the regular graph, The degree variance of a regular graph is always zero (because the numerator of Equation 11.14 will always be zero).\n\n\n11.8.3 Node Average Nearest Neighbor Degree\nJust in the same way we can compute the degree of each node, we may be interested in in the question of whether nodes in the network tend to connect to others of high degree, or whether the connections in the network occur at random, irrespective of degree. The first situation, where people tend to connect to other people of high degree is called preferential attachment in network science (Barabási and Albert 1999). It is also referred to as a popularity tournament structure in sociology (Waller 1937). If you went to a real live high school, you may know how this works.\nTo get a sense of whether a given node prefers to connect to others who also have a large number of connections, we can compute an index called the average nearest neighbor degree, which is conventionally written \\(\\bar{k}_{nn(i)}\\).\nThis is given by the following formula:\n\\[\n  \\bar{k}_{nn(i)} = \\frac{1}{k_i} \\sum_{j \\in \\mathcal{N(i)}} k_j\n\\tag{11.15}\\]\nThe formula says that to compute the \\(k_{nn}\\) of node i we take sum of the degrees of each of their neighbors j (remember that \\(j \\in \\mathcal{N(i)}\\) means “as long as node j belongs to the set \\(N(i)\\)”), and then divide it by the degree of node i.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n3.8\n4.3\n3.8\n3.6\n4.3\n3.5\n3.3\n3.3\n3.3\n\n\n\nTable 11.5: Average nearest neighbor degree of nodes in an undirected graph\n\n\nAs an example, the \\(\\bar{k}_{nn(i)}\\) for each node in the undirected graph shown in Figure 11.1 are shown in Table 11.5. This is the vector of average nearest neighbor degrees for the graph \\(k_{nn}\\). The table tells us that, nodes B and and E seem to have the strongest tendencies to connect to others who are also popular, while G, H, and I display the weakest such tendencies.\nBut can do we know if a \\(\\bar{k}_{nn(i)} = 4.3\\) is a big number or a small number? Well, we can compare each node’s \\(\\bar{k}_{nn(i)}\\) to the average degree (\\(\\bar{k}\\)) as defined previously. If a node’s \\(\\bar{k}_{nn(i)}\\) is bigger than the average degree of the graph, then we can say that their neighbors are more likely to be popular than expected by chance. For instance, we know, from the calculation shown in Equation 11.9, that the average degree for this graph is 3.6, so a \\(\\bar{k}_{nn(i)} = 4.3\\) tells us that a node connects to others that are (on average) more popular than the average person.\n\n\n11.8.4 The Graph’s Average Nearest Neighbor Degree\nIf we take the average of the average nearest neighbor degree vector for each node shown in Table 11.5, this would give us the graph’s average nearest neighbor degree (\\(\\bar{k}_{nn}\\)). But what could this possibly mean? Well, this would tell us the typical number of friends that the typical friend of the typical node in the graph has! Essentially, the expected number of friends of friends of the average person.\nIn a mind-bending paper, the sociologist Scott Feld (1991) showed, that with very few exceptions (e.g., regular graphs), the average number of friends of the typical person (the graph’s average degree) is smaller than the average number of friends of friends of the typical person in the same social network (the graph’s average nearest neighbor degree). That is, your friends have more friends than you do. Don’t get depressed. This is just how social networks work.\nCheck it out for yourself! If we compute the graph’s average nearest neighbor degree from the numbers in the vector shown in Table 11.5, this would give us \\(\\bar{k}_{nn} = 3.7\\) which is larger than the same graph’s average degree we computed earlier (\\(\\bar{k} = 3.6\\)). Weird!"
  },
  {
    "objectID": "lesson-degree-based-graph-metrics.html#references",
    "href": "lesson-degree-based-graph-metrics.html#references",
    "title": "11  Degree-Based Graph Metrics",
    "section": "References",
    "text": "References\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12.\n\n\nBenjamin, Arthur, Gary Chartrand, and Ping Zhang. 2015. The Fascinating World of Graph Theory. Princeton University Press.\n\n\nBuckley, Fred, and Frank Harary. 1990. Distance in Graphs. Addison-Wesley.\n\n\nFeld, Scott L. 1991. “Why Your Friends Have More Friends Than You Do.” American Journal of Sociology 96 (6): 1464–77.\n\n\nKim, Hyunju, Zoltán Toroczkai, Péter L Erdős, István Miklós, and László A Székely. 2009. “Degree-Based Graph Construction.” Journal of Physics A: Mathematical and Theoretical 42 (39): 392001.\n\n\nSnijders, Tom AB. 1981. “The Degree Variance: An Index of Graph Heterogeneity.” Social Networks 3 (3): 163–74.\n\n\nWaller, Willard. 1937. “The Rating and Dating Complex.” American Sociological Review 2 (5): 727–34."
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#the-dyad-census",
    "href": "lesson-directed-graph-metrics.html#the-dyad-census",
    "title": "12  Directed Graph Metrics",
    "section": "12.1 The Dyad Census",
    "text": "12.1 The Dyad Census\n\n\n\n\n\nFigure 12.1: A directed graph.\n\n\n\n\nWhen examining the structure of a social network composed of asymmetric edges, and represented as a directed graph, we may be interested in counting how many dyads of each type (mutual, asymmetric, and null) exist in the whole graph. This graph metric is called the dyad census. For instance, if we were to perform the dyad census on the graph shown in Figure 12.1 we would end up with:\n\n\n\n\n\n\n\nMutual\nAsymmetric\nNull\n\n\n\n\n4\n10\n7\n\n\n\nTable 12.1: Dyad census of a directed graph.\n\n\nThe dyad census tells us that out of all the connected dyads in Figure 12.1, four of them are mutual: \\(\\{BD, CG, DE, FE\\}\\), and ten of them are asymmetric: \\(\\{AC, AE, BC, DF, EG, DF, EG, FA, FB, FG\\}\\).\nThe number of mutual dyads in a directed graph is represented by the symbol \\(L^\\leftrightarrow\\). The number of asymmetric dyads, on the other hand is represented by the symbol \\(L^\\rightarrow\\). Note that the cardinality of the edge set \\(|E|\\) (a.k.a. the total number of edges) in a directed graph is therefore:\n\\[\n    |E| = 2L^\\leftrightarrow + L^\\rightarrow\n\\]\nRemember, we multiply the number of mutual dyads by two, because each mutual dyad is composed of two directed edges!\nIn the same way, the total number of connected dyads (\\(L\\)) in a directed graph is:\n\\[\n    L = L^\\leftrightarrow + L^\\rightarrow\n\\]\n\n12.1.1 Advanced topic: Mechanics of the dyad census\nWhere the numbers in Table 12.1) come from? Well, it turns out that they can be readily computed from the asymmetric adjacency matrix (\\(A\\))of the corresponding directed graph. The one corresponding to Figure 12.1 is shown as Table 12.2.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n0\n1\n0\n1\n0\n1\n\n\nB\n0\n–\n1\n1\n0\n0\n0\n\n\nC\n0\n0\n–\n0\n0\n0\n1\n\n\nD\n1\n1\n0\n–\n1\n1\n0\n\n\nE\n0\n0\n0\n1\n–\n1\n1\n\n\nF\n1\n1\n0\n0\n1\n–\n1\n\n\nG\n0\n0\n1\n0\n0\n0\n–\n\n\n\nTable 12.2: Asymmetric adjancency matrix of a directed graph.\n\n\nTo find out the number of mutual dyads (\\(L^\\leftrightarrow\\)) all we need to do is cycle through the lower triangle of the matrix (cells below the main diagonal), and compute the product of that cell and the corresponding symmetric cell on the upper triangle (cells above the main diagonal).1\nIn matrix format:\n\\[\n  L^\\leftrightarrow = \\sum_{i>j} a_{ij}a_{ji}\n\\tag{12.1}\\]\nWhere the subscript below \\(\\sum\\) says to sum through the cells where the row subscript i is always larger than the column subscript j (lower triangle). The product of \\(a_{ij}a_{ji}\\) will only equals one if the edges go in both directions \\(a_{ij} = 1, a_{ji} = 1\\), and the sum will equal the number of mutual dyads \\(L^\\leftrightarrow\\).\nOnce we have \\(L^\\leftrightarrow\\) we can figure out \\(L^\\rightarrow\\), the number of asymmetric dyads, using the formula (from Wasserman and Faust (1994), p. 512):\n\\[\n  L^\\rightarrow = \\sum_i \\sum_j a_{ij} - 2L^\\leftrightarrow\n\\tag{12.2}\\]\nWhere the double \\(\\sum\\) indicates that we are summing through all the cells in the adjacency matrix (essentially counting all the cells that have a one in the matrix) and are subtracting twice the number of mutuals (because these dyads contribute two directed links each)."
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#advanced-topic-the-triad-census",
    "href": "lesson-directed-graph-metrics.html#advanced-topic-the-triad-census",
    "title": "12  Directed Graph Metrics",
    "section": "12.2 Advanced Topic: The Triad Census",
    "text": "12.2 Advanced Topic: The Triad Census\n\n\n\n\n\n\n\n\nCount\n\n\n\n\nNNN\n0\n\n\nNNA\n5\n\n\nNNM\n4\n\n\nAAN(O)\n3\n\n\nAAN(I)\n1\n\n\nAAN(L)\n5\n\n\nMAN(I)\n3\n\n\nMAN(O)\n4\n\n\nAAA(I/O)\n3\n\n\nAAA(C)\n0\n\n\nMMN\n1\n\n\nMAA(O)\n1\n\n\nMAA(I)\n1\n\n\nMAA(C)\n3\n\n\nMMA\n1\n\n\nMMM\n0\n\n\n\nTable 12.3: Triad census of a directed graph.\n\n\nJust like we did in the case of directed dyads, it is possible to go through a directed graph and count the number of directed triadic motifs. This is called the triad census and was developed by sociologists and mathematicians working together in the 1970s (Davis and Leinhardt 1972). For instance, if we were to perform the triad census on the graph shown in Figure 12.1 we would end up with the counts shown in Table 12.3). For your reference, the sixteen triadic configurations are shown in Figure 10.21."
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#graph-reciprocity",
    "href": "lesson-directed-graph-metrics.html#graph-reciprocity",
    "title": "12  Directed Graph Metrics",
    "section": "12.3 Graph Reciprocity",
    "text": "12.3 Graph Reciprocity\nRecall from our discussion in the lesson of types of graphs, that asymmetric relations in a directed graph have an important property that symmetric relations in an undirected graph lack: Reciprocity. That is, connected dyads in directed graphs can be either mutual (with directed edges going in both directions) or asymmetric with (with directed edges going only in one direction). So for instance, in the graph shown in Figure 12.1 the connected \\(BD\\) dyad is mutual, but the \\(FB\\) dyad is asymmetric.\n\n\n\n\n\nFigure 12.2: Another directed graph.\n\n\n\n\nIn some applications we may be interested in Figuring out the extent to which the whole graph is more or less likely to contain mutual as opposed to asymmetric dyads. This metric is called Graph Reciprocity and is usually written as \\(R(G)\\). For instance, if we compare the graph shown in Figure 12.1 to the one shown in Figure 12.2 it seems like there’s much more reciprocity going in Figure 12.1 than in Figure 12.2. How can we quantify it?\nWell, the count of the different types of directed dyads provided by the dyad census in Table 12.1) provide us with the information we need. Accordingly, if we wanted to quantify the amount of reciprocity in the graph shown in Figure 12.1, we could do it by computing the following ratio (Krackhardt 1994):\n\\[\n    R(G) = \\frac{L^\\leftrightarrow}{L}\n\\tag{12.3}\\]\nIn our example, this would be: \\[\n    R(G) = \\frac{4}{4 + 10} = \\frac{4}{14} = 0.29\n\\]\nNote that in a graph in which all dyads are mutual (\\(L^\\rightarrow =0\\)), then \\(R = \\frac{L^\\leftrightarrow}{L^\\leftrightarrow} = 1\\), the theoretical maximum of graph reciprocity. Conversely, a graph with no mutual dyads, \\(L^\\leftrightarrow = 0\\), then \\(R = 0\\). For instance, a tree graph composed of all anti-symmetric edges (which by definition cannot be mutual), will always have \\(R=0\\).\n\n\n\n\n\n\n\nMutual\nAsymmetric\nNull\n\n\n\n\n1\n10\n10\n\n\n\nTable 12.4: Dyad census of a directed graph.\n\n\nThe dyad census corresponding to the directed graph shown in Figure 12.2 is shown in Table 12.4). Can you figure out the graph reciprocity?"
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#graph-connectedness",
    "href": "lesson-directed-graph-metrics.html#graph-connectedness",
    "title": "12  Directed Graph Metrics",
    "section": "12.4 Graph Connectedness",
    "text": "12.4 Graph Connectedness\nRecall that in our discussion of indirect relationships, we found that paths work differently in undirected versus directed graphs. In an undirected graph, if we know the graph is connected (it is not split into separate components), then we also know that every node can reach every other node via a an undirected path of some length, even if that path is very long. The reason is that undirected paths are symmetric: In an undirected graph, if a path exists with node A as source and node B as destination, then necessarily a path also exists with node B as source and node A as destination. In an undirected graph that is also connected, all nodes are mutually reachable by construction.\nIn a directed graph, things work differently. Directed paths are asymmetric. This means that In a directed graph, a path may have node A as source and node B as destination. However, this does not necessarily mean a path also exists with node B as source and node A as destination. It could (in which case the two nodes are strongly connected), but it could also not (in which case the nodes are only unilaterally connected). This means that in a directed graph, connectivity is a matter of more or less rather than all or none as it is in the undirected case.\nThis means that we can compute a metric telling us the extent to which a directed graph is connected. To do this, we can use the reachability matrix as we defined it in the lesson on social network matrices. Recall that the reachability matrix R is a square matrix with the nodes on the row and columns. Each cell of the reachability contains a \\(r_{ij} = 1\\) if node i can reach node j and a \\(r_{ij} = 0\\) otherwise.\nConsider for instance the graph shown in Figure 12.1. The corresponding reachability matrix is shown as Table 12.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n1\n1\n1\n1\n1\n1\n\n\nB\n1\n–\n1\n1\n1\n1\n1\n\n\nC\n0\n0\n–\n0\n0\n0\n1\n\n\nD\n1\n1\n1\n–\n1\n1\n1\n\n\nE\n1\n1\n1\n1\n–\n1\n1\n\n\nF\n1\n1\n1\n1\n1\n–\n1\n\n\nG\n0\n0\n1\n0\n0\n0\n–\n\n\n\nTable 12.5: Reachability matrix for a directed graph.\n\n\n[1] 0.4761905\n\n\nEvery zero in the reachability matrix represents an actor who cannot reach another actor. These are “gaps” of reachability in the social system which may impair efficiency, coordination, and adaptability in the group (Krackhardt 1994). The less zeroes there are in the reachability matrix, the more connected the network is. So we can calculate graph connectedness directly from the reachability matrix R, just by summing up the value of all the cells of the reachability matrix and dividing this sum by the total number of cells. This can be expressed using the following formula:\n\\[\n  C = \\frac{\\sum_i \\sum_j r_{ij}}{N(N-1)}\n\\tag{12.4}\\]\nWhere the double \\(\\sum\\) like in Equation 12.2 indicates summing across all the cells in the matrix, and \\(N(N-1)\\) is the number of (non-diagonal) cells. A reachability matrix corresponding to a graph with no connectedness violations will have a numerator that is equal total number of cells in the matrix (which is the same as the number of edges of the corresponding complete graph of the same order). A graph with some connectedness violations where the sum of the cells (\\(\\sum_i\\sum_j r_{ij}\\)) is less than \\(N(N-1)\\) will have a connectedness score between zero and one.\nFor instance, for the reachability matrix shown in Table 12.5, the numerator in the fraction corresponding to Equation 12.4 is:\n\\[\n  \\sum_i \\sum_j r_{ij} = 32\n\\]\nThe denominator is: \\[\n  N(N-1) = 7 \\times (7 - 1) = 42\n\\]\nAnd the graph connectedness score is:\n\\[\n  C = \\frac{32}{42} = 0.76\n\\]\nWhich tells us that while not a perfect \\(1.0\\), Figure 12.1 is still a pretty well-connected social network!"
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#graph-hierarchy",
    "href": "lesson-directed-graph-metrics.html#graph-hierarchy",
    "title": "12  Directed Graph Metrics",
    "section": "12.5 Graph Hierarchy",
    "text": "12.5 Graph Hierarchy\nIn a hierarchical social structure, things (like commands, orders, authority, and influence) tend to flow in one direction: From the boss to the middle manager, from the general to the lieutenant, from the parent to the child. The tree graph we considered before, full of anti-symmetric links going in one direction only, is thus the perfect model of a purely hierarchical structure in social networks. One version is shown in Figure 12.3.\n\n\n\n\n\nFigure 12.3: A tree graph.\n\n\n\n\nIn Table 12.5, we derived the corresponding reachability matrix for that directed graph. Now let us do the same, but this time for a tree graph describing hierarchical anti-symmetric relations like the one shown in Figure 12.3. Table 12.6 shows the results.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n1\n1\n1\n1\n1\n1\n\n\nB\n0\n–\n0\n1\n1\n0\n0\n\n\nC\n0\n0\n–\n0\n0\n1\n1\n\n\nD\n0\n0\n0\n–\n0\n0\n0\n\n\nE\n0\n0\n0\n0\n–\n0\n0\n\n\nF\n0\n0\n0\n0\n0\n–\n0\n\n\nG\n0\n0\n0\n0\n0\n0\n–\n\n\n\nTable 12.6: Reachability matrix for an anti-symmetric tree graph.\n\n\n[1] 1\n\n\nOne interesting thing to observe about the reachability matrix shown in Table 12.6 is that it only has non-zero entries above the diagonal, in what we called the upper triangle of the matrix in the lesson on network matrices. Recall this are cells in which the row index i is always smaller than the column index j. You can see for yourself that all the cells in which the row index is larger than the column index (the lower triangle of the matrix) are indeed all zero.\nThe other thing to observe is that the reachability matrix of a tree graph encodes the authority or hierarchical structure of the tree graph. This is easy to see by the fact that the row corresponding to the “top” node (A) is full of ones. This says that this node can reach every other node in the graph via some path. So if this was the general in an army, they could influence every other node, by giving an order to the people directly connected to them (e.g., B, or C) and then they would pass it down to the others.\nNow if we look at A’s column, we see that it is full of zeroes, indicating that while A can reach everybody, no one can reach them back. The same goes for every level; the nodes at the top can reach the ones at the bottom, but they can’t reach them back. This is why the lower triangle of the matrix has to be made up of all zeroes, and why it encodes the idea that in a tree, things only flow in one direction (downwards, from the top to the bottom).\n\n\n\n\n\nFigure 12.4: An asymmetric graph arranged as a tree.\n\n\n\n\nNow let’s break the anti-symmetry principle and add some reciprocal directed ties to the network shown in Figure 12.3, keeping the same set of nodes. This transforms the graph from an anti-symmetric to an asymmetric one. The new graph is shown as Figure 12.4. In this network, node B sends a reciprocal link to A, node D sends a reciprocal link to B, and G reciprocates their tie to **C*. The reachability matrix for this graph, is shown in Table 12.7.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nG\nE\nF\n\n\n\n\nA\n–\n1\n1\n1\n1\n1\n1\n\n\nB\n1\n–\n1\n1\n1\n1\n1\n\n\nC\n0\n0\n–\n0\n1\n0\n1\n\n\nD\n1\n1\n1\n–\n1\n1\n1\n\n\nG\n0\n0\n1\n0\n–\n0\n1\n\n\nE\n0\n0\n0\n0\n0\n–\n0\n\n\nF\n0\n0\n0\n0\n0\n0\n–\n\n\n\nTable 12.7: Reachability matrix for an asymmetric graph.\n\n\n[1] 0.7777778\n\n\nNote that one big difference between Table 12.6 and Table 12.7 is that Table 12.7 now features some non-zero entries in the lower-triangle. So it seems like the lower triangle in the reachability matrix of an asymmetric graph, records anti-symmetry violations.\nAnother big difference between Table 12.6 and Table 12.7 is that now nodes B and D, like A in the original tree graph, can reach everybody else in the network. They can do that because now they can reach A, and since A could reach everyone initially, they can do so through that node.\nThus, if we think of the tree graph as representing a purely hierarchical structure, in which authority, influence, orders, and so forth only flow in one direction, then the addition of reciprocity into the structure can be seen as a relaxation of the hierarchy principle. In Figure 12.4 A doesn’t just influence or order B around in a unidirectional way like they do in Figure 12.3, B can “talk back”, or exert influence upon A and so forth. So by adding asymmetric reciprocal links, we have made the graph less hierarchical.\nThis insight can be taken as motivation to develop a graph metric called graph hierarchy (Krackhardt 1994). The measure is simple. It is equal to the number of unilaterally reachable (\\(U^r\\)) pairs of nodes in the graph to the total number of connected pairs in the graph (\\(E^r\\)). The total number of connected pairs, in turn is given by the sum of mutually reachable (\\(M^r\\)) and unilaterally connected (\\(U^r\\)) pairs.\n\\[\n    H(G) = \\frac{U^r}{M^r + U^r}\n\\tag{12.5}\\]\nThe graph hierarchy (\\(H(G)\\)) score in Equation 12.5 is maximized (\\(H(G) = 1\\)) when there are no mutually reachable connected pairs in the graph (\\(M^r = 0\\)). It is at its minimum (\\(H(G) = 0\\)) when all the connected pairs are mutually reachable \\(U^r = 0\\). Note that this ratio is similar measure of reciprocity for non-valued directed graphs discussed earlier, but applied to the reachability matrix instead of the adjacency matrix (Krackhardt 1994). Except that here we are interested in non-reciprocal (unilateral) relations in the reachability matrix not the adjacency matrix. Like before, we can find out \\(M^r\\) by using a variation of Equation 12.1 but applied to the reachability matrix instead of the adjacency matrix:\n\\[\n  M^r = \\sum_{i>j} r_{ij}r_{ji}\n\\tag{12.6}\\]\nWhere the subscript below \\(\\sum\\) says to sum through the cells where the row subscript i is always larger than the column subscript j (lower triangle of the reachability matrix). The product of \\(r_{ij}r_{ji}\\) will only equal one if two nodes are i and j are mutualy reachable \\(r_{ij} = 1, r_{ji} = 1\\), and the sum will equal the number of mutually reachable dyads in the graph \\(M^r\\).\nOnce we have \\(M^r\\) we can figure out \\(U^r\\), the number of unilaterally connected dyads, using a variation of Equation 12.2:\n\\[\n  A = \\sum_i \\sum_j r_{ij} - 2M^r\n\\tag{12.7}\\]\nRecall that we know that a reachability relation is unilateral, just in case every time a node A can reach a node B then B is unable reach A via a directed path in the graph. The tree graph has that property for all pairs of nodes. This implies that tree graphs display maximum graph hierarchy: In a tree graph, \\(M^r = 0\\) and therefore \\(H(G) = 1.0\\). A completely reciprocal reachability matrix, on the other hand, where every pair of nodes is mutually reachable, implies minimum graph hierarchy, with \\(H(G) = 0\\).\nApplying Equation 12.5 to the tree graph shown in Figure 12.3 gives us the expected value of \\(1.0\\), but applying the same equation to the asymmetric graph shown in Figure 12.4 gives us the value of \\(H(G) = 0.77\\), which is pretty high, but also shows that this network structure is less hierarchical than the pure anti-symmetric tree. Overall, the closer to zero the \\(H(G)\\) score is, the less hierarchical the structure. For instance, the graph hierarchy score corresponding to the graph shown in Figure 12.1 is \\(H(G) = 0.47\\), showing that this network is even less hierarchical than the one shown in Figure 12.4."
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#graph-efficiency",
    "href": "lesson-directed-graph-metrics.html#graph-efficiency",
    "title": "12  Directed Graph Metrics",
    "section": "12.6 Graph Efficiency",
    "text": "12.6 Graph Efficiency\nTree graphs like the one shown in Figure 12.3 have another interesting property. The number of links in a tree is always equal to the number of nodes minus one! That is, in a tree graph \\(E = N - 1\\). You can go back to Figure 12.3 and check for yourself. We have seven nodes \\(N\\) and six links \\(E\\) and \\(6 = 7 -1\\). That’s neat!\n\n\n\n\n\nFigure 12.5: A tree graph that is missing one link.\n\n\n\n\nThis means that trees are networks that have the minimum number of links to keep them connected. If you remove even one node from a tree you create a connectivity gap in the structure, splitting the graph into separate components. Like the one shown in Figure 12.5. After removing the \\(BE\\) link node \\(E\\) becomes isolated from the rest. The same would happen (creating disconnected clumps) if we were to remove any other link from Figure 12.5."
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#advanced-topic-reciprocity-in-directed-weighted-graphs",
    "href": "lesson-directed-graph-metrics.html#advanced-topic-reciprocity-in-directed-weighted-graphs",
    "title": "12  Directed Graph Metrics",
    "section": "12.7 Advanced topic: Reciprocity in directed weighted graphs",
    "text": "12.7 Advanced topic: Reciprocity in directed weighted graphs\nOne phenomenon we can better understand using weighted graphs is the idea of reciprocity. Take for instance Figure 12.6, which shows a directed weighted graph, with the same set of nodes and edges as Figure 8.2. Like all directed graphs, we can see that there are three kinds of dyads: mutual, asymmetric and null). This could represent the social network created by the flow of directed communications (e.g., emails or text messages send from one person to another) in an office or work group.\nIn unweighted directed graphs, we tend to say that two nodes that are part of a mutual dyad have a reciprocal relationship. So if we were going by a binary (on or off) idea, of reciprocity, we would say that all the mutual dyads in Figure 12.6, such as \\(AB\\), \\(AF\\), and \\(BD\\) are reciprocal. But as we said, when it comes to the sort of interactions that are best captured in a weighted graph, reciprocity is a matter of more or less, not all or none. The weights (\\(w\\)) provide a richer picture of what’s going on in the network.\n\n12.7.1 Notation\nLet us come up with some notation. In a directed weighted graph, the weight of the directed edge going from source node A to source destination node B is referred to as \\(w_{AB}\\). In the graph shown in in Figure 12.6, \\(w_{AB} = 17\\). If the dyad is mutual, we would refer to the directed edge coming from source node B to destination node A as \\(w_{BA}\\). As you can see, \\(w_{BA}=20\\).\n\n\n\n\n\nFigure 12.6: A directed weighted graph\n\n\n\n\nAre mutual dyads in a directed weighted graph necessarily “reciprocal”? The answer is no. For instance, in Figure 12.6, the relationship between B and D seems to be very uneven; B sent thirty messages to D (\\(w_{BD} = 30\\)) but D only sends two messages back (\\(w_{BD} = 2\\)). So we would say that reciprocity definitely seems to be lacking in the \\(BD\\) dyad.\nThe relationship between A and B that we considered earlier, on the hand, seems much more even (twenty versus seventeen) So there is more reciprocity in the \\(AB\\) dyad than there is in the \\(BD\\) dyad even though both dyads are “mutual” going by the unweighted graph criterion.\nPerhaps B does not think much of D or thinks that D is pestering them with too many messages. Lack of weighted reciprocity may be indicative of the quality of the relationship between B and D.\n\n\n12.7.2 A Measure of Dyadic Reciprocity in Directed Weighted Graphs\nIs there a way to quantify the amount of reciprocity in mutual dyads in directed graphs? The answer is yes (Squartini et al. 2013). The trick is to separate the amount of interaction in the edge weights (\\(w\\)) that is “reciprocal” from the amount that is “non-reciprocal.”\nFor instance, let’s go back to the \\(AB\\) dyad in Figure 12.6. If these were messages going back and forth, then we would say that A and B are even in seventeen of those messages. This is called the reciprocated weight of the \\(AB\\) mutual dyad. So in a mutual weighted dyad, the reciprocated weight (let’s call it \\(w_{ij}^\\leftrightarrow\\)) will always be equal to the smallest of the two weights! Note this also means that \\(w_{ij}^\\leftrightarrow\\) will always be equal to \\(w_{ji}^\\leftrightarrow\\).\nIn equation form:\n\\[\n   w^\\leftrightarrow_{ij} = \\min(w_{ij}, w_{ji})\n\\tag{12.8}\\]\nWhere i and j are just our generic stand ins for any pair of nodes in the graph \\({A, B, C...}\\), and min is the symbol for the mathematical operation of “take the minimum” of a set of numbers (e.g., \\(\\min(1, 2, 3) = 1\\) and \\(\\min(10, 100) = 10\\)). So, \\(w^\\leftrightarrow_{AB} = \\min(20,17) = 17\\).\nNow, once we have \\(w^\\leftrightarrow_{ij}\\), we can compute how much directed non-reciprocity goes from one node to the other. This called the non-reciprocated weight of the mutual tie (let’s call it \\(w^\\rightarrow_{ij}\\)). To figure this out, we subtract reciprocated weight from the directed weight going from one node to another:\nIn equation form:\n\\[\n   w^\\rightarrow_{ij} = w_{ij} - w^\\leftrightarrow_{ij}\n\\tag{12.9}\\]\nNote that the non-reciprocated weight of the tie will either be equal to zero if the i node is the who contributes less to the relationship, or it will be a number larger than zero, if the i node is the one who contributes more. So for the \\(AB\\) dyad in Figure 12.6, \\(w^\\rightarrow_{AB} = 17 - 17 = 0\\) and \\(w^\\rightarrow_{BA} = 20 -17 = 3\\).\n\n\n12.7.3 A Measure of Graph Reciprocity in Directed Weighted Graphs\nAbove, we used the dyad census to come up with a measure of the extent to which a directed graph is reciprocal, which we called \\(R(G)\\). We can do something similar for the case of directed weighted graphs. Recall that \\(R(G)\\) is just a ratio of the number of mutual dyads \\(L^\\leftrightarrow\\) to the total number of directed edges (\\(E\\)). In the weighted directed case, we can compute \\(R^w(G)\\), which is a ratio of the sum of the reciprocated weight of all dyads to the total sum of the weights across each directed edge in the graph.\nIn equation form:\n\\[\n   R^w(G) = \\frac{\\sum_i \\sum_{i \\neq j} w_{ij}^\\leftrightarrow}{\\sum_i \\sum_{i \\neq j} w_{ij}}\n\\tag{12.10}\\]"
  },
  {
    "objectID": "lesson-directed-graph-metrics.html#references",
    "href": "lesson-directed-graph-metrics.html#references",
    "title": "12  Directed Graph Metrics",
    "section": "References",
    "text": "References\n\n\n\n\nDavis, James A, and Samuel Leinhardt. 1972. “The Structure of Positive Interpersonal Relations in Small Groups.” In Sociological Theories in Progress, Volume 2, 218–51. Boston: Houghton Mifflin.\n\n\nKrackhardt, David. 1994. “Graph Theoretical Dimensions of Informal Organizations.” Computational Organization Theory 89 (112): 123–40.\n\n\nSquartini, Tiziano, Francesco Picciolo, Franco Ruzzenenti, and Diego Garlaschelli. 2013. “Reciprocity of Weighted Networks.” Scientific Reports 3: 2729.\n\n\nWasserman, Stanley, and Katherine Faust. 1994. “Social Network Analysis: Methods and Applications.”"
  },
  {
    "objectID": "lesson-directed-indirect-connections.html#mutual-reachability",
    "href": "lesson-directed-indirect-connections.html#mutual-reachability",
    "title": "13  Directed Indirect Connections",
    "section": "13.1 Mutual Reachability",
    "text": "13.1 Mutual Reachability\nIn directed graphs, some pairs of nodes can be mutually reachable. That is, there can be a directed path going from one node to the other, and vice versa. Figure 13.3 shows an example of this case involving nodes \\(B\\) and \\(C\\). Starting with origin node \\(B\\) we can get to \\(C\\) via the highlighted purple path of length \\(l_{BC} = 3\\) formed by the edges \\((BE, ED, DC)\\). In the same way, starting with node \\(C\\), we can get to node \\(B\\) via the separate, highlighted red path of length \\(l_{CB} = 3\\) formed by the edges \\((CD, DE, EB)\\).\nFor mutual reachability between two nodes to happen in a directed graph, the two paths do not have to be the same length in fact, in the case of nodes \\(B\\) and \\(C\\) node \\(C\\) can reach \\(B\\) via path shorter than length 3. This is shown in Figure 13.4), where we can see that \\(C\\) can reach \\(B\\) via the shortest path possible short of being directly connected \\((l_{CB} = 2)\\), formed by the edges \\((CE, EB)\\) (highlighted in red). On the other hand, \\(B\\) cannot reach \\(C\\) via a path shorter than 3.\n\n\n\n\n\nFigure 13.5: A directed graph showing a directed cycle starting and ending with node C."
  },
  {
    "objectID": "lesson-directed-indirect-connections.html#directed-cycles",
    "href": "lesson-directed-indirect-connections.html#directed-cycles",
    "title": "13  Directed Indirect Connections",
    "section": "13.2 Directed Cycles",
    "text": "13.2 Directed Cycles\nJust like in the undirected case, a directed path that begins and ends with the same node is called a directed cycle. We refer to different cycles by the number of directed edges they include. For instance a 3-directed cycle is a directed cycle with three directed edges, a 4-directed cycle is a directed cycle with four edges and so forth. For instance, in Figure 13.5, the directed 3-cycle features node \\(C\\) as both the origin and destination node involving the directed edges \\((CE, ED, DC)\\) is highlighted in red.\nA directed graph that does not contain any cycles is a special kind of directed graph (composed of anti-symmetric ties) called a directed acyclic graph.\n\n\n\n\n\nFigure 13.6: A directed graph showing two weakly connected nodes A and B."
  },
  {
    "objectID": "lesson-directed-indirect-connections.html#types-of-indirect-connections-in-directed-graphs",
    "href": "lesson-directed-indirect-connections.html#types-of-indirect-connections-in-directed-graphs",
    "title": "13  Directed Indirect Connections",
    "section": "13.3 Types of Indirect Connections in Directed Graphs",
    "text": "13.3 Types of Indirect Connections in Directed Graphs\nIn a directed graph, when pairs of nodes are mutually reachable they are also said to be strongly connected. Otherwise if nodes in a directed graph are connected via at least one directed path that only goes in one direction (like nodes A and B in Figure 13.3)), they are said to be unilaterally connected. Two nodes are said to be recursively connected when they are strongly connected and and at least one of the pairs of directed paths going in both directions use the same nodes (like paths \\((BE, ED, DC)\\) and \\((CD, DE, EB)\\) connecting nodes B and C in Figure 13.3)).\nFinally, in a directed graph, two nodes are weakly connected if we can trace a path from one to the other, but only by ignoring the direction of the arrows! For instance, Figure 13.6 shows a directed graph in which nodes A and B have a weak connection via the \\((BE, CE, AC)\\) path (highlighted in red), although this is not the only weak connection they share.\nCan you trace other weakly connected paths between nodes A and B in Figure 13.6)?"
  },
  {
    "objectID": "lesson-other-graphs.html#tie-strength-and-weighted-graphs",
    "href": "lesson-other-graphs.html#tie-strength-and-weighted-graphs",
    "title": "14  Other Types of Graphs",
    "section": "14.1 Tie Strength and Weighted Graphs",
    "text": "14.1 Tie Strength and Weighted Graphs\nIn the preceding lessons, our understanding of relationships have centered around their existence or absence. In certain situations, it may be socially meaningful to consider relationships in terms of their intensity or frequency- or what is often referred to as the “strength” of the tie (Marsden and Campbell 1984).\nFor example, people might have many friends, and friendship ties can be turned into graphs as done in the above examples. However, people often have different types of friends, and some friends are more important than others. This is the idea behind the concept of having a best friend. Your best friend might be more important to you than all your other friends, and it might be sociologically meaningful to the topic you are studying to capture this difference in your network. While it might be a little meaningless to just mark your best friend different from the rest, we can think of social situations where a series of gradations might make sense.\nFor example, let’s say that we want to understand who is the leader in a group of friends. By definition, we’ve already bound the case as an existing group of friends. If everyone had a tie to the others, because they’re all friends, then we would not be able to detect any variation between these different friends. However, if we looked at the frequency of text messages sent from one of these friends to another one, then we would likely begin to detect variation.\n\n\n\n\n\nFigure 14.1: A undirected weighted graph\n\n\n\n\nThe variation in the strength of these ties in social networks is captured by using weighted graphs to represent such networks. Figure 14.1 shows an example of a undirected weighted graph. In weighted graphs, the relative intensity of the relationships between actors in the network is quantified, thus facilitating the comparison of particular actors and relationships within the network. This is done by associating each edge in the graph with a number, call the weight of that edge. So instead of being just a set of vertices and edges, a weighted graph (\\(G_w\\)) is a set of three sets. A set of vertices (\\(V\\)), a set of edges (\\(E\\)), and a set of weights (\\(w\\)) associated with each edge:\n\\[\n  G_w = (E, V, w)\n\\tag{14.1}\\]\nThinking of Figure 14.1 as a type of undirected tie, the numbers can be thought of as the intensity of the link between two people. Perhaps these are the number of times two people have met for coffee or a drink during last year. Inspecting the figure, we can see that actors C and E hang out together quite frequently (perhaps they are close, or are working on a project together). Actors B and G, on the other hand, hang out together less often.\nActors also seem to have preferences as to which people they hang out with most frequently with among those they are connected to. For instance, actor C has four contacts in the network. However, they have met only a few times with A but meet quite a lot times with their other contacts. This means that C has a weak tie with A and a strong tie to the rest of their friends.\n\n\n\n\n\nFigure 14.2: A directed weighted graph\n\n\n\n\nWeighted graphs can also be directed, like the one shown in Figure 14.2. The number along each asymmetric tie could be the number of times one actor calls or texts the other, or the number of times they retweet the other other person, or the number of times they like a post from the other person on Instagram. Note that all of these things can lead to imbalance, such that in weighted graphs, relationships can be non-reciprocal even if the two actors are connected bi-directionally (as we will see in a future lesson). For instance, C directs an edge of weight \\(w= 20\\) towards I (perhaps a number of texts), but I only sends a directed edge of weight \\(w = 5\\) towards C.\nFrom varying weights among edges in a social network, representing some varying frequency or intensity of the relationship in the real world, we can better understand important sociological phenomena."
  },
  {
    "objectID": "lesson-other-graphs.html#sentiment-relations-and-signed-graphs",
    "href": "lesson-other-graphs.html#sentiment-relations-and-signed-graphs",
    "title": "14  Other Types of Graphs",
    "section": "14.2 Sentiment Relations and Signed Graphs",
    "text": "14.2 Sentiment Relations and Signed Graphs\n\n14.2.1 Sentiment Networks\nSo far we have talked about social relations as having different properties above and beyond being either “on” or “off.” Social relations can be weak or strong or they can be multiplex or uniplex. Another property that social relations can have is valence. That is, you can be connected to other people via either positive or negative links.\nFor instance, you can love or hate someone. You can like or dislike a person. Somebody can consider you their enemy or their friend. A terrible person can bully you, or you a kind person can help you. What all of these contrasts in connectivity have is that they distinguish relations by their valence, and that valence takes on one of two possible values: They can be either be positive or negative relations.1 Social Networks that are composed of valenced relationships are called sentiment networks.\nCan you think of other examples of sentiment networks you have experience with?\n\n\n14.2.2 Signed Graphs\nAs you might have already suspected, there is a special type of graph that is useful for representing sentiment networks. This is called a signed graph. An example of a signed graph is shown in Figure 14.3). This signed graph is complete because all the possible relations between nodes exist. Note also that the signed graph is directed because each node is both a source and a destination node for directed asymmetric ties.\n\n\n\n\n\nFigure 14.3: A signed graph\n\n\n\n\nMathematically, one way to think about a signed graph is as a special kind of multigraph (\\(G_S\\)) featuring two disjoint sets of edges: positive links (\\(E^+\\)) and negative links (\\(E^-\\)). Recall that two sets are disjoint when they don’t share any members. That means their intersection is the empty set. In this case:\n\\[\nE^+ \\cap E^- = \\emptyset\n\\]\nThus, a signed graph is a set of three sets:\n\\[\nG_S = (E^+, E^-, V)\n\\tag{14.2}\\]\nSigned graphs have a number of unique properties. Some of them are the basis for entire network theories, such as balance theory, status theory, and karma theory that we will discuss later. For instance, reciprocity, just as in weighted graphs, takes on a different meaning in complete signed graphs. In the usual graph theory sense, all the relations in Figure 14.3 are “reciprocal” because the graph is complete and thus all the dyads are mutual.\nHowever, in complete signed graphs, reciprocity is better defined as mutual dyads that have the same sentiment going from one node to the other. In a signed graph mutual dyad, the relationship is reciprocal if both people think that they are friends or both people hate one another. A mutual dyad in a signed graph is non-reciprocal if one person likes the other person, but that person hates the first person. In the graph theoretic sense, reciprocal dyads in a signed graph are those that are connected by two asymmetric edges of the type: either positive or negative. A dyad is non-reciprocal if the two nodes are connected by asymmetric edges of different types.\nThus, in Figure 14.3, A and C have a mutually positive relationship; A likes C and C reciprocates by liking A back. In the same way, A and D have a mutually negative relationship; A hates D and D reciprocates by hating A back. While the notion of “reciprocity” in negative interactions like hating, or bullying seems counter-intuitive (because we tend to think of reciprocity as an inherently positive thing), we will see later, when discussing theories of negative interactions, that negative reciprocity makes sense as a driver of human behavior, and may explain important phenomena like the escalation of violence among urban gangs (Papachristos, Hureau, and Braga 2013). Finally, note that nodes B and C have a non-reciprocal sentiment relation; C likes B but B does not reciprocate the sentiment. Instead, B dislikes C. This brings us to another property of signed graphs, which is that this type of “imbalance” makes us think that there is something wrong with this dyadic state, and that something will have to give. Either B starts to hate C (because their feelings are hurt), or B ultimately convinces C to like them back.\nThe idea that there are some states of signed graph that makes “more sense” than others (because the various sentiment relations are reciprocated) is behind the notion of balance. It makes sense to us that if someone likes somebody they should like them back, or if they hate somebody, that other person should hate them back. Those states seem “balanced”; it makes less sense when sentiment relations have opposite signs across a dyad. Imbalance makes you think about a process that in the future will change the state of the links in the graph so that they go from imbalanced (e.g., non-reciprocal sentiment relations) to balanced (reciprocal sentiment relations). We will see later, that this “balance” reasoning can be extended, in signed graphs, to triadic configurations (subsets of three nodes in the graph), and from from there to the entire graph, so that we can speak of balanced and imbalanced triads, and balanced and imbalanced graphs.2"
  },
  {
    "objectID": "lesson-other-graphs.html#advanced-topic-multiplexity-and-multigraphs",
    "href": "lesson-other-graphs.html#advanced-topic-multiplexity-and-multigraphs",
    "title": "14  Other Types of Graphs",
    "section": "14.3 Advanced Topic: Multiplexity and Multigraphs",
    "text": "14.3 Advanced Topic: Multiplexity and Multigraphs\nOne simplifying assumption made in much of previous and contemporary network research, is that people in the network are linked by only type of tie at a time (e.g., linking, friendship, texting). The reality is that connected dyads in social networks are usually connected by multiple type of ties at the same time. For instance, you text your friends, are in the same class as them, and sometimes work together. This means that a friend, who you text frequently, who is also a co-worker and takes the same class as you is linked to you in at least four different ways! This phenomenon, first noticed by early qualitative fieldwork by social network anthropologists (Barnes 1954; Bott 1955) and early quantitative work by sociologists (Verbrugge 1979) is called multiplexity. In a network a multiplex dyad is a dyad in which the two nodes are connected by multiple types of ties at the same time.\n\n\n\n\n\nFigure 14.4: An undirected multigraph\n\n\n\n\nMultiplexity, as a common feature of social life, can be represented using a special type of graph called a multigraph. A multiple graph (\\(G_M\\)) is just like a regular graph, except that instead of having a single edge set \\(E\\), it has multiple edge sets \\((E_1, E_2,\\dots V_K)\\), where \\(K\\) is the total number of different types of relations in the network:\n\\[\nG_M = (E_1, E_2,\\dots E_K, V)\n\\tag{14.3}\\]\nA network diagram of a multigraph is shown in Figure 14.4. This graph has eight nodes joined by three different types of ties (\\(K = 3\\)). The ties in a multigraph are labeled so that we can tell the different kinds apart. In the figure, the type-of-tie labels are represented by different edge colors. For instance, if the three relations we are studying are friendship (blue), co-working (red), and being a member of the same soccer club (green), then we can see nodes D and C are a multiplex dyad because are connected in two distinct ways (they are co-workers and members of the soccer club). Nodes C A are also a multiplex dyad because they are friends who also happen to work together. Nodes B and H, by way of contrast, are a regular old uniplex dyad being connected by a single type of tie (they are both in the soccer club, but do not work together nor do they think of one another as friends). The same goes for the dyad formed by nodes D and F who are just friends who neither work together nor belong to the same club. Finally, note that in Figure 14.4 nodes A and D are part of a regular old null dyad (they are not connected by any type of relation)."
  },
  {
    "objectID": "lesson-other-graphs.html#references",
    "href": "lesson-other-graphs.html#references",
    "title": "14  Other Types of Graphs",
    "section": "References",
    "text": "References\n\n\n\n\nBarnes, John Arundel. 1954. “Class and Committees in a Norwegian Island Parish.” Human Relations 7 (1): 39–58.\n\n\nBott, Elizabeth. 1955. “Urban Families: Conjugal Roles and Social Networks.” Human Relations 8 (4): 345–84.\n\n\nMarsden, Peter V, and Karen E Campbell. 1984. “Measuring Tie Strength.” Social Forces 63 (2): 482–501.\n\n\nPapachristos, Andrew V, David M Hureau, and Anthony A Braga. 2013. “The Corner and the Crew: The Influence of Geography and Social Networks on Gang Violence.” American Sociological Review 78 (3): 417–47.\n\n\nVerbrugge, Lois M. 1979. “Multiplexity in Adult Friendships.” Social Forces 57 (4): 1286–1309."
  },
  {
    "objectID": "lesson-special-matrices.html#the-reachability-matrix",
    "href": "lesson-special-matrices.html#the-reachability-matrix",
    "title": "15  Special Matrices",
    "section": "15.1 The Reachability Matrix",
    "text": "15.1 The Reachability Matrix\nConsider the directed graph shown in Figure 8.2. Earlier, we derived an asymmetric adjancency matrix from a graph similar to this one. But directed graphs do not only encode information about adjacency relations between nodes. As we saw in the graph theory lessson, in directed graphs (and as we will later, in disconnected undirected graphs), there is another pairwise relationship between nodes we may be interested in; namely, reachability (Krackhardt 1994):\nIn a graph, node B is said to be reachable by node A if there is path (of any length) that has A as the origin node and B as the destination node. In that case, we say that A can reach B.\nSometimes, it is useful to encode reachability relations between nodes to in order to compute some important graph metrics. This is done using the reachability matrix, written \\(D^r\\). The reachability matrix is just like the adjacency matrix, except that instead of putting a one the corresponding matrix cell if the node in the row sends a tie to the node in the respective column, we put a one in the corresponding matrix cell if the node in the rode can reach the node in the column via a path. Table 15.1 shows the reachability matrix corresponding to the directed graph shown in Figure 8.2.\nNote that if a node sends a regular old directed edge (a path of length one!) that counts for reachability too. This means that if the cell corresponding to the relationship between two nodes has a one in the adjacency matrix, then it should also have a one in the reachability matrix.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n1\n0\n1\n0\n1\n1\n\n\nB\n1\n–\n0\n1\n0\n1\n1\n\n\nC\n1\n1\n–\n1\n0\n1\n1\n\n\nD\n1\n1\n0\n–\n0\n1\n1\n\n\nE\n1\n1\n1\n1\n–\n1\n1\n\n\nF\n1\n1\n0\n1\n0\n–\n1\n\n\nG\n1\n1\n0\n1\n0\n1\n–\n\n\n\nTable 15.1: Reachability matrix corresponding to a directed graph.\n\n\nSo show does this work? Let us take a look at how the first row (corresponding to whether node A can reach the other nodes in the graph) was filled out. First we ask, can node A reach node B? The answer is yes, because they are is a direct link between them! Node A sends a tie to node B, so the corresponding cell \\(d^r_{12} = 1\\).\nThen we ask, can node A reach node C? The answer is no. Note that there is no directed path we can trace that would start from node A and end in node C. So we put a zero in the corresponding cell (row 1, column 3) of the reachability matrix (\\(d^r_{13} = 0\\)).\nFurther, we can ask, can node A reach node D? Note that here the answer is yes! While node A is not directly connected to node D, they are indirectly connected, so node A can reach node D in two ways: First, via a path of length two that goes: \\(A \\rightarrow B, B \\rightarrow D\\), and via path of length three that goes: \\(A \\rightarrow F, F \\rightarrow G, G \\rightarrow D\\).1 So \\(d^r_{13} = 1\\).\nWe can continue like this and finish the row for node A and the the rows for all the other nodes. If we do we end up with numbers shown in $tbl-reachmat. There are some interesting things about this matrix. Reading across the rows for each node, we can figure out the number of other nodes that that particular node can reach.\nNote than an interesting feature of the graph shown in Figure 8.2 is that while some nodes (like node A) cannot reach all the nodes in the graph, other nodes (like node E) can! It seems like E has access to everyone in the network, whether directly or indirectly. Maybe they are pretty important (Krackhardt 1994). Node C is almost like node E. They can reach almost everyone in the network, except for E. Maybe they are the second in command.\nIn the same way, note that reading across the columns, tell us whether a particular node is reachable by the other nodes. So we see that some nodes, like A, B, D, F, and G are reachable by everyone. Other nodes like E are reachable by no one. Finally, a node like C is not reachable by almost anyone else, except node E. It seems like reachability can encode some interesting properties, and can be used to develop some graph metrics related to power and hierarchy.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\n\n\n\n\nA\n–\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n1\n\n\nB\n0\n–\n0\n0\n1\n1\n1\n0\n0\n0\n0\n0\n0\n\n\nC\n0\n0\n–\n0\n0\n0\n0\n1\n1\n1\n0\n0\n0\n\n\nD\n0\n0\n0\n–\n0\n0\n0\n0\n0\n0\n1\n1\n1\n\n\nE\n0\n0\n0\n0\n–\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nF\n0\n0\n0\n0\n0\n–\n0\n0\n0\n0\n0\n0\n0\n\n\nG\n0\n0\n0\n0\n0\n0\n–\n0\n0\n0\n0\n0\n0\n\n\nH\n0\n0\n0\n0\n0\n0\n0\n–\n0\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0\n0\n0\n–\n0\n0\n0\n0\n\n\nJ\n0\n0\n0\n0\n0\n0\n0\n0\n0\n–\n0\n0\n0\n\n\nK\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n–\n0\n0\n\n\nL\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n–\n0\n\n\nM\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n–\n\n\n\nTable 15.2: Reachability matrix corresponding to a tree graph.\n\n\nFor instance, the reachability matrix corresponding to a perfectly hierarchical tree graph containing only antisymmetric relations, such as the one shown in Figure 8.7, has an interesting property. This is shown in Table 15.2. If you look at the reachability matrix’s lower-triangle, it is full of zeroes! The only ones present in the matrix are contained in the matrix’s upper-triangle. This means that when looking at a reachability matrix of any directed graph, we can get a sense of much they approximate a pure antisymetric hierachy by counting the number of ones that appear in the reachability matrix’s lower-trinagle."
  },
  {
    "objectID": "lesson-special-matrices.html#the-geodesic-distance-matrix",
    "href": "lesson-special-matrices.html#the-geodesic-distance-matrix",
    "title": "15  Special Matrices",
    "section": "15.2 The Geodesic Distance Matrix",
    "text": "15.2 The Geodesic Distance Matrix\nConsider the directed graph shown in Figure 8.1 again. In Table 6.5 we derived an symmetric adjacency matrix from the same graph. However, as noted in the graph theory lesson and our previous discussion of indirect connections, adjacency is only one way (the direct way) in which nodes can be connected in graph. A particularly important way in which two nodes can be connected is via shortest paths. The length of shortest path between two nodes is called the geodesic distance between them. So it is possible to create a matrix D in which each cell d\\(_{ij}\\) contains the length of the shortest path between the row node i and the column node j. This is called the distance matrix for the corresponding graph. The D matrix corresponding to the graph shown in Figure 8.1, is shown in Table 15.3.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\n2\n3\n3\n3\n\n\nB\n1\n0\n1\n1\n2\n2\n3\n3\n3\n\n\nC\n1\n1\n0\n1\n1\n2\n3\n3\n3\n\n\nD\n1\n1\n1\n0\n1\n1\n2\n2\n2\n\n\nE\n1\n2\n1\n1\n0\n2\n3\n3\n3\n\n\nF\n2\n2\n2\n1\n2\n0\n1\n1\n1\n\n\nG\n3\n3\n3\n2\n3\n1\n0\n1\n1\n\n\nH\n3\n3\n3\n2\n3\n1\n1\n0\n1\n\n\nI\n3\n3\n3\n2\n3\n1\n1\n1\n0\n\n\n\nTable 15.3: Distance matrix for an undirected graph.\n\n\nThe distance matrix reveals a number of things about the network. First, note that nodes that are adjacent in Figure 8.1 have a geodesic distance of 1.0 by definition. In addition, nodes are at minimum distance from themselves, so we put a value of 0 in the diagonal cells; d\\(_{ij} = 0\\) for all \\(i=j\\). Second, note that the maximum geodesic distance between any two nodes in the graph is 3.0. As we saw in the lesson on indirect connections, this is an important graph metric, called the graph diameter. Finally, note that just like an undirected graph yields a symmetric adjacency matrix, it also yields a symmetric distance matrix. If the geodesic distance between nodes A and G in an the undirected graph is 3.0, then the geodesic distance between G and A is also 3.0: d\\(_{ij}\\) = d\\(_{ji}\\) for all \\(i\\) and \\(j\\).\nWe can also generate geodesic distance matrices for directed graphs, such as the one shown in Figure 8.2. The corresponding distance matrix for this graph is shown in Table 15.4.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n0\n1\nInf\n2\nInf\n1\nInf\n\n\nB\n1\n0\nInf\n1\nInf\n2\nInf\n\n\nC\n2\n1\n0\n2\nInf\n3\nInf\n\n\nD\n2\n1\nInf\n0\nInf\n3\nInf\n\n\nE\n3\n2\n1\n1\n0\n4\nInf\n\n\nF\n1\n2\nInf\n3\nInf\n0\nInf\n\n\nG\n2\n2\nInf\n1\nInf\n1\n0\n\n\n\nTable 15.4: Distance matrix for a directed graph.\n\n\nThis distance matrix is very different from the one corresponding to the undirected graph. First note that some shortest paths are not defined, because some pairs of nodes are disconnected in the directed graph; that is there is directed path linking them. So the corresponding cells are noted with Inf in the matrix.2 For instance, node A cannot reach nodes C, E or G. Note also that now the matrix is asymmetric, so the numbers above the diagonal do not have to match the numbers below the diagonal. Thus, while node A cannot reach node G via a shortest path, node G can reach node A via shortest path of length 2."
  },
  {
    "objectID": "lesson-special-matrices.html#the-shortest-paths-matrix",
    "href": "lesson-special-matrices.html#the-shortest-paths-matrix",
    "title": "15  Special Matrices",
    "section": "15.3 The Shortest Paths Matrix",
    "text": "15.3 The Shortest Paths Matrix\nRecall that in our discussion of shortest paths in the indirect connectivity lesson, we noted that nodes can be connected by more than one shortest path at the same time. So sometimes it is useful to create a matrix that records this number for each pair of nodes. This is called the shortest paths matrix (S). Each cell in the matrix s\\(_{ij}\\) gives us the number of shortest paths connecting the row node i with the column node j. The shortest path matrix corresponding to the graph in Figure 8.1, is shown in Table 15.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\n2\n3\n3\n3\n\n\nB\n1\n0\n1\n1\n2\n2\n3\n3\n3\n\n\nC\n1\n1\n0\n1\n1\n2\n3\n3\n3\n\n\nD\n1\n1\n1\n0\n1\n1\n2\n2\n2\n\n\nE\n1\n2\n1\n1\n0\n2\n3\n3\n3\n\n\nF\n2\n2\n2\n1\n2\n0\n1\n1\n1\n\n\nG\n3\n3\n3\n2\n3\n1\n0\n1\n1\n\n\nH\n3\n3\n3\n2\n3\n1\n1\n0\n1\n\n\nI\n3\n3\n3\n2\n3\n1\n1\n1\n0\n\n\n\nTable 15.5: Shortest paths matrix for an undirected graph.\n\n\nThe S matrix contains useful information. For instance, it tell us that some pairs of nodes in the graph, have multiple ways of reaching other nodes to which they are not directly connected using shortest paths. For instance, actor A can get to actor G via three distinct shortest paths. So this gives us a sense of the capacity of that actor to reach the other one in an efficient way; if one of those shortest paths were to be compromised, A would still be able to send something to G via the other non-compromised paths."
  },
  {
    "objectID": "lesson-special-matrices.html#the-neighborhood-overlap-matrix",
    "href": "lesson-special-matrices.html#the-neighborhood-overlap-matrix",
    "title": "15  Special Matrices",
    "section": "15.4 The Neighborhood Overlap Matrix",
    "text": "15.4 The Neighborhood Overlap Matrix\nAs we noted in the original graph theory lesson, it is possible for the neighborhood of two nodes in a graph to overlap. Recall that for each node, we define its neighborhood as the set of other nodes that they are adjacent to. That means the neighborhood between two nodes can have members in common.\nThis can be used as a measure of the overlap of the neighborhood between two nodes. For instance, imagine you have a friend and that friend knows all your friends and you know all their friends. In which case we would say that the overlap between your node neighborhoods is pretty high; in fact the two neighborhoods overlap completely.\nNow imagine you just met a new person online who lives in a far away country, and as far as you know, they know none of your friends and you know none of their friends. In which case, we would say that the overlap if the two neighborhoods is nil or as close to zero as it can get.\n\\[\n  o_{ij} = \\frac{|\\mathcal{N}(i) \\cap \\mathcal{N}(j)|}{|\\mathcal{N}(i) \\cup \\mathcal{N}(j)|}\n\\tag{15.1}\\]\nGiven a graph, we can construct the neighborhood overlap matrix for the graph O, containing such overlap scores between the neighborhood sets of each pair of nodes in the graph. The overlap score ranges from 0 (not overlap), to 1.0 (complete overlap), with values in-between for partial overlap (which is the more common case). Each cell in the matrix is filled in using equation Equation 22.2.\nThis equation says that the overlap between node i and node j, written \\(o_{ij}\\), is equivalent to the cardinality (||) of the set defined by the intersection (\\(\\cap\\)) of i’s neighborhood (\\(\\mathcal{N}(i)\\)) and j’s neighborhood (\\(\\mathcal{N}(j)\\)), or the number of common neighbors, divided by the cardinality of the set defined by the union (\\(\\cup\\)) of i’s neighborhood (\\(\\mathcal{N}(i)\\)) and j’s neighborhood (\\(\\mathcal{N}(j)\\)), or the total number of neighbors.\nThus, the overlap is the number of common neighbors, divided by the number of total neighbors. For instance, the neighborhood overlap for the undirected graph shown in Figure 8.1 is shown in Table 15.6.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n1.00\n0.40\n0.60\n0.50\n0.40\n0.14\n0.00\n0.00\n0.00\n\n\nB\n0.40\n1.00\n0.40\n0.33\n1.00\n0.17\n0.00\n0.00\n0.00\n\n\nC\n0.60\n0.40\n1.00\n0.50\n0.40\n0.14\n0.00\n0.00\n0.00\n\n\nD\n0.50\n0.33\n0.50\n1.00\n0.33\n0.00\n0.14\n0.14\n0.14\n\n\nE\n0.40\n1.00\n0.40\n0.33\n1.00\n0.17\n0.00\n0.00\n0.00\n\n\nF\n0.14\n0.17\n0.14\n0.00\n0.17\n1.00\n0.40\n0.40\n0.40\n\n\nG\n0.00\n0.00\n0.00\n0.14\n0.00\n0.40\n1.00\n0.50\n0.50\n\n\nH\n0.00\n0.00\n0.00\n0.14\n0.00\n0.40\n0.50\n1.00\n0.50\n\n\nI\n0.00\n0.00\n0.00\n0.14\n0.00\n0.40\n0.50\n0.50\n1.00\n\n\n\nTable 15.6: Neighborhood Overlap Matrix for an undirected graph.\n\n\nLooking at the first row of the matrix, we can see that nodes A and C have a pretty high neighborhood overlap score \\(o_{AC} = 0.60\\). But the node neighborhood A has no overlap with that of nodes G, H and I, as is evident by looking at Figure 8.1.\nWe can examine the overlap pattern of each node in Figure 8.1 by going down each row of the matrix. Note that the common neighbors matrix is symmetric: If A has an overlap of o with B, then B necessarily has the same overlap score with A. So all the information in the neighborhood overlap matrix is contained in either the lower or upper triangle.\nWe can think of neighborhood overlap as a measure of structural similarity of two nodes in a graph based on their pattern of social connections. In fact, the formula shown in equation @ref(eq:overlap) is called Jaccard’s Coefficient (named after the French Botanist Paul Jaccard, who introduced it) and it is generally used (along with many variations) as a measure of similarity between two sets (Jaccard 1901).\nSo looking at Table 15.6, we can see node F is most similar to nodes G, H and I in the graph shown in Figure 8.1, and least similar to node D.\nCan you think of which of your friends you are most similar to in terms of neighborhood overlap?\nAs we noted in the lesson on graph theory, common neighbors are defined for all the dyads in the network, whether they are connected or null. So that means that two nodes can have overlapping neighborhoods even if they do not have tie between them! Sometimes it happens that you meet someone new and then you realize that you had friends in common. This is such a common occurrence that it has a name: the small world phenomenon (Milgram 1967)."
  },
  {
    "objectID": "lesson-special-matrices.html#references",
    "href": "lesson-special-matrices.html#references",
    "title": "15  Special Matrices",
    "section": "References",
    "text": "References\n\n\n\n\nJaccard, Paul. 1901. “Distribution of the Alpine Flora in the Dranse’s Basin and Some Neighbouring Regions.” Bulletin de La Societe Vaudoise Des Sciences Naturelles 37 (1): 241–72.\n\n\nKrackhardt, David. 1994. “Graph Theoretical Dimensions of Informal Organizations.” Computational Organization Theory 89 (112): 123–40.\n\n\nMilgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67."
  },
  {
    "objectID": "lesson-matrix-operations.html#matrix-addition",
    "href": "lesson-matrix-operations.html#matrix-addition",
    "title": "16  Matrix Operations",
    "section": "16.1 Matrix Addition",
    "text": "16.1 Matrix Addition\nPerhaps the simplest operation we can do with matrices is add them up. To add two matrices, we simply add up the corresponding entries in each cell. In matrix notation:\n\\[\n\\mathbf{H} + \\mathbf{C} = h_{ij} + c_{ij}\n\\tag{16.1}\\]\nWhere \\(h_{ij}\\) is the corresponding entry for nodes i and j in the hanging out adjacency matrix \\(\\mathbf{H}\\), and \\(c_{ij}\\) is the same entry in the co-working adjacency matrix \\(\\mathbf{C}\\).\nWhy would we want to do this? Well, if we were studying the network shown in Figure 16.1, we might be interested in which dyads have uniplex (or single-stranded) relations, and which ones have multiplex (or multi-stranded) relations. That is, while some actors in the network either hang out together or work together, some of the do both. Adding up the adjacency matrices shown in Table 16.1, will tell us who these are. The result is shown in Table 16.2.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    1 \n    2 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    -- \n    2 \n    2 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    2 \n    2 \n    -- \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    2 \n    1 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    -- \n    2 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    2 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    2 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    2 \n    -- \n    1 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    -- \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    -- \n    1 \n    2 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    1 \n  \n  \n    L \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    2 \n    1 \n    -- \n  \n\n\n\nTable 16.2:  Uniplex and Multiplex relationship matrix. \n\n\nTable 16.2 shows that the \\(BC\\) dyad has a multiplex relation (there is a “2” in the corresponding cell entry) and so does the \\(AC\\), \\(FH\\), \\(GH\\), \\(EF\\), and \\(JL\\) dyads."
  },
  {
    "objectID": "lesson-matrix-operations.html#the-matrix-dot-product",
    "href": "lesson-matrix-operations.html#the-matrix-dot-product",
    "title": "16  Matrix Operations",
    "section": "16.2 The Matrix Dot Product",
    "text": "16.2 The Matrix Dot Product\nAnother way of figuring out which pairs of people in a network have multiplex ties is to compute the matrix dot product (symbol: \\(\\cdot\\)). Just like matrix addition, we find the matrix dot product by multiplying the corresponding entries in each of the matrices. In matrix format:\n\\[\n\\mathbf{H} \\mathbf{\\cdot} \\mathbf{C} = h_{ij} \\times c_{ij}\n\\tag{16.2}\\]\nIf we take the dot product of two adjacency matrices like \\(\\mathbf{H}\\) and \\(\\mathbf{C}\\), then the resulting matrix will have a one in a given cell only if \\(h_{ij} = 1\\) and \\(c_{ij} = 1\\). Otherwise, it will have a zero. This means that the dot product of two adjacency matrices will retain only the multiplex ties and erase all the other ones. The result of the dot products of the adjancency matrices shown in Table 16.1 is shown in Table 16.3.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    0 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    -- \n  \n\n\n\nTable 16.3:  Multiplex relationship matrix. \n\n\nAs we can see, the only dyads that have non-zero entries in Table 16.3 are the multiplex dyads in Table 16.2. The resulting network, composed of the combined “hanging + co-working” relation is shown in Figure 16.1 (c). Note that this network is much more sparse than either of the other two, since there’s an edge between nodes only when they are adjacent in both the Figure 16.1 (a) and Figure 16.1 (b) networks."
  },
  {
    "objectID": "lesson-matrix-operations.html#sec-trans",
    "href": "lesson-matrix-operations.html#sec-trans",
    "title": "16  Matrix Operations",
    "section": "16.3 The Matrix Transpose",
    "text": "16.3 The Matrix Transpose\nOne thing we can do with a matrix is “turn it 90 degrees” so that the rows of the new matrix are equal to the columns of the resulting matrix and the columns of the first matrix equal the rows of the resulting matrix. This is called the matrix transpose (symbol: \\(^T\\)).\nFor instance, if we have a matrix \\(\\mathbf{A}_{4 \\times 5}\\) of dimensions \\(4 \\times 5\\) (four rows and five columns), then the transpose \\(A^T_{5 \\times 4}\\) will have five rows and four columns, with the respective entries in each matrix given by the formula:\n\\[\na_{ij} = a^T_{ji}\n\\] That is the number that in the first matrix appears in the \\(i^{th}\\) row and \\(j^{th}\\) column now appears in the transposed version of the matrix in the \\(j^{th}\\) row and \\(i^{th}\\) column.\nAn example of a matrix and its tranpose is shown in Table 16.4.\n\n\nTable 16.4: A matrix and its transpose\n\n\n\n\n(a) Original Matrix. \n\n  \n    3 \n    4 \n    5 \n  \n  \n    7 \n    9 \n    3 \n  \n  \n    4 \n    6 \n    2 \n  \n  \n    5 \n    3 \n    4 \n  \n  \n    2 \n    5 \n    4 \n  \n\n\n\n\n\n\n(b) Transposed Matrix. \n\n  \n    3 \n    7 \n    4 \n    5 \n    2 \n  \n  \n    4 \n    9 \n    6 \n    3 \n    5 \n  \n  \n    5 \n    3 \n    2 \n    4 \n    4 \n  \n\n\n\n\n\n\nSo let’s check out how the transpose works. The original matrix in Table 16.4 (a) has five rows and three columns. The transposed matrix has three rows and five columns. We can find the same numbers in the original and transposed matrices by switching the rows and columns. Thus, in the original matrix, the number in third row and second column is a six (\\(a_{32} = 6\\)). In the transposed version of the matrix, that same six is in second row and third column (\\(a^T_{23} = 6\\)). If you check, you’ll see that’s the case for each number! Thus, the transposed version of a matrix has the same information as the original, it is just that the rows and columns are switched. While this might seem like a totally useless thing to do (or learn) at the moment, we will see in Chapter 19 that the matrix transpose comes in very handy in the analysis of social networks, and particular in the analysis of two mode networks and cliques."
  },
  {
    "objectID": "lesson-matrix-operations.html#sec-matmult",
    "href": "lesson-matrix-operations.html#sec-matmult",
    "title": "16  Matrix Operations",
    "section": "16.4 Matrix Multiplication",
    "text": "16.4 Matrix Multiplication\nMatrix multiplication (symbol: \\(\\times\\)) is perhaps the more complex of the matrix algebra operations we will cover. It is a bit involved, but relatively easy once you get the hang of it. We will begin with a simple example before doing more complicated stuff.\n\n16.4.1 Matrix Multiplication Rules\nFirst, we will let out the basic rules of matrix multiplication:\n\nYou can always multiply two matrices as long as the number of columns of the first matrix equal the rows of the second matrix. To check whether this is the case, all you have to do is put the two matrices side by side and list their dimensions.\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6}\n\\] - The two little “fives” in bold are called the inner dimensions of the two matrices. The little “three” on the left and the little “six” on the right are called the outer dimensions. So another way of stating the first rule of matrix multiplication is that the product of two matrices is defined as long as their inner dimensions equal to one another when you line them up from left to right.\n\nWhen the number of columns of a matrix equal the number of rows of another matrix so that their inner dimensions match we say that the the two matrices are conformable. When this is not the case, we say the matrices are non-conformable.\nThus, another way of stating the first rule is that only the product of conformable matrices is defined. If the matrices are not conformable then their product is not defined (e.g., there is no answer to the question of what we get if we multiply them!).\nThis means that unlike numbers or the matrix dot product, where the order of the two things you are multiplying doesn’t matter (\\(4 \\times 3 = 3 \\times 4\\) or \\(\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{B} \\cdot \\mathbf{A}\\)), in matrix multiplication it does matter. Alas, for any two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\),\n\n\\[\n\\mathbf{A} \\times \\mathbf{B} \\neq \\mathbf{B} \\times \\mathbf{A}\n\\]\n\nWhen you multiply a matrix times another matrix, the resulting matrix will have number of rows equal to the number of rows of the first matrix and number of columns equal to the number of columns of the second matrix. Thus:\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6} = \\mathbf{C}_{3 \\times 6}\n\\tag{16.3}\\]\n\nEquation 16.3 says that the product of a three by five matrix \\(\\mathbf{A}\\) (three rows and five columns) times a five by six matrix \\(\\mathbf{B}\\) (five rows and six columns) is a third matrix \\(\\mathbf{C}\\) with three rows and six columns. Another way of saying this last rule is that the product of two conformable matrices will have dimensions equal to their outer dimensions.\n\n\n\n16.4.2 Multiplying a Matrix Times its Transpose\n\nBy definition, as discussed in Section 16.4.2, the rows of a matrix are equal to the columns of its transpose, and vice versa. The product of a matrix times its transpose and the transpose times the original matrix is always defined, no matter what the dimensions of the original matrix are. Thus,\n\n\\[\n\\mathbf{A} \\times \\mathbf{A}^T = defined!\n\\]\n\\[\n\\mathbf{A}^T \\times \\mathbf{A} = defined!\n\\]\n\nWhen you multiply a matrix times its transpose, the resulting matrix will be a square matrix with number of rows and columns equal to the number of rows of the original matrix. For instance, say matrix \\(\\mathbf{A}_{5 \\times 3}\\) is of dimensions \\(5 \\times 3\\) (like the matrix shown in Table 16.4 (a)). Then its transpose \\(A^T_{3 \\times 5}\\) will be of dimensions \\(3 \\times 5\\) (like the matrix shown in Table 16.4 (b)). That means the product of the matrix times its transpose will be:\n\n\\[\n\\mathbf{A}_{5 \\times 3} \\times \\mathbf{A}_{3 \\times 5}^T = \\mathbf{B}_{5 \\times 5}\n\\tag{16.4}\\]\n\nEquation 16.4 says that a five by three matrix multiplied by its transposed yields a square matrix \\(\\mathbf{B}\\) of dimensions five by five (a square matrix with five rows and five columns). In the same way,\n\n\\[\n\\mathbf{A}_{3 \\times 5}^T \\times \\mathbf{A}_{5 \\times 3} = \\mathbf{B}_{3 \\times 3}\n\\tag{16.5}\\]\n\nEquation 16.5 says that the transpose of a five by three matrix multiplied by the original yields a product matrix \\(\\mathbf{B}\\) of dimensions three by three (a square matrix with three rows and three columns).\n\n\n\n16.4.3 Matrix Powers\n\nYou can multiply a matrix times itself to get matrix powers but only if matrix is a square matrix (has the same number of rows and columns). Thus,\n\n\\[\n\\mathbf{A}^2 = \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^3 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^4 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^n = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\ldots\n\\]\n\nFor all square matrices \\(\\mathbf{A}\\) of any dimension. Since matrices used to represent social networks, like the adjacency matrix are square matrices, that means that you can always find the powers of an adjacency matrix.\nWhen you multiply a square matrix times another square matrix of the same dimensions, the resulting matrix is of the same dimensions as the original two matrices. Thus,\n\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = \\mathbf{A}^2_{5 \\times 5}\n\\]"
  },
  {
    "objectID": "lesson-matrix-operations.html#sec-matmultex",
    "href": "lesson-matrix-operations.html#sec-matmultex",
    "title": "16  Matrix Operations",
    "section": "16.5 Matrix Multiplication Examples",
    "text": "16.5 Matrix Multiplication Examples\nNow let’s see some examples of how matrix multiplication works. Table 16.5 shows the result of multiplying the matrix shown in Table 16.4 (a) times its transpose, shown in Table 16.4 (b).\n\n\n\n\n\n\n  \n    50 \n    72 \n    46 \n    47 \n    46 \n  \n  \n    72 \n    139 \n    88 \n    74 \n    71 \n  \n  \n    46 \n    88 \n    56 \n    46 \n    46 \n  \n  \n    47 \n    74 \n    46 \n    50 \n    41 \n  \n  \n    46 \n    71 \n    46 \n    41 \n    45 \n  \n\n\n\nTable 16.5:  Matrix resulting from multiplying a matrix times its transpose \n\n\nNow where the heck did these numbers come from? Don’t panic. We’ll break it down. First, let’s begin with the number \\(50\\) in cell corresponding to the first row and first column of Table 16.5. To find out where this number came from, let’s look at the first row of Table 16.4 (a), composed of the vector \\(\\{3, 4, 5\\}\\), and the first-column of Table 16.4 (b), composed of the same vector \\(\\{3, 4, 5\\}\\). Now, the number \\(50\\) comes from the fact that we multiply each of the corresponding entries of the two vectors, and then add them up, as follows:\n\\[\n(3 \\times 3) + (4 \\times 4) + (5 \\times 5) = 9 + 16 + 25 = 50\n\\]\nNeat! Now let’s see where the number \\(74\\) in the fourth row and second column of Table 16.5 came from. For that we look at the entries in the fourth row of Table 16.4 (a), composed of the vector \\(\\{5, 3, 4\\}\\) and the second column of Table 16.4 (b) composed of the vector \\(\\{7, 9, 3\\}\\). Like before, we take the first number of the first vector and multiply it by the first number of the second vector, the second number of the first vector and multiply it by the second number of the second vector, and the third number of the first vector and multiply it by the third number of the second vector and add up the results:\n\\[\n(5 \\times 7) + (3 \\times 9) + (4 \\times 3) = 35 + 27 + 12 = 74\n\\] And we keep on going like this to get each of the twenty five numbers in Table 16.5 (there are twenty five numbers because Table 16.5 has five rows and five columns and five times five equal twenty five). In general terms, the number in the \\(i^{th}\\) row and \\(j^{th}\\) column of Table 16.5 is equal to the sum of the products of the numbers in the \\(i^{th}\\) row of the Table 16.4 (a) and the \\(j^{th}\\) column of Table 16.4 (b).\nNote that the resulting product matrix shown in Table 16.5 is symmetric. The same numbers that appear in the upper-triangle also appear in the lower triangle, such that \\(b_{ij} = b_{ji}\\). So once you know the numbers in one of the triangles, you can fill up the numbers in the other one without having to do all the multiplying and adding up!\nNow, let’s multiply the matrix in Table 16.4 (b) times the matrix in Table 16.4 (a). As the rules of matrix multiplication show, this will result in a matrix of dimensions \\(3 \\times 3\\) because Table 16.4 (b) has three rows and $tbl-trans-1 has three columns. This is shown in Table 16.6.\n\n\n\n\n\n\n  \n    103 \n    124 \n    72 \n  \n  \n    124 \n    167 \n    91 \n  \n  \n    72 \n    91 \n    70 \n  \n\n\n\nTable 16.6:  Matrix resulting from multiplying a matrix times its transpose \n\n\nLike before, if we want to figure out where the number \\(72\\) in the third row and first column of Table 16.6 came from, we go to the first row of Table 16.4 (b) composed of the vector \\(\\{5, 3, 2, 4, 4\\}\\) and the first column of Table 16.4 (a), composed of the vector \\(\\{3, 7, 4, 5, 2\\}\\) match up each number in terms of order, multiplying them and add up the result:\n\\[\n(5 \\times 3) + (3 \\times 7) + (2 \\times 4) + (4 \\times 5) + (4 \\times 2) =\n\\]\n\\[\n15 + 21 + 8 + 20 + 8 = 72\n\\]\n\n\nTable 16.7: Powers of a matrix.\n\n\n\n\n(a) A matrix. \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) Matrix squared. \n\n  \n    1 \n    1 \n    2 \n    0 \n  \n  \n    1 \n    1 \n    2 \n    1 \n  \n  \n    2 \n    1 \n    2 \n    2 \n  \n  \n    1 \n    1 \n    1 \n    2 \n  \n\n\n\n\n\n\n(c) Matrix cubed. \n\n  \n    2 \n    2 \n    3 \n    3 \n  \n  \n    3 \n    2 \n    4 \n    3 \n  \n  \n    4 \n    3 \n    5 \n    4 \n  \n  \n    3 \n    2 \n    4 \n    2 \n  \n\n\n\n\n\n\nMatrix powers work the same as regular matrix multiplication, except that we are working on just one matrix not two. So for instance, the number \\(2\\) in the first row and third column of Table 16.7 (b) comes from the numbers in the first row of Table 16.7 (a) (\\(\\{0, 1, 0, 1\\}\\)) and the numbers in the third column of Table 16.7 (a) (\\(\\{0, 1, 1, 1\\}\\)). We line them up, multiplying them, and add them:\n\\[\n(0 \\times 1) + (1 \\times 1) + (0 \\times 1) + (1 \\times 1) = 0 + 1 + 0 + 1 = 2\n\\] Since we are working with a binary matrix, the product of each of the cell entries will be either a zero (when at least one of the entries is zero) or a one (when both entries are one).\nTo get the cubed entries in Table 16.7 (c), we just take Table 16.7 (b) as the first matrix and Table 16.7 (a) as the second matrix, and do matrix multiplication magic. Thus, to get the number \\(4\\) in the third row and fourth column of Table 16.7 (c), we take the numbers in the third row of Table 16.7 (b) \\(\\{2, 1, 2, 2\\}\\) and the numbers in the fourth column of Table 16.7 (a) \\(\\{1, 0, 1, 0\\}\\), line them up, multiply them, and add them:\n\\[\n(2 \\times 1) + (1 \\times 0) + (2 \\times 1) + (1 \\times 0) = 2 + 0 + 2 + 0 = 4\n\\]\nPretty easy!"
  },
  {
    "objectID": "lesson-matrix-operations.html#matrix-multiplication-of-vectors",
    "href": "lesson-matrix-operations.html#matrix-multiplication-of-vectors",
    "title": "16  Matrix Operations",
    "section": "16.6 Matrix Multiplication of Vectors",
    "text": "16.6 Matrix Multiplication of Vectors\nRecall from ?sec-degset that a vector is a sequence of numbers of a given length. So for instance, the vector \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\) is a vector of length five.\nWell, and here comes the big reveal, it turns out that another way to think of a vector, is as a special case of matrix. That is, a matrix with one row, and as many columns as the length of the vector! This is a called a row vector. So the row vector \\(\\mathbf{a}\\) vector can be thought of as a matrix of dimensions \\(1 \\times 5\\) (one row and five columns) or \\(\\mathbf{A}_{1 \\times 5}\\).\nIn matrix form:\n\n\n\n\n\n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\nTable 16.8:  Matrix resulting from multiplying a matrix times its transpose \n\n\nSince vectors are matrices, we can perform the same type of matrix operations on them as we did with matrices. For instance, we can compute the transpose of a vector. In the case of \\(\\mathbf{a}\\), the transpose \\(\\mathbf{a}^T\\) is:\n\n\n\n\n\n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\nTable 16.9:  Matrix resulting from multiplying a matrix times its transpose \n\n\nThe transpose of a row vector is called (you may have guessed) a column vector. The column vector in Table 16.9 is a matrix with five rows and one column.\nThis also means that the same rules of matrix multiplication apply. For instance, we can always multiply a row vector times a column vector, because it is the equivalent of multypling a matrix times its transpose, and we have already seen in Section 16.4.2, that this can always be done:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{a}^T_{5 \\times 1} = b_{1 \\times 1}\n\\tag{16.6}\\]\nEquation 16.7 says that the product of the \\(1 \\times 5\\) row vector \\(\\mathbf{a}\\) times a \\(5 \\times 1\\) column vector \\(\\mathbf{a}^T\\) is a \\(1 \\times 1\\) “matrix” otherwise known as a scalar (that is, a regular old number). We’ve already seen examples of this, because in regular matrix multiplication, each cell of the product matrix is a scalar obtained from multiplying the corresponding terms taken from a row of the first matrix (which is a row vector) times those of the column of the second matrix (which is a column vector).\nSo in this case this would be:\n\\[\n(2 \\times 2) + (4 \\times 4) + (7 \\times 7) + (2 \\times 2) + (4 \\times 4) =\n\\]\n\\[\n4 + 16 + 49 + 4 + 16 = 89\n\\]\n\nThe first rule of vector matrix multiplication is that you can always multiply a row vector times a column vector (even when their entries are not the same) as long as they are the same length (e.g., the number of columns of the row vector equal the number of rows of the column vector).\nThe second rule of vector matrix multiplication is that when you multiply a row vector times another a column vector the result is always scalar (a single number).\n\nNow notice that if we change the order, and multiply the transpose of a vector times the original? This should be allowed because it conforms to the rules that we have already discussed:\n\\[\n\\mathbf{a}^T_{5 \\times 1} \\times \\mathbf{a}_{1 \\times 5} = B_{5 \\times 5}\n\\tag{16.7}\\]\nThis matrix multiplication is defined because the inner dimensions of the two matrices (the column and row vectors) are the same (one). But note that, according to the rules of matrix multiplication, when you multiply the transpose of a vector times the original, the result is a square matrix, with dimensions \\(n \\times n\\) where \\(n\\) is the length of the original row vector (the number of columns). In our example if the original vector is \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\), then \\(\\mathbf{a}^T \\times \\mathbf{a}\\) is equal to the matrix shown in Table 16.10.\n\n\n\n\n\n\n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n  \n    14 \n    28 \n    49 \n    14 \n    28 \n  \n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n\n\n\nTable 16.10:  Matrix resulting from multiplying a matrix times its transpose \n\n\n\nSo, the third and final rule of vector matrix multiplication is that when you multiply a column vector times a row vector of the same length, the result is a square matrix of row and column dimensions equal to the length of the original vectors."
  },
  {
    "objectID": "lesson-matrix-operations.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "href": "lesson-matrix-operations.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "title": "16  Matrix Operations",
    "section": "16.7 Multiplying a Vector Times A Matrix (and Vice Versa)",
    "text": "16.7 Multiplying a Vector Times A Matrix (and Vice Versa)\nSince vectors are just matrices, it means that we can always multiply a vector times a matrix (and a matrix times a vector), as long as we follow the matrix multiplication rules laid out in Section 16.4.1.\n\n16.7.1 Row Vector Times Matrix\nFor instance, take the row vector \\(\\mathbf{b} = \\{4, 9, 3, 5\\}\\) and the binary matrix \\(\\mathbf{A}\\) shown in Table 16.7 (a). Because the row vector \\(\\mathbf{b}\\) is of dimensions \\(1 \\times 4\\) and matrix \\(\\mathbf{A}\\) is of dimensions \\(4 \\times 4\\), it is possible to multiply the vector times the matrix as follows:\n\\[\n\\mathbf{b}_{1 \\times 4} \\times \\mathbf{A}_{4 \\times 4} = \\mathbf{c}_{1 \\times 4}\n\\tag{16.8}\\]\nEquation 16.8 says that the product of a \\(1 \\times 4\\) row vector times a \\(4 \\times 4\\) square matrix is another vector of dimensions equal to the original row vector. The result for this example is shown in Table 16.11.\n\n\nTable 16.11: Row vector resulting from multiplying a row vector times a square matrix\n\n\n\n\n(a) 1 X 4 row vector \n\n  \n    4 \n    9 \n    3 \n    5 \n  \n\n\n\n\n\n\n\n\n(b) 4 X 4 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 4 product vector \n\n  \n    8 \n    13 \n    17 \n    7 \n  \n\n\n\n\n\n\nOf course, it is also possible to multiply a row vector times a rectangular matrix (where the number of rows is not necessarily equal to the number of columns), as long as the number of rows of the rectangular matrix equals the length of the original row vector. For instance, take a row vector \\(\\mathbf{a}_{1 \\times 5}\\) shown in Table 16.12 (a) and a matrix \\(B_{5 \\times 3}\\). Its product would be given by:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{B}_{5 \\times 3} = \\mathbf{c}_{1 \\times 3}\n\\tag{16.9}\\]\nEquation 16.9 says that the product of a row vector of dimensions \\(1 \\times 5\\) and a matrix of dimensions \\(5 \\times 3\\) is another row vector \\(\\mathbf{c}\\) of dimensions (\\(1 \\times 3\\)). A numerical example corresponding to this situation is shown in Table 16.12.\n\n\nTable 16.12: Row vector resulting from multiplying row vector times a rectangular matrix\n\n\n\n\n(a) 1 X 5 row vector \n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\n\n\n\n\n\n(b) 5 x 3 matrix \n\n  \n    17 \n    15 \n    8 \n  \n  \n    18 \n    13 \n    9 \n  \n  \n    8 \n    10 \n    12 \n  \n  \n    3 \n    13 \n    19 \n  \n  \n    4 \n    6 \n    3 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 3 product vector \n\n  \n    184 \n    202 \n    186 \n  \n\n\n\n\n\n\nTo get the “179” entry in row one and column one of Table 16.12, we take the entries of the row vector shown in Table 16.12 (a) and multiply them by the corresponding entries in the first column of the matrix shown in Table 16.12 (b) and add up the results:\n\\[\n(2 \\times 2) + (4 \\times 5) + (7 \\times 11) + (2 \\times 15) + (4 \\times 12) =\n\\]\n\\[\n4 + 20 + 77 + 30 + 48 = 179\n\\]\nAnd so on for the other two entries in Table 16.12 (c). So the main rule of multiplying a row vector times a matrix with number of rows equal to the length of the row vector is that the result will always be another row vector of length equal to the number of columns of the matrix.\n\n\n16.7.2 Matrix Times Column Vector\nIn the same way, we can always multiply a matrix times a column vector, as long as the the number of columns of the matrix is equal to the length of the column vector. For instance, take the binary square matrix \\(A_{5 \\times 5}\\) shown in Table 16.13 (a) and the column vector \\(\\mathbf{b}_{5 \\times 1}\\) shown in Table 16.12 (b). Their product \\(\\mathbf{c}\\) would be given by:\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{b}_{5 \\times 1} = \\mathbf{c}_{5 \\times 1}\n\\tag{16.10}\\]\nEquation 16.10 says that the product of a matrix of dimensions \\(5 \\times 5\\) and a column vector of dimensions \\(5 \\times 1\\) is another column vector \\(\\mathbf{c}\\) of dimensions equal to the original column vector (\\(5 \\times 1\\)). A numerical example of this situation is shown in Table 16.13.\n\n\nTable 16.13: Column vector resulting from multiplying a square matrix times a column vector\n\n\n\n\n(a) 5 x 5 square matrix \n\n  \n    6 \n    2 \n    10 \n    7 \n    11 \n  \n  \n    15 \n    1 \n    8 \n    9 \n    11 \n  \n  \n    3 \n    19 \n    14 \n    6 \n    15 \n  \n  \n    15 \n    17 \n    12 \n    7 \n    11 \n  \n  \n    7 \n    6 \n    11 \n    8 \n    12 \n  \n\n\n\n\n\n\n(b) 5 x 1 column vector \n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\n\n\n\n(c) 5 X 1 product vector \n\n  \n    148 \n  \n  \n    152 \n  \n  \n    252 \n  \n  \n    240 \n  \n  \n    179"
  },
  {
    "objectID": "lesson-matrix-operations.html#multiplying-matrices-times-the-all-ones-vector",
    "href": "lesson-matrix-operations.html#multiplying-matrices-times-the-all-ones-vector",
    "title": "16  Matrix Operations",
    "section": "16.8 Multiplying Matrices Times the All Ones Vector",
    "text": "16.8 Multiplying Matrices Times the All Ones Vector\nIn matrix multiplication, there is a special row and column vector called the all ones vector. As you may have guessed this is a vector of all ones, of some length \\(n\\). For instance and all ones row vector of length five is \\(\\mathbf{1}_{1 \\times 5} = \\{1, 1, 1, 1, 1\\}\\) (the symbol for the all ones vector is a boldface “1”). We can also get the transpose of this all ones row vector to get the all ones column vector \\(\\mathbf{1}^T\\).\nWhy do we care about vectors full of ones? Well, it turns out that the all one row and column vectors have a neat property when we multiplied by matrices. We already know, from the rules of vector matrix multiplication reviewed earlier, that the product of a row vector times a square matrix is always a row vector of the same length as the original, and the product of a square matrix times a column vector is always a column vector of the same length as the original.\nLet’s say we a matrix \\(\\mathbf{A}\\) of dimensions \\(5 \\times 5\\), and we multiplied the all ones row vector of length five times this matrix, which would result in the row vector \\(\\mathbf{b}\\). This would be given by the formula:\n\\[\n\\mathbf{1}_{1 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = b_{1 \\times 5}\n\\tag{16.11}\\]\nA numerical example of the situation depicted in Equation 16.11 is shown in Table 16.14.\n\n\nTable 16.14: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 1 X 5 all ones row vector \n\n  \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 5 product row vector \n\n  \n    3 \n    5 \n    2 \n    3 \n    1 \n  \n\n\n\n\n\n\nIf you look at the resulting row vector in Table 16.14 (c), we can see that the result of multiplying the all ones row vector times a matrix is a vector that contains the column sums of the matrix entries! So the “2” in position 1 of Table 16.14 (c) comes from adding up the numbers in the first column of the matrix, the “1” in position 2 of Table 16.14 (c) comes from adding the numbers in the second column and so forth.\n\n\nTable 16.15: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) 5 X 1 all ones column vector \n\n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n\n\n\n\n\n\n(c) 5 X 1 product column vector \n\n  \n    2 \n  \n  \n    3 \n  \n  \n    4 \n  \n  \n    2 \n  \n  \n    3 \n  \n\n\n\n\n\n\nIn the same way, if we multiply the same matrix times the all ones column vector, we get the results shown in Table 16.15. We can see that the result of multiplying a matrix times the all ones column vector, is another column vector contains the row sums of the original matrix! So, the “2” in the first position of the column vector comes from adding the numbers in the first row of the matrix, the “3” comes from adding the numbers in the second row, and so forth."
  },
  {
    "objectID": "lesson-matrix-operations.html#the-identity-matrix",
    "href": "lesson-matrix-operations.html#the-identity-matrix",
    "title": "16  Matrix Operations",
    "section": "16.9 The Identity Matrix",
    "text": "16.9 The Identity Matrix\nThe last “interesting” matrix we will cover is called the identity matrix. This is a square matrix, usually written using the symbol \\(\\mathbf{I}\\) of dimensions \\(n \\times n\\). This matrix will have “1” in every diagonal cell, and “0” in every off-diagonal cell. For instance, an identity matrix of dimensions \\(5 \\times 5\\) is shown Table 16.16.\n\n\n\n\n\n\n  \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\nTable 16.16:  A 5 X 5 Identity Matrix. \n\n\nThe interesting thing about this matrix is that when you multiply it times another square matrix of the same dimensions, the result is always the original matrix! So it plays the role that the number “1” plays in regular number multiplication, in matrix algebra. This means that, for any square matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} \\times \\mathbf{I} = \\mathbf{A}\n\\tag{16.12}\\]\nAnd also,\n\\[\n\\mathbf{I} \\times \\mathbf{A} = \\mathbf{A}\n\\tag{16.13}\\] Neat!"
  },
  {
    "objectID": "lesson-matrix-operations.html#references",
    "href": "lesson-matrix-operations.html#references",
    "title": "16  Matrix Operations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lesson-applied-matrix-operations.html#matrix-powers-and-cohesive-groups",
    "href": "lesson-applied-matrix-operations.html#matrix-powers-and-cohesive-groups",
    "title": "17  Applied Matrix Operations",
    "section": "17.1 Matrix Powers and Cohesive Groups",
    "text": "17.1 Matrix Powers and Cohesive Groups\nIt turns out that the matrix powers operation discussed in Section 16.4.3 was one of the earliest applications of formal social network analysis in the social sciences, discovered about the same time by mathematicians and social psychologists Duncan (Luce and Perry 1949), Albert Perry and Leon (Festinger 1949). The basic idea is that when we obtain the powers of an adjacency matrix the resulting matrix has an intuitive interpretation in terms of indirect connections between people (see Chapter 7), which gives us a sense of how strongly related in a formal sense pairs of nodes in the graph are.\nLet’s see an example.\n\n\nTable 17.1: An adjancency matrix and its powers.\n\n\n\n\n(a) Original adjacency matrix. \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\n(b) Adjacency matrix squared. \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2 \n    2 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    B \n    2 \n    2 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    1 \n    1 \n    4 \n    2 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    D \n    1 \n    1 \n    2 \n    4 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    0 \n    4 \n    2 \n    2 \n    2 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    1 \n    0 \n    2 \n    3 \n    2 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    1 \n    0 \n    2 \n    2 \n    3 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    H \n    1 \n    1 \n    0 \n    1 \n    2 \n    2 \n    2 \n    4 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    3 \n    1 \n    0 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    3 \n    1 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    L \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    3 \n  \n\n\n\n\n\n\n\n\n(c) Adjacency matrix cubed. \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2 \n    2 \n    6 \n    6 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    B \n    2 \n    2 \n    6 \n    6 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    C \n    6 \n    6 \n    4 \n    7 \n    2 \n    2 \n    2 \n    7 \n    1 \n    2 \n    0 \n    2 \n  \n  \n    D \n    6 \n    6 \n    7 \n    4 \n    2 \n    1 \n    1 \n    2 \n    1 \n    1 \n    1 \n    6 \n  \n  \n    E \n    1 \n    1 \n    2 \n    2 \n    6 \n    8 \n    8 \n    9 \n    1 \n    6 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    2 \n    1 \n    8 \n    6 \n    7 \n    8 \n    1 \n    2 \n    0 \n    1 \n  \n  \n    G \n    1 \n    1 \n    2 \n    1 \n    8 \n    7 \n    6 \n    8 \n    1 \n    2 \n    0 \n    1 \n  \n  \n    H \n    1 \n    1 \n    7 \n    2 \n    9 \n    8 \n    8 \n    6 \n    1 \n    2 \n    0 \n    2 \n  \n  \n    I \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    2 \n    5 \n    3 \n    5 \n  \n  \n    J \n    1 \n    1 \n    2 \n    1 \n    6 \n    2 \n    2 \n    2 \n    5 \n    2 \n    1 \n    5 \n  \n  \n    K \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    3 \n    1 \n    0 \n    1 \n  \n  \n    L \n    1 \n    1 \n    2 \n    6 \n    1 \n    1 \n    1 \n    2 \n    5 \n    5 \n    1 \n    2"
  },
  {
    "objectID": "lesson-applied-matrix-operations.html#the-squared-adjacency-matrix",
    "href": "lesson-applied-matrix-operations.html#the-squared-adjacency-matrix",
    "title": "17  Applied Matrix Operations",
    "section": "17.2 The Squared Adjacency Matrix",
    "text": "17.2 The Squared Adjacency Matrix\nTable 17.1 (a) shows the adjacency matrix (\\(\\mathbf{A}\\)) corresponding to the “hangout” network in Figure 16.1 (a). Table 17.1 (b) shows the entries in \\(\\mathbf{A}^2\\) computed like we did earlier in the examples shown in Table 16.7. What is the meaning of the entries in each cell of Table 17.1 (b)?\nWell for the off-diagonal entries, the numbers in each cell tell us the number of indirect connections (specifically the number of walks; see Chapter 7) of length two between each pair of nodes.\nSo, for instance, we learn that node \\(A\\) can reach node \\(B\\) via two walks of length two, and can reach nodes \\(C\\), \\(D\\), \\(G\\), and \\(I\\) via one walk of length two. Remember from Chapter 7 that an indirect connection of a given length (in this case two) joins two nodes when it features them as the end nodes of the sequence of nodes and edges. Looking back at Figure 16.1 (a), we can see that the two walks of length two joining nodes \\(A\\) and \\(B\\) are \\(\\{AC, CB\\}\\) and \\(\\{AD, DB\\}\\), and that the walks of length two joining \\(A\\) to nodes \\(\\{C, D, G, I\\}\\) are:\n\\[\n\\{AD, DC\\}, \\{AC, CD\\}, \\{AC, CG\\}, \\{AK, KI\\}\n\\] The diagonal entries of Table 17.1 (b), on the other hand, give us the number of walks of length two that begin and end in the same node. Now what is the meaning of this? If you think of it, a walk of length two that starts in a node and goes to another node, and then comes back to the same node is just an edge in a symmetric graph! So the diagonals of \\(\\mathbf{A}^2\\) just count the number of edges incident to a node, which is the same as the degree of each node!"
  },
  {
    "objectID": "lesson-applied-matrix-operations.html#the-cubed-adjacency-matrix",
    "href": "lesson-applied-matrix-operations.html#the-cubed-adjacency-matrix",
    "title": "17  Applied Matrix Operations",
    "section": "17.3 The Cubed Adjacency Matrix",
    "text": "17.3 The Cubed Adjacency Matrix\nTable 17.1 (c) shows the corresponding entries for \\(\\mathbf{A}^3\\). What do these numbers mean? Well, you may have guessed. For the off-diagonal cells they are the number of walks of length three linking each pair of nodes. For instance, the “1” in the cell corresponding to nodes \\(H\\) and \\(F\\) tells us that there is walk of length three linking these two nodes. Looking at Figure 16.1 (a), we can see that this is given by: \\(\\{HG, GE, EF\\}\\).\nWhat do the numbers in the diagonal cells of Table 17.1 (c) mean? Well, as you may have guessed, they are actually the number of walks of length three that begin and end in that same node! As you may recall from Chapter 7, this is called a cycle of length three. So, the “2” in the diagonal cell entry corresponding to node \\(A\\) tells us that there are two cycles of length three featuring node \\(A\\) as its end nodes. Looking at Figure 16.1 (a), we can see that these are given by the sequences: \\(\\{AC, CD, DA\\}\\), \\(\\{AD, DC, DA\\}\\). In the same way, the number “4” in the diagonal cell for node \\(C\\) tells us that there are four cycles of length three that begin and end in that node. These are given by the sequences: \\(\\{CA, AD, DC\\}\\), \\(\\{CD, DA, AC\\}\\), \\(\\{CB, BD, DC\\}\\), and \\(\\{CD DB, BC\\}\\).\nNote that the edge sequence corresponding to cycles of length three is the same as that which corresponding to a clique of size three. So the diagonals in Table 17.1 (c), counts the number of cliques of size three that node belongs to. It actually counts twice the number of cliques of size three, because each clique is counted twice, once going in one direction (e.g., \\(\\{CA, AD, DC\\}\\)) and once going in the other direction \\(\\{CD, DA, AC\\}\\), so in Table 17.1 (c), the diagonal cell divided by two gives us the number of cliques of size three that node belongs to. When a node belongs to no clique, like node \\(K\\) in Figure 16.1 (a), then it gets a zero entry in the corresponding diagonal cell of \\(\\mathbf{A}^3\\).\n\n\n\n\n\nFigure 17.1: Graph with weighted edges representing number of indirect connections of length three between nodes\n\n\n\n\nFigure 17.1 shows the same graph as Figure 16.1 (a), but this time with the connections between nodes in the graph drawn as weighted edge with size and color intensity proportional to the entries in Table 17.1 (c) (the larger the number, the thicker and darker the edge), recording the number of walks of length three between each pair of nodes. As we can see, this reveals distinct hangout cliques like nodes \\(\\{A, B, C, D\\}\\) and nodes \\(\\{E, F, G, H\\}\\) that share multiple indirect connections with one another.\n\n17.3.1 To Infinity and Beyond!\nMore generally, for any adjacency matrix \\(\\mathbf{A}\\), the \\(n^{th}\\) power of the adjacency matrix gives us a symmetric matrix (\\(\\mathbf{A}^{n}\\)) whose off-diagonal entries \\(a^{n}_{ij}\\) record the number of walks of length \\(n\\) featuring the \\(i^{th}\\) node as the starting node and the \\(j^{th}\\) node as the end node, and whose diagonal entries \\(a^{n}_{ii}\\) record the the number of cycles of length \\(n\\) that begin and end with that node."
  },
  {
    "objectID": "lesson-applied-matrix-operations.html#matrix-multiplication-and-common-neighbors",
    "href": "lesson-applied-matrix-operations.html#matrix-multiplication-and-common-neighbors",
    "title": "17  Applied Matrix Operations",
    "section": "17.4 Matrix Multiplication and Common Neighbors",
    "text": "17.4 Matrix Multiplication and Common Neighbors\nAs we noted in Section 16.4.2, it is always possible to multiply any matrix times its transpose. Well, the adjacency matrix of a network (\\(\\mathbf{A}\\)) like Table 17.1 (a) is a matrix. That means it call always be multiplied times its transpose (\\(\\mathbf{A}^T\\)), resulting in some other matrix \\(\\mathbf{B}\\), of the same dimensions as the original adjacency matrix:\n\\[\n\\mathbf{A} \\times \\mathbf{A}^T = \\mathbf{B}\n\\] The entries corresponding to \\(\\mathbf{B}\\) computed according to the matrix multiplication rules laid out in Section 16.5, are shown in Table 17.2.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2 \n    2 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    B \n    2 \n    2 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    1 \n    1 \n    4 \n    2 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    D \n    1 \n    1 \n    2 \n    4 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    0 \n    4 \n    2 \n    2 \n    2 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    1 \n    0 \n    2 \n    3 \n    2 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    1 \n    0 \n    2 \n    2 \n    3 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    H \n    1 \n    1 \n    0 \n    1 \n    2 \n    2 \n    2 \n    4 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    3 \n    1 \n    0 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    3 \n    1 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    L \n    1 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    3 \n  \n\n\n\nTable 17.2:  Adjacency matrix multiplied by its transpose. \n\n\nWhat are the entries in Table 17.2? Well, first note one thing, the diagonal entries of Table 17.2 are the same as the diagonal entries of Table 17.1 (b). That means that it is counting the degree of each node.\nWhat are the off-diagonal entries of \\(\\mathbf{B}\\) though? Let’s see where they come from using the rules of matrix multiplication (see Section 16.5). Let’s take the entry corresponding to nodes \\(A\\) and \\(C\\) (the cell corresponding to the first row and third column) in Table 17.2. We see there is a “1” there. We know it must have come from matching the numbers in the first row of Table 17.1 (a) with the numbers in the third column of the same table. These are:\n\n\nTable 17.3: Entries from an adjacency matrix\n\n\n\n\n(a) First row entries (node A) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\n(b) Third column entries (node C) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) Product of first row entries and third column entries \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\nTo get the entry in cell \\(b_{13}\\) of matrix \\(\\mathbf{B}\\) all we need to do is multiply each of the zeros and ones in Table 17.3 (a) and Table 17.3 (b) and add up the result. When we do that we get the numbers in Table 17.3 (c). We see that the only lonely “1” in Table 17.3 (c) corresponds to node \\(D\\), note that this happens to be the only common neighbor shared by nodes \\(A\\) and \\(C\\).\nSo we cracked the mystery of the off-diagonal entries of Table 17.2! When we multiply an adjacency matrix times its transpose, we end up with a matrix whose off-diagonal cells count the number of common neighbors shared by the row node and the column node, and whose diagonal entries count the total number of neighbors of that node."
  },
  {
    "objectID": "lesson-applied-matrix-operations.html#references",
    "href": "lesson-applied-matrix-operations.html#references",
    "title": "17  Applied Matrix Operations",
    "section": "References",
    "text": "References\n\n\n\n\nFestinger, Leon. 1949. “The Analysis of Sociograms Using Matrix Algebra.” Human Relations 2 (2): 153–58.\n\n\nLuce, RD, and Albert D Perry. 1949. “A Method of Matrix Analysis of Group Structure.” Psychometrika 14 (2): 95–116."
  },
  {
    "objectID": "lesson-egonet-metrics.html#sec-egographs",
    "href": "lesson-egonet-metrics.html#sec-egographs",
    "title": "18  Ego Network Metrics",
    "section": "18.1 Ego Graphs",
    "text": "18.1 Ego Graphs\nConsider Figure Figure 8.1 again. If we solved the boundary specifiction problem, then this could be considered an example of a whole network. All the relevant actors and social ties are included. However, sometimes, we do not have that information. Instead, what we have is network data collected from the point of a single person. For instance, let us say, that instead of collecting information on all the actors depicted in Figure Figure 8.1), we only interviewed actor D and ask them to name the people they spend time with and tell us about the relations between those people.\nIn that case, we would have end up with a graph that looks like Figure 18.1. This is called a simple ego graph (ego graphs are also referred to as centered graphs (Freeman 1982)). The reason we call it “simple” is because the ego graph, just like the overall graph of which it is a subgraph, contains only undirected, binary (present or absent) edges, and no loops (self-edges), which are the conditions that define simple graphs.\nNote that in the ego graph shown as Figure Figure 18.1, all nodes are a subset of the original graph, and all edges are also a subset of the original graph shown in Figure Figure 8.1). So if \\(G_D\\) if node D’s ego graph, and \\(G\\) is the original undirected graph, then \\(G_D = (E_D, V_D)\\), \\(E_D \\subset E\\) and \\(V_D \\subset V\\).1\n\n\n\n\n\nFigure 18.1: Ego graph for point D."
  },
  {
    "objectID": "lesson-egonet-metrics.html#ego-networks",
    "href": "lesson-egonet-metrics.html#ego-networks",
    "title": "18  Ego Network Metrics",
    "section": "18.2 Ego Networks",
    "text": "18.2 Ego Networks\nEgo graphs are useful for representing egocentric networks (often shortened to just ego networks or even ego nets), also called personal networks. These are a particular type of network meant to represent a set of social relations from the perspective of a focal person. In the point and line graph representation of an ego-network, the person of interest, or ego, which is the Latin word for “self”, is usually represented at the center of the graph surrounded by their contacts; these are referred by the Latin word for “others,” namely, alters. Ego networks will usually contain links indicating the relationship between alters, as seen from the perspective of ego.\nIn the typical ego network diagram, D is the ego node (shown in tan in Figure Figure 18.1) and is depicted as the center of the ego graph. The alter nodes are A, B, C, E and F (shown in red in Figure Figure 18.1) are shown as “satellites” orbiting the ego node.\nFigure Figure 18.1 also shows that in an ego network there are two kinds edges we may be interested in and that we need to keep separate. First there are ego-to-alter edges (shown in blue in Figure Figure 18.1)), indicating the alters ego is connected to. In an ego network, ego-to-alter links are obligatory, meaning that they all should be present. Thus, if, like in Figure Figure 18.1 there are five alters depicted, there should also be five ego-to-alter links. As you can see, that is indeed the case: \\(\\{DA, DB, DC, DE, DF\\}\\).\nSecond, there are alter-to-alter links (shown in purple in Figure 18.1) indicating relationships between alters that do not involve ego, but that only include other nodes that are connected to ego via ego-to-alter links. These are optional, in the sense that they could or could not exist. While, in an ego network, ego is necessarily connected to their alters, alters may or may not be connected to one another. Thus, in Figure Figure Figure 18.1, there are five alter-to-alter links, namely, \\(\\{AC, AB, AE, BC, CE\\}\\), but these are not all the possible ones that could exist. For instance, alters E and F are a null (disconnected) dyad in the ego graph, and so are alters E and F. Later in the lesson, we will show you how to compute the maximum number of expected connections in an ego network so as to compare them to the ones we actually observe (hint: it has something to do with the formula for graph density).\nFor example, if you were to ask someone to name their friends (or any other type of alter), they will tell you who their friends are. They could even tell you whether two or their friends knew one another or not. However, they typically will not be able to you about the entirety of each of their alter’s networks (e.g., every person known by their alters and the connections between these others). Thus, while in Figure Figure 8.1) we have a graph of the whole network, in the ego-network case, such as Figure Figure 18.1, we are only getting a small part of the overall social network, centered on a single person.2"
  },
  {
    "objectID": "lesson-egonet-metrics.html#what-ego-networks-tell-us",
    "href": "lesson-egonet-metrics.html#what-ego-networks-tell-us",
    "title": "18  Ego Network Metrics",
    "section": "18.3 What Ego Networks Tell Us",
    "text": "18.3 What Ego Networks Tell Us\nWhile it might seem like we are unable to do much with ego-networks, that is definitely not the case! While having data on only one person’s ego would not tell scholars much, when we have tens, hundreds, or thousands of ego-networks we are able to analyze systematic differences in the ways different types of people structure their social worlds (Marsden 1987), or, as is more likely from a sociological perspective, have it structured for them by social forces beyond their direct control.\nFor example, if all the students in the class fill out ego-networks of their friends and family, could we find that certain types of people have more friends? Do people with more family members also have more friends, or do they have fewer friends? Are the friends of some people more likely to be friends with one another? Do some people have all women or all people their same race or ethnicity in their ego-networks?\nThese are empirical questions which, when scholars creatively compare egocentric networks, we are able to potentially answer. Research does not have to only use one type of relationship when collecting network data, and this would be one such case when combining egocentric networks of both friendship and family ties into a single personal network of emotional support or some other socially meaningful exchange.\nOnce we have collected ego network data, using the standard battery of name generators and name interpreters, we may be interested in computing certain metrics that allow us to characterize each personal (ego) network in terms of certain ego network properties of interest. There are four pieces of information we need to compute all the relevant ego network properties. These are:\n\nThe number of ego-to-alter ties\nThe number of alters\nThe number of alter-to-alter ties\nSociodemographic characteristics of ego (e.g., age, gender identity, racial identity, etc.)\nSociodemographic characteristics of alter (e.g., age, gender identity, racial identity, etc.).\n\nGiven this information, there are four main types of ego network properties that we could be interested in. These are:\n\nSize: Total number of ego-alter ties\nDiversity: Variation in alter attributes\nHomogeneity: Ties to alter same/different from ego\nComposition: Proportion of certain types of alter ties\nClustering: Density of alter-to-alter network\n\nTable tbl-ego-net-props shows each ego-network property with a brief definition, showing the type of information we need to compute each.\n\n\nTable 18.1: Ego Network Properties\n\n\n\n\n\n\n\n\n\n\nEgo-alter ties\nAlter-Alter Ties\nEgo Attributes\nAlter Attributes\n\n\n\n\nSize\nX\n\n\n\n\n\nHomogeneity\nX\n\nX\nX\n\n\nDiversity\n\n\n\nX\n\n\nComposition\n\n\n\nX\n\n\nClustering\n\nX\n\n\n\n\n\n\n\n18.3.1 Ego Graph Notation\nIn what follows we will abide by the following notational conventions:\n\nWe will refer to the ego node as \\(Ego\\).\nWe will refer to the set of ego-to-alter edges as \\(E_{ea}\\).\nWe will refer to the set of alter-to-alter edges as \\(E_{aa}\\).\nWe will refer to the set of alter nodes as \\(N(Ego)\\) which can be read as “ego’s neighborhood.”"
  },
  {
    "objectID": "lesson-egonet-metrics.html#ego-network-size",
    "href": "lesson-egonet-metrics.html#ego-network-size",
    "title": "18  Ego Network Metrics",
    "section": "18.4 Ego Network Size",
    "text": "18.4 Ego Network Size\nThe size (\\(S(ego)\\)) of the ego network is the simplest metrics we can compute. It is given by a count of the ego-to-alter ties (\\(E_{ij}\\)). Note that in this case, ego-network-size is equivalent to the number of neighbors (\\(N(Ego)\\)) ego has in the ego graph, which is also the degree of the ego node (\\(k^{Ego}\\)) in the larger graph. Thus, for ego networks, size and degree are the same metric, and is given either by the cardinality of ego’s neighborhood:\n\\[\n  S(Ego) = |N(ego)|\n\\tag{18.1}\\]\nOr the cardinality of the ego-to-alter edge set:\n\\[\n  S(Ego) = |E_{ea}|\n\\tag{18.2}\\]\nThus, in the example shown in Figure Figure 18.1:\n\\[\n  S(Ego) = |N(Ego)| = |\\{A, B, C, E, F\\}| = 5\n\\]\nOr equivalently:\n\\[\n  S(Ego) = |E_{ea}| = |\\{DA, DB, DC, DE, DF\\}| = 5\n\\]"
  },
  {
    "objectID": "lesson-egonet-metrics.html#ego-network-homophily",
    "href": "lesson-egonet-metrics.html#ego-network-homophily",
    "title": "18  Ego Network Metrics",
    "section": "18.5 Ego Network Homophily",
    "text": "18.5 Ego Network Homophily\nAnother property of ego networks we may be interested in measuring is homogeneity. That is, we may wan to ask: Do people connect to others that similar to themselves or do they connect to to others that are different from them? People can be similar or different from others in an infinity of ways. Sociologists are primarily interested in similarity or difference along lines of social position or social categories. For instance, in human societies gender is an important marker of social position, as is racial identification, class identification, age, occupation, or educational attainment.\nHomophily is the idea that people with similar personal or social traits will tend to have relationships with each other compared to having relationships with those unlike themselves. Etymologically, the word is a simple combination of homo, meaning same, and philia, meaning love or liking. Thus, homophily is literally a like or love of the same. Many languages have some phrase capturing this propensity, and in English it is often idiomatically expressed as “birds of a feather flock together” (McPherson, Smith-Lovin, and Cook 2001)\nThe theory of homophily says that all else equal, people will tend to associate with others that are similar to themselves. As we have seen before, there are various reasons for why this might be, both psychological (people prefer similarity) to sociological (social contexts induce people to link to similar others).\n\n18.5.1 Fun Facts:\nMany languages have idioms for homophily as a social phenomenom. Just a few include\n\nJapanese- Racoon dogs from the same den (Onazi ana no mujina)\nFrench- Those who ressemble each other assemble together (Qui se ressemble s’assemble)\nItalian- God makes them then couples them (Dio li fa e poi li accoppia)\n\nHave an idiom from another language for homophily? Let us know so we can include it!"
  },
  {
    "objectID": "lesson-egonet-metrics.html#ei-homophily-index",
    "href": "lesson-egonet-metrics.html#ei-homophily-index",
    "title": "18  Ego Network Metrics",
    "section": "18.6 EI Homophily Index",
    "text": "18.6 EI Homophily Index\nThe EI homophily index is one of the most useful ways to think about homophily in an ego-network. The EI homophily index is a relative measure of homophily because it does not consider the underlying population as would be required in an expected rate. Although it might be nice to calculate an expected rate, researchers can often do interesting things even without this data.\nThe EI homophily index is a measure of in- and out-group preference. One simply subtracts the number of out-group ties from the number of in-group ties, divided by the total number of ties. This measure thus uses information on ego-to-alter ties, and both ego and alter characteristics.\n\\[\n  EI=\\frac{External-Internal}{External+Internal}\n\\tag{18.3}\\]\nThus, an EI score of -1 means complete homophily- the individual only has relationships with actors of the same “type” as they themselves are. An EI score of 1 means complete heterophily- all the alters are of a different “type” than they themselves are. Finally, an EI score of 0 means that an equal number of alters are of both the same “type” as the ego, and different types.\n\n\n\n\n\nFigure 18.2: An ego graph with node color indicating gender identification.\n\n\n\n\nTo calculate the EI score for the above ego, we must first ask along what dimension of homophily we are interested in. This must be done to reflect our research question. Let us presume however that we are interested in the relationship between gender identification and friendship networks. We might ask if the friendship networks of high-schoolers are changing over time. If we had older data on friendships, say from 40 years ago, we could compare it with friendship networks from high school students today.\nAn example of an ego network in which the nodes are classified by gender identification is shown in Figure Figure 18.2. In the figure, nodes that identify as men are shown in green, those that identify as women are orange, and those that identify as nonbinary are blue. Notice that the ego in this case identifies as a man. also notice that ego has four alters who also identify as men, two that identify as women, and one non-binary alter Thus, ego has four ego-to-alter ties that connect him to alters who are in the same category as himself on the social attribute of interest. These are internal ties because they connect ego to people who are similar to himself (shown in teal). Ego also has three ego-to-alter ties who are not the same as ego in the attribute of interest, regardless of them belonging to different classifications among themselves. These are external ties, because they connect ego to people who are different from himself (shown in red).\nGiven this information, the EI index for the ego in Figure Figure 18.2 can be computed as:\n\\[\n  EI=\\frac{External-Internal}{External+Internal}=\\frac{3-4}{3+4}=\\frac{-1}{7}=-0.14\n\\]\nThis gives us an EI score just under zero, which indicates a slight preference for associating with similar others in terms of gender identity. The EI score ranges from -1.0 to +1.0. EI scores below zero (and approaching -1) indicate a tendency towards homophily (associating with similar others). EI scores above zero (and approching 1.0) indicate a tendecy towards associating with dissimilar others.3 Finally, an EI score around zero suggests that the ego-network is balanced in terms of an equal number of similar and dissimilar alters."
  },
  {
    "objectID": "lesson-egonet-metrics.html#ego-network-diversity",
    "href": "lesson-egonet-metrics.html#ego-network-diversity",
    "title": "18  Ego Network Metrics",
    "section": "18.7 Ego Network Diversity",
    "text": "18.7 Ego Network Diversity\nWe may be interested in how diverse ego’s contacts are. Do they all belong to the same social group or is ego connected to a wide range of contacts? Note that the only piece of information we need here have to do with characteristics of ego’s contacts. We do not need to know anything about ego to know how diverse their networks are. So the connection between diversity and ego’s characteristics is empirical rather than definitional.4\nThe most popular measure of diversity for ego networks is Blau’s heterogeneity index (H). Quite simply, H is the sum of the square of the percentage (p) of the ego’s network belonging to particular groups, subtracted from one:\n\\[\n  H = 1-\\sum_{k}p_k^{2}\n\\tag{18.4}\\]\nUnlike in the EI homophily index, ego’s own group membership does not matter. A woman with only men friends would have an H-index of 0, just as a woman with only female friends would also have an H index of 0. An H-index of zero simply means that there is no diversity in the types of friends the ego has. Conversely, an H index score that approaches towards 1 implies greater heterogeneity, or diversity, in an ego’s network relations. For instance, someone with equal number of men and women friends, or a woman who one-third women friends, one-third men friends, and one-third of friends who are nonbinary would have an H-index of 1.0 with respect to gender diversity.\nRemember that we can to compute ego network diversity, we need to pick which characteristic we are measuring gender diversity on. For instance, and ego network can be non-diverse with respect to race (each of ego’s alters is of the same race), but be very diverse with respect to education (ego has contacts from a wide range of educational backgrounds). So there is no such thing as a “diverse” ego network in general (or a non-diverse one). You always have to specify what social characteristic you are talking about. So there is ego-network diversity with respect to gender identification, which is different from that with respect to racial identity of alters, and so on.\nFor instance, we may want to know how diverse and ego network is with respect to the gender identification of alters like we considered in the previous example. Thus, to the H diversity index of the ego network shown in Figure Figure 18.2, first it is a matter of identifying the different groups the alters in ego’s network belong to, where the groups are defined their gender identification. As we saw, this spans three groups, men, women, and nonbinary. So to figure out ego’s diversity with respect to gender using Equation 18.4, we would have to compute:\n\\[\n  H = 1- (p^2_{men} + p^2_{women} + p^2_{nonbinary})\n\\]\nAs shown above, the ego’s network has ties to three different groups that we are interested in as this is how many gender identities are in the ego’s network. The next step is to find what percentage of each group makes up the ego’s network. Once this is done, a little bit of math solves for the H index score. So we know that ego has four alters who identify as men. This means that \\(p_{male} = \\frac{4}{7}\\). Four out of the seven contacts in ego’s network are men. Writing this quantity for all three gender identification groups in ego’s network yields:\n\\[\n  \\begin{split}\n    H &= 1- \\left(\\left[\\frac{4}{7}\\right]^2_{men} + \\left[\\frac{2}{7}\\right]^2_{women} + \\left[\\frac{1}{7}\\right]^2_{nonbinary}\\right) \\\\\n    H &= 1- (0.57^2+ 0.28^2 + 0.14^2)  \\\\\n    H &= 1- (0.33+ 0.08 + 0.02) \\\\\n    H &= 1 - 0.43 \\\\\n    H &= 0.57\n  \\end{split}\n\\]\nThe resulting H index is 0.57. We might ask: What does this mean? Well, we can compare this score to some extreme hypothetical cases. For instance, if Ego’s contacts where all of the same gender identification then there would be no diversity in the ego-network. In this case, the H-index would take its minimum value of zero, meaning that all of a person’s friends come from the same group. The fact that the H-index is substantially above zero tells us that there’s quite a lot of gender diversity in this ego network.\nThe H-index gets larger (approaching one) the more diverse a person’s ego network gets. The H index reaches its maximum when people from different groups are equally represented in Ego’s network. For instance, a maximally diverse ego-network with respect to gender identification would contain equal number of men, women, and nonbinary identifying people. So, if Ego has six friends, they would have maximum gender diversity of they have two women friends, two men friends and two nonbinary identifying friends.\nTechnically, the maximum H is given by (Solanas et al. 2012):\n\\[\n  H_{max} = 1 - \\frac{1}{k}\n\\tag{18.5}\\]\nWhere \\(k\\) is the number of categories of the sociodemographic dimension being considered. In the gender identification example \\(k=3\\), but if we were studying ego-network diversity along different demographics (e.g., race or education), \\(k\\) would be a larger number.\nSo to return to our example is \\(H= 0.57\\) a large number? Well, let’s compare it to the maximum value it could take. With \\(k=3\\) gender categories, the maximum H is given by:\n\\[\n  H_{max} = 1 - \\frac{1}{3} =  1 - 0.33 = 0.67\n\\]\nWe can divide the observed \\(H\\) by this hypothetical maximum to see how close Ego is to the ideal of a maximally diverse network with respect to gender. This gives us \\(\\frac{H}{H_{max}} = \\frac{0.57}{0.67} = 0.85\\). So this means that Ego’s network is pretty diverse (at 85% percent maximum diversity), but has a bit more room to grow to reach maximum diversity!"
  },
  {
    "objectID": "lesson-egonet-metrics.html#clustering-coefficient",
    "href": "lesson-egonet-metrics.html#clustering-coefficient",
    "title": "18  Ego Network Metrics",
    "section": "18.8 Clustering Coefficient",
    "text": "18.8 Clustering Coefficient\nBy definition, everyone knows the ego, but to what extent does someone’s friends know each other? For ego-networks, the tendency of ego’s friends to be friends with one another is called clustering and this ego network property is measured via the clustering coefficient (\\(CC\\)). Consider Figure Figure 18.1, again. In that ego graph, we can see that some of ego’s alters do know each other but some of them do not.\nThe \\(CC\\) is calculated by computing the density of the subgraph among alters that remains when ego and the edges that are incident on ego (e.g., the ego-to-alter edges) are removed. This can range from zero (none of ego’s alters are connected to one another) to one (all of ego’s alters are friends with each other) or some fraction in between (for instance, a CC of 0.5 means that 50% of the total possible number of relationships between ego’s alters are present).\nSince we are already know how to compute the density of undirected graphs (see the lesson on graph metrics), then we know that the clustering coefficient for an ego network can be obtained using this formula:\n\\[\n  CC_i = \\frac{2m}{n(n-1)}\n\\tag{18.6}\\]\nWhere \\(n\\) is the size of the ego network size (number of alters) and \\(m\\) is the number of alter-to-alter edges among alters surrounding ego. Thus, as noted, the clustering coefficient calculation ignores ego and the ego-to-alter edges.\nWe can compute the clustering coefficient for the ego network depicted in Figure Figure 18.1, because we know that \\(n\\) = 5 and \\(m\\) = 4. We then plug in these values into Equation 18.6, which gives us:\n\\[\n  CC_i = \\frac{2m}{n(n-1)}=\\frac{2 \\times 4}{5 \\times (5-1)}=\\frac{8}{5 \\times 4} = \\frac{8}{20}=\\frac{2}{5}=0.40\n\\]\nBecause the resulting clustering coefficient of \\(0.40\\), we can conclude that \\(40\\%\\) of all possible ties among ego’s alters exist. Another way of putting it is that, if we were to pick two of ego’s alters at random the probability that the two would be part of a connected dyad is \\(p = 0.40\\). With multiple egos, we might thus be able to compare their personal networks to build theories about how the social world operates."
  },
  {
    "objectID": "lesson-egonet-metrics.html#references",
    "href": "lesson-egonet-metrics.html#references",
    "title": "18  Ego Network Metrics",
    "section": "References",
    "text": "References\n\n\n\n\nFreeman, Linton C. 1982. “Centered Graphs and the Structure of Ego Networks.” Mathematical Social Sciences 3 (3): 291–304.\n\n\nMarsden, Peter V. 1987. “Core Discussion Networks of Americans.” American Sociological Review 52 (1): 122–31.\n\n\nMcPherson, Miller, Lynn Smith-Lovin, and James M Cook. 2001. “Birds of a Feather: Homophily in Social Networks.” Annual Review of Sociology 27 (1): 415–44.\n\n\nSolanas, Antonio, Rejina M Selvam, José Navarro, and David Leiva. 2012. “Some Common Indices of Group Diversity: Upper Boundaries.” Psychological Reports 111 (3): 777–96."
  },
  {
    "objectID": "lesson-affiliation-networks.html#bipartite-graphs",
    "href": "lesson-affiliation-networks.html#bipartite-graphs",
    "title": "19  Affiliation Networks",
    "section": "19.1 Bipartite Graphs",
    "text": "19.1 Bipartite Graphs\nA bipartite graph is useful to represent a network where, rather than ties occurring between nodes of the same kind (e.g., people connected with other people), ties occur only between nodes of different kinds but never between nodes of the same kind. Typically, the two different types of nodes are located at different levels of analysis or aggregation. As such, bipartite graphs are perfect for capturing the sociological concept of affiliation or membership with larger groups or events (Breiger 1974). For instance, actors and the movies they make, scientists and the papers they write, or people and the groups they belong to.1\n\n\n\n\n\nFigure 19.1: A bipartite graph. Circles are people and triangles are the corporate boards they belong to\n\n\n\n\nFor example, people work at companies, so we might say that a worker is connected with the company, rather than any specific individual there. People also connect to sports teams, schools, religious communities, and other organizations which can have an influence in structuring their social world.\nIn the graph theoretic sense, a bipartite graph \\(G_B\\) is a graph featuring two sets of nodes \\(V_1\\) and \\(V_2\\) and one set of edges \\(E\\). Thus a bipartite graph, like a signed and a weighted graph, is a set of three sets:\n\\[\n    G_B = (E, V_1, V_2)\n\\tag{19.1}\\]\n\nrepresents a network diagram of a bipartite graph where circles connect to triangles (with the shapes standing as labels for the two set of nodes). In the Figure, \\(V_1 = \\{A, B, C, D, E\\}\\) and \\(V_2 = \\{1, 2, 3, 4, 5\\}\\). The edge set \\(E\\) is \\(\\{A1, A2, B2, B3, C2, C4, D4, D3, E3, E5\\}\\).\n\nOne common example of two-mode networks that be represented using bipartite graphs in sociology are corporate interlock networks (Mizruchi 1983). If 1) represented such a network, we could think of the circles as members of the company’s board, and the triangles are the board from each company. Because the same executive can be a member of more than one company’s board, board member A is on the board of both companies 1 and 2, while board member B is on the board of companies 2 and 3.\nNote that edges in a bipartite graph are symmetrical and thus bipartite graphs are (generally) undirected. This makes sense, since the relationship affiliation or membership is indeed symmetrical by definition. If person A is a member of the board in company 2 then it is understood that company 2 has person A as a board member.\nIn the same way, note that there is no reason why the cardinality of two node sets in a bipartite graph have to be same (although they are in the example provided). In a real world corporate interlock network, for instance, there will generally be more people than companies, so \\(|V_1| > |V_2|\\)."
  },
  {
    "objectID": "lesson-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "href": "lesson-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "title": "19  Affiliation Networks",
    "section": "19.2 Unipartite Projections of Bipartite Graphs",
    "text": "19.2 Unipartite Projections of Bipartite Graphs\nWhile the information we can glean from looking at the original bipartite graph alone may be useful, you might realize that board members A and B both are on the boards of company 2! In fact, board member C is also on the board of company 2! We might thus conclude that board members A, B, and C all know each other from sitting in the same company board.\n\n\n\n\n\nFigure 19.2: A unipartite graph. People are linked if they serve in the same company board\n\n\n\n\nIf this sort of information was important, we could convert the bipartite into a simple unipartite graph capturing connections between the same level of analysis. This is called a projection of the original bipartite graph. In the projected graph, two board members are joined by a symmetric tie if they both serve on the board of at least one company together.\n\n\n\n\n\nFigure 19.3: Another unipartite graph. Boards are linked if they share members.\n\n\n\n\nThus, we could, as shown in Figure 19.2, create a graph that shows board members who know each other because they work at the same company. The resulting (simple, undirected) graph shows that board members A, B, and C all know each other as a result of serving in the board of company 2 together.\nLikewise, we can transform the bipartite graph into a simple unipartite graph that captures companies that share board members. Company 2 is thus connected to Companies 1 (because of person A), 3 (because of person B), and 4 (because of person 5). This is shown in Figure 19.3). In fact, the reason why these are called interlock networks, is because it is easy to see that, ultimately, by virtue of sharing members across boards, most big corporations in the U.S. (and other countries), end up forming part of a single giant network."
  },
  {
    "objectID": "lesson-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "href": "lesson-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "title": "19  Affiliation Networks",
    "section": "19.3 From Biparite Graph to Affiliation Matrix",
    "text": "19.3 From Biparite Graph to Affiliation Matrix\nConsider the two-mode network shown in Figure 19.4. This is an affiliation network meant to represent the memberships of six students in five college activity clubs. As discussed earlier, we use a bipartite graph to represent the network. The bipartite graph represents the two sets of nodes using different shapes or colors (blue and red nodes in Figure 19.4), and draws a link between the people and the group if the person is affiliated with the group.\n\n\n\n\n\nFigure 19.4: Bipartite graph of a two-mode network of students and clubs.\n\n\n\n\nHow can we translate the graph representation into a matrix?\nThe procedure is the same as that used to build the adjacency matrix of the symmetric graph. We build a rectangular matrix whose number of rows is the same as the number of people in the affiliation network, and whose number of rows is the same as the number of groups. The matrix is rectangular (as opposed to square) because in a two-mode network, there is no restriction that the size of the two vertex sets be the same (although if they happen to be the same then you end up with a square matrix; after all, a square is a special case of a rectangle!).\nIn graph theory terms, this is a matrix that we call A, for affiliation matrix of dimensions \\(R \\times C\\), where the number of rows \\(R = |V_1|\\) is the cardinality of the first vertex set in the bipartite graph (persons in Figure 19.4), and where the number of rows \\(C = |V_2|\\) is the cardinality of the second vertex set (clubs in Figure 19.4)). The cells of the affiliation matrix, \\(a_{ij} = 1\\) if person i belongs to club j (there’s an symmetric edge in the graph linking the person to the group), otherwise, \\(a_{ij} = 0\\).\nFollowing these instructions would yield the affiliation matrix shown in Table 19.1.\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nGabriela\n1\n1\n1\n1\n0\n\n\nParker\n1\n0\n1\n0\n0\n\n\nBrandon\n0\n1\n1\n1\n0\n\n\nMarie\n0\n0\n1\n0\n1\n\n\nRahul\n0\n1\n0\n1\n0\n\n\nMinjoo\n0\n0\n0\n0\n1\n\n\n\nTable 19.1: Affiliation matrix of a bipartite graph.\n\n\nThe affiliation matrix has some interesting properties. For instance, just like the adjacency matrix, it can be used to compute node degree centrality for each set of nodes. But since we have two different sets of nodes, we end up with two different sets of centrality scores; one set of centrality scores for the people and another set for the groups (Faust 1997).\nLet us see how this works."
  },
  {
    "objectID": "lesson-affiliation-networks.html#group-and-person-centralities",
    "href": "lesson-affiliation-networks.html#group-and-person-centralities",
    "title": "19  Affiliation Networks",
    "section": "19.4 Group and Person Centralities",
    "text": "19.4 Group and Person Centralities\n\n19.4.1 Person Centralities\nIf we wanted to figure out the degree centrality of the people node set (abbreviated P) in the affiliation matrix, we would sum cell entries across the rows, according to the now familiar equation:\n\\[\n    C_P^{DEG} = \\sum_j a_{ij}\n\\tag{19.2}\\]\nWhich leads to the following vector of degree centrality scores for the people:\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\n4\n2\n3\n2\n2\n1\n\n\n\nTable 19.2: Degree centrality scores for the people.\n\n\nThe degree centrality scores for the people can be interpreted as giving us a sense of their joining activity (e.g., high versus low). Some people, (like Gabriela) join a lot of clubs; they have multiple interests spread out across many organizations. Other people, (like Minjoo), just have a single interest, and thus join only one club (the Cheese Club). If centrality is defined using the “more/more principle” discussed in lesson on centrality, then we would say that Gabriela is more central than Minjoo in the affiliation network.\n\n\n19.4.2 Group Centralities\nIn the same way, if wanted to compute the degree centralities of other mode (the club node set, abbreviated as G), then we would calculate the column sums of the affiliation matrix using a slight variation of Equation 19.2), like we did when we switched from outdegree to indegree:\n\\[\n    C_G^{DEG} = \\sum_i a_{ij}\n\\tag{19.3}\\]\nWhich leads to the following degree centrality scores for the clubs:\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\n2\n3\n4\n3\n2\n\n\n\nTable 19.3: Degree centrality scores for the clubs.\n\n\nJust like the people, the centrality scores for the clubs tell us something about the popularity of each group. Some groups are popular (have lots of members), others are less so. So, it seems like in this student group, the Magic Club is definitely the most popular, containing four members. The Cheese and Fashion Clubs on the other hand, seem to be more niche pursuits, with only two members each."
  },
  {
    "objectID": "lesson-affiliation-networks.html#the-affiliation-matrix-transpose",
    "href": "lesson-affiliation-networks.html#the-affiliation-matrix-transpose",
    "title": "19  Affiliation Networks",
    "section": "19.5 The Affiliation Matrix Transpose",
    "text": "19.5 The Affiliation Matrix Transpose\nAs discussed in Section 16.4.2, it is possible to “flip” the rows and columns of any matrix, so what was previously the rows become the columns, and what was previously the columns become the rows. This is called the matrix transpose and if the original matrix was called A, then the transpose is called A’.2 If the original matrix A was of dimensions \\(R \\times C\\) then the transpose A’ is of dimensions \\(C \\times R\\).\nThe transpose of the affiliation matrix shown in Table 19.1 is shown in Table 19.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nFashion\n1\n1\n0\n0\n0\n0\n\n\nNerdfighters\n1\n0\n1\n0\n1\n0\n\n\nMagic\n1\n1\n1\n1\n0\n0\n\n\nSuper Smash Brs.\n1\n0\n1\n0\n1\n0\n\n\nCheese\n0\n0\n0\n1\n0\n1\n\n\n\nTable 19.4: Transpose of the affiliation Matrix.\n\n\nNote that the transpose of the affiliation matrix contains exactly the same information as the original affiliation matrix. The group affiliations of every person are preserved as are memberships of each group. If we used equations Equation 19.2, and Equation 19.3) to compute the person and group centralities using the affiliation matrix transpose A’, we would get the same results, except that the first equation (summing across the rows) would now give us the group centralities, and the second equation (summing down the columns) would give use the people centralities!\nWe learn from matrix algebra that an important property of rectangular matrices is that you can always multiply a rectangular matrix by its transpose (see Section 16.4.1). Recall a key condition of matrix multiplication is that the two matrices be conformable so that the columns of the first matrix need to match the number of rows of the second matrix. Well, it’s clear than since any matrix that is of dimensions \\(R \\times C\\), will have a transpose of dimensions \\(C \\times A\\) then the multiplication of the two matrices will be defined:\n\\[\n    A^{}_{R \\times C} \\times A^{'}_{C \\times R} = defined!\n\\tag{19.4}\\]\nIn the same way, the transpose of a matrix can always be multipled by the original matrix:\n\\[\n    A^{'}_{C \\times R} \\times A^{}_{R \\times C} = defined!\n\\tag{19.5}\\]"
  },
  {
    "objectID": "lesson-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "href": "lesson-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "title": "19  Affiliation Networks",
    "section": "19.6 The Person and Group Overlap Matrices",
    "text": "19.6 The Person and Group Overlap Matrices\nIf the transpose of the affiliation matrix contains the same information as the original why do we care about it? Well the reason is that we can use the multiplication property described in Section 16.4.2 to extract two new matrices that contain new (or at least not obvious, especially for large two-mode networks), information from the original affiliation matrix. The first is called the person overlap matrix (written \\(O^P\\)), this is defined for an original affiliation matrix, in which people are listed in the rows and groups, events, or project, listed in the columns using the following matrix equation:\n\\[\n    O^P = A^{ }_{R \\times C} \\times A^{'}_{C \\times R}\n\\tag{19.6}\\]\n\n19.6.1 The Person Overlap Matrix\nUsing the rules for matrix multiplication discussed Section 16.4.1, the person overlap matrix obtained using the affiliation matrix shown in Table 19.1 is shown in Table 19.5.\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nGabriela\n4\n2\n3\n1\n2\n0\n\n\nParker\n2\n2\n1\n1\n0\n0\n\n\nBrandon\n3\n1\n3\n1\n2\n0\n\n\nMarie\n1\n1\n1\n2\n0\n1\n\n\nRahul\n2\n0\n2\n0\n2\n0\n\n\nMinjoo\n0\n0\n0\n1\n0\n1\n\n\n\nTable 19.5: Person Overlap Matrix.\n\n\nThe person overlap matrix transforms the initial rectangular affiliation matrix, which has people in the rows and groups in the columns, to a square matrix, which like the usual relationship matrices we have been dealing with, feature people in both the rows and the columns. Each entry in the person overlap matrix \\(o^P_{ij}\\) now gives us the number of groups in which person i and j mutually belong to (Breiger 1974). So we learn that Gabriela and Brandon have three memberships in common (I bet they seen another a lot!) but that Rahul and Parker have no memberships in common (so they are less likely to encounter one another).\nNote also that, in the person overlap matrix, (in contrast to the usual adjacency matrix), there are valid entries along the diagonal cells (\\(o^P_{ii}\\)). These cells now record the total number of memberships that the node corresponding to that row (or column) has. Which we ascertained by computing the node centralities in the original affiliation matrix using Equation 19.2). You can see that the vector of degree centralities shown in Table 19.2) is the same as the vector formed by the diagonal entries in ?tbl-comem).\n\n\n19.6.2 The Group Overlap Matrix\nIn the same way we can compute the person overlap matrix, it is possible to calculate another matrix, called the group overlap matrix (written \\(O^G\\)), this time by multiplying the transpose of the original affiliation matrix times the original We do that using the following equation:\n\\[\n    O^G = A^{'}_{C \\times R} \\times A^{ }_{R \\times C}\n\\tag{19.7}\\]\nRecall from Section 16.4 that matrix multiplication is not commutative (if \\(A\\) is a rectangular matrix, then \\(A \\times A^{'} \\neq A^{'} \\times A\\))), so Equation 19.7 gives you a different answer than Equation 19.6. The result is shown in Table 19.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nFashion\n2\n1\n2\n1\n0\n\n\nNerdfighters\n1\n3\n2\n3\n0\n\n\nMagic\n2\n2\n4\n2\n1\n\n\nSuper Smash Brs.\n1\n3\n2\n3\n0\n\n\nCheese\n0\n0\n1\n0\n2\n\n\n\nTable 19.6: Group Overlap Matrix.\n\n\nThe group overlap matrix (O), like the person overlap matrix, is also square. But this time it has groups in both the rows and columns. Each cell in the group overlap matrix \\(o^P_{ij}\\) records the number of people groups i and groups j have in common (Breiger 1974). Thus, we learn that the Super Smash Brothers group and the Nerdfighters groups share three members in common but that the Super Smash Brothers and the Cheese group have no members in common (pointing to a disaffinity between these activities).\nNote that both the person and group overlap matrices are symmetric. It is easy to see why this is; if I have three group overlaps with you, then you by definition also have three group overlaps with me; if group A has three members in common with group B, then group B has three members in common with group A. This means that if they were to be taken as representing a network, then the resulting graph would be undirected (but weighted because there can be more or less overlap between people and groups). We will see how to do that below."
  },
  {
    "objectID": "lesson-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "href": "lesson-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "title": "19  Affiliation Networks",
    "section": "19.7 Overlapping Node Neighborhoods in Two-Mode Networks",
    "text": "19.7 Overlapping Node Neighborhoods in Two-Mode Networks\nThe notion of overlap used to construct the person and group overlap matrix is the same as the idea of overlapping node neighborhoods for regular networks, discussed in Chapter 3 Thus, while nodes of the same kind cannot be connected in a two-mode network (by construction), they can share neighbors. In a two-mode network if a node belong to one of the vertex sets, let’s say \\(V_1\\), then all of their neighbors have to belong to the other vertex set (\\(V_2\\)) and vice versa.\nFor instance, in Figure 19.4), Gabriela’s node neighborhood is:\n\\[\n    Gab_{NN} = \\{Fashion, Nerdfighters, Magic, SuperSmashBros\\}\n\\]\nRahul’s node neiborhood is:\n\\[\n    Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nThe intersection between their neighborhoods is:\n\\[\n    Gab_{NN} \\cap Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nSo now we can see that the number “2” recorded in the cell that corresponds to Gabriela and Rahul in the person overlap matrix shown in Table 19.5) is the cardinality of the subset formed by the intersection of their two neighborhoods, which in this case contain two members (the Nerdfighters and Super Smash Brothers clubs). The same procedure can be used to figure out the overlap between the node neighborhoods of groups (which happen to be subsets of people in the larger two-mode network)."
  },
  {
    "objectID": "lesson-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "href": "lesson-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "title": "19  Affiliation Networks",
    "section": "19.8 One Mode Projections of two-mode Networks",
    "text": "19.8 One Mode Projections of two-mode Networks\nNote that both the comembership and group overlap matrices, being square matrix with values that go beyond zero and one in the cells, look like a lot like the adjacency matrix that could be obtained from a weighted graph as discussed in the lesson on types of graphs.\n\n\n\n\n\nFigure 19.5: One mode (persons) projection of the original bipartite graph.\n\n\n\n\nSo using formulas Equation 19.6) and Equation 19.7), it is possible to go from a two-mode network in which no links exist between nodes of the same kind, to a weighted graph, in which the links between nodes of the same kind are defined by the overlap of their neighborhoods in the original bipartite graph. As we noted earlier, this is called the one mode projection of the two-mode network. Each two-mode network thus has two one mode projection one for each node set.\n\n\n\n\n\nFigure 19.6: One mode (groups) projection of the original bipartite graph.\n\n\n\n\nThe one mode projection for the person node set of the bipartite graph show in Figure 19.4) is shown in Figure 19.5), this is an undirected weighted graph with the edge weight between people being set to the number of comemberships between each dyad as recorded in the person overlap matrix shown in Table 19.5). In this respect, the number of comemberships can be seen as a proxy of the tie strength between two people, when we only have information on their affiliations. As the Figure shows, Brandon, Gabriela and Rahul form a tightly connected clique, given the number of memberships they share. Minjoo, who does not share many affiliations with anyone, stands toward the periphery of the person-to-person comembership network.\nThe corresponding one-mode projection for the group node set is shown in Figure 19.6). This weighted graph can be read the same way: The thickness of the ties between groups are proportion to the people they share as recorded in the group overlap matrix shown in Table 19.6), thus speaking to the similarity or strength of connectivity between groups.\nSo we see, as we noted before, that Super Smash Brothers and Nerdfighters are tightly connected, but that the Cheese Club is largely peripheral in the group-to-group network. This peripheral status mirrors the marginal status of Minjoo (one of the few members of the Cheese Club) in the person-to-person network.\nThe fact that peripheral people belong peripheral groups and central people belong to central groups encodes a fundamental principle in the analysis of two-mode networks (Breiger 1974) and that is the duality principle.\nThe duality principle in two-mode network analysis says that the position of people in a two-mode network is defined by the positions the groups they affiliate with occupy, and in the same way, the position of the groups in a two-mode network is defined by the positions of the people that belong to them (Bonacich 1991)."
  },
  {
    "objectID": "lesson-affiliation-networks.html#references",
    "href": "lesson-affiliation-networks.html#references",
    "title": "19  Affiliation Networks",
    "section": "References",
    "text": "References\n\n\n\n\nBonacich, Phillip. 1991. “Simultaneous Group and Individual Centralities.” Social Networks 13 (2): 155–68.\n\n\nBreiger, Ronald L. 1974. “The Duality of Persons and Groups.” Social Forces 53 (2): 181–90.\n\n\nFaust, Katherine. 1997. “Centrality in Affiliation Networks.” Social Networks 19 (2): 157–91.\n\n\nMizruchi, Mark S. 1983. “Who Controls Whom? An Examination of the Relation Between Management and Boards of Directors in Large American Corporations.” Academy of Management Review 8 (3): 426–35."
  },
  {
    "objectID": "lesson-centrality.html#the-big-three-centrality-metrics",
    "href": "lesson-centrality.html#the-big-three-centrality-metrics",
    "title": "20  Centrality",
    "section": "20.1 The “big three” centrality metrics",
    "text": "20.1 The “big three” centrality metrics\nLinton Freeman (1979), in the aforementioned paper, defined the “big three” classic centrality metrics, roughly corresponding to the extent that a node accumulates one of the three network goods mentioned above. - So the degree centrality metric deal with nodes that have more edges directly incident upon them (Nieminen 1974). - The closeness centrality metric has to do with nodes that can reach more nodes via smallest shortest paths and thus accumulate as many of these paths in which they figure as the origin node as possible (Sabidussi 1966). - Finally, the betweenness centrality metric has to do with a node’s accumulation of the largest share of shortest paths in which they intermediate between two other nodes, and thus featuring them as one of the inner nodes in the paths between others (Freeman 1977).\nOther centrality metrics can be seen as generalizations or special cases of any of these three basic notions (Borgatti 2005).\nThe rest of the lesson goes over the basic interpretation and calculation (using the graph theory and matrix algebra tools discussed in previous lessons) of “big three” centrality metrics."
  },
  {
    "objectID": "lesson-centrality.html#the-star-graph",
    "href": "lesson-centrality.html#the-star-graph",
    "title": "20  Centrality",
    "section": "20.2 The Star Graph",
    "text": "20.2 The Star Graph\nFreeman showed that the three basic measures reach their theoretical maximum for the central node in a star graph, such as the one shown in Figure 20.1).\n\n\n\n\n\nFigure 20.1: A star graph\n\n\n\n\nA star graph is a graph containing a central or inner node (in Figure 20.1, node A), who is connected to all the other nodes in the graph, called the satellite or outer nodes (in Figure 20.1, nodes B through F). These nodes in contrast have only one connection and that is to the central node, none among themselves.\nBecause of these restrictions, it is easy to see that if \\(G = (V, E)\\) is a star graph of order \\(n\\), then we know that that graph size \\(m = |E|\\) (the size of the edge set), has to be \\(n-1\\). So in the example shown in Figure 20.1, \\(n =7\\) and \\(m = n-1 = 7-1=6\\). Neat!"
  },
  {
    "objectID": "lesson-centrality.html#degree-centrality",
    "href": "lesson-centrality.html#degree-centrality",
    "title": "20  Centrality",
    "section": "20.3 Degree Centrality",
    "text": "20.3 Degree Centrality\nThe first way of defining centrality is simply as a measure of how many alters an ego is connected to. This simply takes a node’s degree as introduced in the lesson on graph theory, and begins to consider this measure as a reflection of importance of the node in the network. The logic is that those with more direct connections to others, compared to those with fewer, hold a more prominent place in the network.\nOnce we have constructed the adjacency matrix for the network (A), then degree centrality is easy to calculate. As Equation 20.1) for a given node i the degree centrality is given by summing the entries of its corresponding row.\n\\[\n  C_i^{DEG} = \\sum_{j= 1}^{n}a_{ij}\n\\tag{20.1}\\]\nEquation 20.1 thus ranks each node in the graph based on the number of other nodes that it is adjacent to. Just like real life, some nodes will be popular (they will be adjacent to lots of other nodes), while others will be unpopular.\nAlthough it might seem a simple task to just add up the number of connections of each node, that is essentially what the below mathematical equation is doing! Mathematical notation plays an important role in expressing network measures in succinct formats.\nFor instance, if we were to use Equation 20.1 to calculate the degree centrality of each node from the symmetric adjacency matrix corresponding to the graph shown in Figure 8.1) then we would end up with the following degree centralities for each node:\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 20.1: Degree centralities of nodes in an undirected graph.\n\n\n\n20.3.1 How to Read Equations for Matrix Operations\nBefore we continue, a note on something obvious. A lot of centrality measures are expressed as equations and these can be hard to interpret initially. Depending on your background in math, you may or may not already know how to interpret Equation 20.1). Essentially, the number at the bottom of the sigma is where to start. The number at the top of the Sigma symbol (\\(\\Sigma\\)) is where to end. The equation to the left is the operation to be performed. Thus, one reads the Equation 20.1 as, starting at column \\(j=1\\), and ending at the last possible column \\(n\\) (\\(n\\) is simply the total number of rows in the matrix, \\(n\\) means to go to the final value in the matrix), add up all possible values of the cells designated by the row \\(i\\) and column \\(j\\) combination in matrix A. Thus, to calculate the degree centrality of each \\(j = a, b, c\\) in the below matrix, each of the following calculations would be performed.\n\n\nTable 20.2: Simple matrix.\n\n\n\na\nb\nc\n\n\n\n\na\n-\n1\n0\n\n\nb\n1\n-\n1\n\n\nc\n0\n1\n-\n\n\n\n\n\\(C_D(a)=aa+ab+ac=1\\)\n\\(C_D(b)=ba+bb+bc=2\\)\n\\(C_D(c)=ca+cb+cb=1\\)\nIn the same way if we had the formula:\n\\[\n  C_D(j) = \\sum_{i = 1}^{n}a_{ij}\n\\]\nThen it would be telling us to sum values of each column \\(j\\) down each row \\(i\\):\n\\(C_D(a)=aa+ba+ca=1\\)\n\\(C_D(b)=ab+bb+cb=2\\)\n\\(C_D(c)=ac+bc+cc=1\\)\nThe sigma notation is useful for summarizing this repetitive process in a simple, condensed form."
  },
  {
    "objectID": "lesson-centrality.html#indegree-and-outdegree-centrality",
    "href": "lesson-centrality.html#indegree-and-outdegree-centrality",
    "title": "20  Centrality",
    "section": "20.4 Indegree and Outdegree Centrality",
    "text": "20.4 Indegree and Outdegree Centrality\nIf we are talking about a directed graph, then there are two types of degree centralities that can be calculated. On the one hand, we may be interested in how central a node is in terms of sociability or expansiveness that is how many other nodes in the graph a given node sends links to. This is called the outdegree centrality of that node, written as \\(C_i^{OUT}\\). As with the undirected case, this is computed by summing across the rows of the asymmetric adjacency matrix corresponding to the directed graph in question, using Equation 20.1:\n\\[\n  C_i^{OUT} = \\sum_ja_{ij}\n\\tag{20.2}\\]\nHowever, in a directed graph, we may also be interested in how popular or sought after by others a given node is. That is, how many other actors send ties to that node. In which case we need to sum across the columns of the asymmetric adjacency matrix, and modify the formula as follows:\n\\[\n  C_i^{IN} = \\sum_ia_{ij}\n\\tag{20.3}\\]\nNote that in this version of the equation, we are summing over j (the columns) not over i (the rows) as given by subscript under the \\(\\sum\\) symbol.\nFor instance, if we were to use equations Equation 20.2 and Equation 20.2 to calculate the outdegree and indegree centrality of each node from the asymmetric adjacency matrix corresponding to the graph shown in Figure 8.2), then we would up with the following centralities for each node:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nOutdegre\n2\n2\n1\n1\n2\n1\n2\n\n\nIndegree\n2\n3\n1\n3\n0\n2\n0\n\n\n\nTable 20.3: Out and Indegree centralities of nodes in a directed graph.\n\n\nJust like the degree centrality for undirected graphs, the outdegree and indegree centralities rank each node in a directed graph. The first, outdegree centrality, ranks each node based on the number of other nodes that they are connected to. This is a kind of popularity based on sociability, or the tendency to seek out the company of others. The second, indegree centrality, ranks each node in the graph based on the number of other nodes that connect to that node. This is a kind of popularity based on on being sought after a kind of status.\n\n20.4.1 Normalized Degree Centrality\nWhen we compute the degree centrality of a node, are counting the number of other nodes that they are connected to. Obviously, the more nodes there are to connect to, the more opportunities there will be to reach a larger number. But what happens if we wanted to compare the degree centrality of nodes in two very different networks?\nFor instance, if your high-school has one thousand people and you have twenty friends, that’s very different from having twenty friends in a high-school of only one hundred people. It seems like the second person, with twenty friends (covering 20% of the population) in a high-school of one-hundred people is definitely more popular than the second person with twenty friends (covering 2% of the population), in a high school with one thousand people.\nThat’s why Freeman Freeman (1979) proposed normalizing the degree centrality of each node by the maximum possible it can take in a given network. As you may have guessed, the maximum degree in a network is \\(N-1\\) the order of the graph minus one. Essentialy, everyone but you!\nWe can compute the normalized degree centrality using the following equation:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{C_{i}^{DEG}}{N-1}\n\\tag{20.4}\\]\nWhere we just divide the regular degree centrality computed using Equation 20.1 by the order of the graph minus one. This will be equal to \\(1.0\\) if a person knows everyone and \\(0\\) is a person knows no one. For all the other nodes it will be a number between zero and one.\nMoreover, this measure is sensitive to the order of the graph. Thus, for a person with twenty friends in a high-school of a thousand people, the normalized degree centrality is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{1000-1}= 0.02\n\\tag{20.5}\\]\nBut for the person with the same twenty friends in a high-school of one-hundred people, it is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{100-1}= 0.20\n\\tag{20.6}\\]\nIndicating that a person with the same number of friends in the smaller place is indeed more central!"
  },
  {
    "objectID": "lesson-centrality.html#closeness-centrality",
    "href": "lesson-centrality.html#closeness-centrality",
    "title": "20  Centrality",
    "section": "20.5 Closeness Centrality",
    "text": "20.5 Closeness Centrality\nSometimes it not important how many people you directly connected to. Instead, what is important is that you are indirectly connected to a lot of others. As we saw in the lesson on indirect connectivity, the best way to conceptualize indirect connectivity in social networks is via the idea of shortest paths. So if you can reach the most other people in the network via shortest paths with only a few hops, then you are better connected that someone who has to use longer paths to reach the same other people.\n\n\n\n\n\nFigure 20.2: An undirected graph showing the node with the maximum closeness centrality (in red).\n\n\n\n\nThis insight serves as an inspiration for a measure of centrality based on closeness. The closeness between two nodes is the inverse of the geodesic distance them (Bavelas 1950). Recall that the geodesic distance is given by the length of the shortest path linking two nodes in the graph. The smallest the length of the shortest path separating two nodes in the graph, the closer the two nodes and vice versa.\nRemember that for any number \\(n\\), the mathematical operation of taking the inverse simply means dividing one by that number. So, the inverse of \\(n\\) is \\(\\frac{1}{n}\\). This means that if \\(d_{ij}\\) is the geodesic distance between nodes i and j in graph \\(G\\), then the closeness between two nodes is \\(\\frac{1}{d+_{ij}}\\).\nThe information on the pairwise geodesic distances between every pair of nodes in a given graph is captured in the geodesic distance matrix, as discussed in Chapter 15. For instance, take the graph shown in Figure 20.2. The distance matrix for this graph is shown in Table 20.4.\n\n\n\n\n\n\n\n\nE\nA\nM\nL\nB\nJ\nG\nN\nF\nH\nD\nC\nK\nI\n\n\n\n\nE\n0\n1\n2\n1\n2\n1\n2\n1\n2\n1\n3\n2\n2\n2\n\n\nA\n1\n0\n3\n1\n3\n2\n2\n2\n3\n2\n2\n1\n1\n2\n\n\nM\n2\n3\n0\n3\n1\n2\n2\n1\n2\n2\n1\n3\n2\n4\n\n\nL\n1\n1\n3\n0\n3\n2\n1\n2\n2\n2\n2\n1\n2\n1\n\n\nB\n2\n3\n1\n3\n0\n1\n2\n2\n1\n2\n2\n2\n3\n4\n\n\nJ\n1\n2\n2\n2\n1\n0\n2\n1\n1\n2\n2\n1\n3\n3\n\n\nG\n2\n2\n2\n1\n2\n2\n0\n3\n1\n2\n1\n2\n2\n2\n\n\nN\n1\n2\n1\n2\n2\n1\n3\n0\n2\n1\n2\n2\n2\n3\n\n\nF\n2\n3\n2\n2\n1\n1\n1\n2\n0\n1\n1\n2\n2\n3\n\n\nH\n1\n2\n2\n2\n2\n2\n2\n1\n1\n0\n2\n3\n1\n3\n\n\nD\n3\n2\n1\n2\n2\n2\n1\n2\n1\n2\n0\n3\n1\n3\n\n\nC\n2\n1\n3\n1\n2\n1\n2\n2\n2\n3\n3\n0\n2\n2\n\n\nK\n2\n1\n2\n2\n3\n3\n2\n2\n2\n1\n1\n2\n0\n3\n\n\nI\n2\n2\n4\n1\n4\n3\n2\n3\n3\n3\n3\n2\n3\n0\n\n\n\nTable 20.4: Geodesic distance matrix for an undirected graph.\n\n\nAs shown in Table 20.4, a node like I, who seems to be at the outskirts of the network, also shows up as having the largest geodesic distances from other nodes in the graph. Other nodes, like E, G, and L seem to be “closer” to others, in terms of having to traverse smaller geodesic distances to reach them.\nThat means that we can use the distance table to come up with a measure of centrality called closeness centrality for each node. We can do that by adding up the entries corresponding to each row in the distance matrix (\\(\\sum_j d_{ij}\\)), to get a summary the total pairwise distances separating the node corresponding to row i in the matrix from the other nodes listed in each column j.\nNote that because closeness is better than “farness,” we would want the node with highest closeness centrality to be the one with the smallest sum of pairwise distances. This can be calculated using the following equation:\n\\[\n  C_i^{CLOS} = \\frac{1}{\\sum_jd_{ij}}\n\\tag{20.7}\\]\nIn Equation 20.7, the denominator is the sum across each column j, for each row i in Table 20.4 which corresponds to the distance between node i and each of the other nodes in the graph j (skipping the diagonal cell when \\(i=j\\), because the geodesic distance of node to itself is always zero!).\nAs noted, we take the mathematical inverse of this quantity, dividing one by the sum of the distances, so that way, the smallest number comes out on top and the bigger number comes out on the bottom (since, as we said, we want to measure closeness not “farness.”)\nLet’s see how this work for the graph in Figure 20.2. First, we get the row sums of geodesic distances from Table 20.4. These are shown in the first column of Table 20.5, under the heading “Sum of Distances.” This seems to work; node \\(E\\) has the smallest number here (\\(\\sum_j d_{Ej} = 22\\)) suggesting it can reach the most nodes via the shortest paths. Node \\(I\\) has the largest number (\\(\\sum_j d_{Ij} = 35\\)) indicating it is the most isolated from the other nodes.\n\n\n\n\n\n \n  \n      \n    Sum of Distances (d) \n    Inverse (1/d) \n    Normalized (N-1/d) \n  \n \n\n  \n    E \n    22 \n    0.045 \n    0.59 \n  \n  \n    A \n    25 \n    0.040 \n    0.52 \n  \n  \n    M \n    28 \n    0.036 \n    0.46 \n  \n  \n    L \n    23 \n    0.043 \n    0.57 \n  \n  \n    B \n    28 \n    0.036 \n    0.46 \n  \n  \n    J \n    23 \n    0.043 \n    0.57 \n  \n  \n    G \n    24 \n    0.042 \n    0.54 \n  \n  \n    N \n    24 \n    0.042 \n    0.54 \n  \n  \n    F \n    23 \n    0.043 \n    0.57 \n  \n  \n    H \n    24 \n    0.042 \n    0.54 \n  \n  \n    D \n    25 \n    0.040 \n    0.52 \n  \n  \n    C \n    26 \n    0.038 \n    0.50 \n  \n  \n    K \n    26 \n    0.038 \n    0.50 \n  \n  \n    I \n    35 \n    0.029 \n    0.37 \n  \n\n\n\nTable 20.5:  Sum of geodesic distances for each node in an undirected graph and its inverse. \n\n\nBut we want closeness, not farness, so the second column of Table 20.5 shows what happens when we divide one by the number in the second column. Now, node \\(E\\) has the largest score \\(CC^{CLOS}_E = 0.045\\) which is what we want.\nHowever, because we are dividing one by a relatively large number, we end up with a bunch of small decimal numbers as centrality scores, and like it happened with degree, this number is sensitive to how big the network is (the larger the network, the more likely there is to be really long short paths). So Freeman (1979) proposes a normalized version of closeness that takes into account network size. It is a variation of Equation 20.7:\n\\[\n  C_i^{CLOS} = \\frac{N-1}{\\sum_jd_{ij}}\n\\tag{20.8}\\]\nEquation 20.8 is the same as Equation 20.7, except that instead of dividing one by the sum of distances, we divide \\(N-1\\) by the sum of distances, where \\(N\\) is the order of the graph (the number of nodes). In this case, \\(N=14\\).\nNormalizing the sum of distances shown in the second column of Table 20.5 according to Equation 20.8, gives us the centrality scores shown in the fourth column of the table, under the heading “Normalized.” These scores range from zero to one, with one being the maximum possible closeness centrality score for that graph.\nThe normalized closeness centrality scores listed in the fourth column of Table 20.5 agree with our informal impressions. Node I comes out at the bottom (\\(CC_I^{CLOS} = 0.37\\)), showing it to be the one with the least closeness centrality, given the relatively large geodesic distances separating it from the other nodes in the graph. Node E (marked red in Figure 20.2) comes out on top (\\(CC_E^{CLOS} = 0.59\\)), given its relative geodesic proximity to other nodes in the graph.\nAs we will see later, having closeness centrality information for nodes in a graph can be useful. For instance, if Figure 20.2 was a social network, and we wanted to spread an innovation or a new product among the actors in the fastest amount of time, we would want to give it to node E first. Note however that if something bad (like a disease) was spreading across the network, then it would also be very bad if actor E got it first!4"
  },
  {
    "objectID": "lesson-centrality.html#houston-we-have-a-problem",
    "href": "lesson-centrality.html#houston-we-have-a-problem",
    "title": "20  Centrality",
    "section": "20.6 Houston, We Have a Problem",
    "text": "20.6 Houston, We Have a Problem\nSo far, so good. Closeness seems to be a great measure of node importance, giving us a sense of who can reach most others in a network in the most efficient way. However, what would happen if we tried to compute closeness centrality for a disconnected graph like the one shown in Figure Figure 7.2 (b)? Well, the shortest paths distance matrix for that graph looks like the one in Table 20.6.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\nInf\nInf\nInf\nInf\n\n\nB\n1\n0\n1\n1\n2\nInf\nInf\nInf\nInf\n\n\nC\n1\n1\n0\n1\n1\nInf\nInf\nInf\nInf\n\n\nD\n1\n1\n1\n0\n1\nInf\nInf\nInf\nInf\n\n\nE\n1\n2\n1\n1\n0\nInf\nInf\nInf\nInf\n\n\nF\nInf\nInf\nInf\nInf\nInf\n0\n1\n1\n1\n\n\nG\nInf\nInf\nInf\nInf\nInf\n1\n0\n1\n1\n\n\nH\nInf\nInf\nInf\nInf\nInf\n1\n1\n0\n1\n\n\nI\nInf\nInf\nInf\nInf\nInf\n1\n1\n1\n0\n\n\n\nTable 20.6: Geodesic distance matrix for an undirected, disconnected graph.\n\n\nNote that in Table 20.6, pairs of nodes that cannot reach one another in the disconnected graph, get a geodesic distance of “Inf” (infinity) in the respective cell of the geodesic distance matrix. This is a problem because when we compute the row sums of the geodesic distance matrix to try to calculate centrality according to Equation 20.7, we get the “numbers” shown in Table 20.7.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n\nTable 20.7: Row sums of a geodesic distance matrix from a disconnected graph.\n\n\nSo that’s a bummer since all the “numbers” in Table 20.7, are just infinity. Not to get too philosophical, but the problem is that when you add any number to “infinity,” the answer is, well, infinity.5 This means that closeness centrality is only defined for connected graphs. When it comes to disconnected graphs, we are out of luck.\nThankfully, there is a solution develoed by Beauchamp (1965). It consists of a modification of Equation 20.7 called harmonic closeness centrality. The formula goes as follows:\n\\[\n  C_i^{HARM} = \\frac{1}{N-1}\\sum_j\\frac{1}{d_{ij}}\n\\tag{20.9}\\]\nNow, this might seem like we just re-arranged the stuff in Equation 20.8, and indeed that’s what we did! But the re-arrangement matters a lot, because it changes the order in which we do the various arithmetic operations (Boldi and Vigna 2014).\nSo, in English, while Equation 20.8 says “first sum the geodesic distances for each node (to get the denominator), and then divide \\(N-1\\) by this sum,” Equation 20.9 says “first divide one by the geodesic distance, and then sum the result of all these divisions, and then multiply this sum by one over \\(N-1\\).\nOnce again, the philosophy of mathematical infinity kicks in here, since the main difference is that one divided by infinity is actually a real number: zero.6\nSo let’s check by taking every entry in Table 20.6 and dividing one by the number in each cell (except for the diagonals, which we don’t care about). The results are shown in Table 20.8.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1.0\n1\n1\n1.0\n0\n0\n0\n0\n\n\nB\n1\n0.0\n1\n1\n0.5\n0\n0\n0\n0\n\n\nC\n1\n1.0\n0\n1\n1.0\n0\n0\n0\n0\n\n\nD\n1\n1.0\n1\n0\n1.0\n0\n0\n0\n0\n\n\nE\n1\n0.5\n1\n1\n0.0\n0\n0\n0\n0\n\n\nF\n0\n0.0\n0\n0\n0.0\n0\n1\n1\n1\n\n\nG\n0\n0.0\n0\n0\n0.0\n1\n0\n1\n1\n\n\nH\n0\n0.0\n0\n0\n0.0\n1\n1\n0\n1\n\n\nI\n0\n0.0\n0\n0\n0.0\n1\n1\n1\n0\n\n\n\nTable 20.8: Reciprocal of the geodesic distance matrix for an undirected, disconnected graph.\n\n\nBeautiful! Now, instead of weird “Inf”s we have zeroes, which is great because we can add stuff to zero and get a real number back. We can then apply Equation 20.9 to the numbers in Table 20.8 (e.g., computing the sum of each row and then multiplying that by \\(\\frac{1}{N-1}\\)) to get the harmonic closeness centrality for each node in Figure 7.2 (b). These are shown in Table 20.9.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.5\n0.44\n0.5\n0.5\n0.44\n0.38\n0.38\n0.38\n0.38\n\n\n\nTable 20.9: Harmonic Closeness Centrality scores for nodes in a disconnected, undirected graph.\n\n\nGreat! Now we have a measure of closeness centrality we can apply to all kinds of graphs, whether they are connected or disconnected."
  },
  {
    "objectID": "lesson-centrality.html#betweenness-centrality",
    "href": "lesson-centrality.html#betweenness-centrality",
    "title": "20  Centrality",
    "section": "20.7 Betweenness Centrality",
    "text": "20.7 Betweenness Centrality\nRecall that in our discussion of shortest paths between pair of nodes in the lesson on indirect connections, we noted the importance of the inner nodes that intervene or mediate between a node that wants to reach another one. Nodes that stand in these brokerage or gatekeeper slots in the network, occupy an important position (Marsden 1983), and this is different from having a lot of contacts (like degree centrality), or being able to reach lots of other nodes by traversing relatively small distances (like closeness centrality). Instead, this is about being in-between the indirect communications of other nodes in graph. We can compute a centrality metric for each node called betweenness centrality that captures this idea Freeman (1980).\n\n\n\n\n\nFigure 20.3: An undirected graph showing the node with the maximum betweenness centrality (in red)\n\n\n\n\nFor instance, let’s say you were actor K in the network shown in Figure 20.2, and you wanted to know who is the person that you depend on the most to communicate with actor J. Here dependence means that you are forced to “go through them” if I wanted to reach N via a shortest path. One way K could figure this out is by listing every shortest path having them as the origin node and having N as the destination node. After you have this list, you can see which of other other nodes shows up as an inner node—an intermediary or gatekeeper—in those paths the most times.\nThis shortest path list would look like this:\n\n\\(\\{KH, HF, FJ\\}\\)\n\\(\\{KD, DF, FJ\\}\\)\n\\(\\{KH, HN, NJ\\}\\)\n\\(\\{KA, AC, CJ\\}\\)\n\\(\\{KA, AE, EJ\\}\\)\n\\(\\{KH, HE, EJ\\}\\)\n\nThere are six shortest paths of length three indirectly connecting actors K and J in Figure 20.2), with nodes \\(\\{A, C, D, E, F, H, N\\}\\) showing up as an inner node in at least one of those paths. To see which other actor in the network is the most frequent intermediary between J and K, we can create a list with the number of times each of these nodes shows up as an intermediary in this shortest path list. This would look like this:\n\n\n\n\n\n\n\nNode\nFreq.\nProp.\n\n\n\n\nA\n2\n0.33\n\n\nC\n1\n0.17\n\n\nD\n1\n0.17\n\n\nE\n2\n0.33\n\n\nF\n2\n0.33\n\n\nH\n3\n0.50\n\n\nN\n1\n0.17\n\n\n\nTable 20.10: Intermediaries between nodes J and K\n\n\nSo it looks like, looking at the second column of Table 20.10, that H is the other actor that J depends on the most to reach K. A better way to quantify this, is to actually look at the proportion of paths linking J and K that a particular other node (like H) shows up in. Let’s call this \\(p_{K(H)J}\\) which can be read as “the proportion of paths between K and J featuring H as an inner node.” This is shown in the third column of Table 20.10 We can write this in equation form like this:\n\\[\n  p_{K(H)J} = \\frac{g_{K(H)J}}{g_{KJ}} = \\frac{3}{6} = 0.5\n\\tag{20.10}\\]\nIn Equation 20.10, \\(g_{K(H)J}\\) is the number of shortest paths linking K and J featuring H as an inner node, and \\(g_{KJ}\\) is the total number of paths linking K and J. Freeman (1980) calls this measure the pair-dependency of actor K on actor H to reach a given node J. In this case, \\(g_{K(H)J} = 3\\) and \\(g_{KJ} = 6\\), which means that actor K depends on actor H for fifty percent of their shortest path access to J. Making H the actor in the network J depends on the most to be able to reach J.\nGeneralizing this approach, we can do the same for each triplet of actors i, j, and k in the network. This is the basis for calculating betweenness centrality. That is, we can count the number of times k stands on the shortest path between two other actors i and j. We can all this number \\(g_{i(k)j}\\). We can then divide it by the total number of shortest paths linking actors i and j in the network, which we refer by \\(g_{ij}\\). Remember that two actors can be indirectly linked by multiple shortest paths of the same length, and that we can figure out how many short paths links pairs of actors in the network using the shortest paths matrix.\nThis ratio, written \\(\\frac{g_{i(k)j}}{g_{ij}}\\) then gives us the proportion of shortest paths in the network that have i and j as the end nodes and that feature k as an intermediary inner node. This can range from zero (no shortest paths between i and j feature node k as an intermediary) to one (all the shortest paths between i and j feature node k as an intermediary).\nWe can then use the following equation to compute the average of this proportion for each node k across each pair of actors in the network i and j:\n\\[\n  C_k^{BET} = \\sum_i \\sum_j \\frac{g_{ikj}}{g_{ij}}\n\\tag{20.11}\\]\nComputing this quantity for the graph shown in Figure 20.3, yields the betweenness centrality scores shown in Table 20.11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nJ\nK\nL\nM\nN\nI\n\n\n\n\n5\n1.5\n3\n6.8\n11.4\n9.3\n6.3\n4.8\n10.8\n3.7\n16.4\n2.8\n5.3\n0\n\n\n\nTable 20.11: Betweenness centrality scores.\n\n\nThe numbers in the Table can be readily interpreted as percentages. Thus, the fact that node J has a a betweenness centrality score of 10.8 tells us that they stand in about 11% of the shortest paths between pairs of nodes in the graph. Interestingly, as shown in Figure 20.3, the node that ends up with the highest betweenness score is L (\\(C_L^{BET} = 16.4\\)), mostly due to the fact that node I, who has the lowest possible betweenness score of zero, depends on this node for access to every other actor in the network.\nNote also that two different nodes end up being ranked first on closeness and betweenness centrality in the same network (compare the red nodes in Figure 20.2 and Figure 20.3). This tells us that closeness and betweenness are analytically distinct measures of node position. One (closeness) gets at reachability, and the other (betweenness) gets at intermediation potential."
  },
  {
    "objectID": "lesson-centrality.html#the-big-three-centralities-in-the-star-graph",
    "href": "lesson-centrality.html#the-big-three-centralities-in-the-star-graph",
    "title": "20  Centrality",
    "section": "20.8 The Big Three Centralities in the Star Graph",
    "text": "20.8 The Big Three Centralities in the Star Graph\nDegree, Closeness, and Betweenness centralities have an interesting property that provides a conceptual connection between them (Freeman 1979). Consider the star graph shown in Figure 20.1 with central node A. The degree, closeness, and betweenness centralities of the different nodes are shown in Table 20.12).\nOf course, by definition, we know beforehand that the central node in a star graph has to have the highest degree, since the degree of peripheral nodes is fixed to one and the degree of the central node is always \\(n-1\\), where \\(n\\) is the graph order.\nHowever, note also that the central node has to have the highest closeness, since it is directed by a path of length one (and edge) to every peripheral node, but each peripheral node can only reach other peripheral nodes in the graph by a path of length two. They are farther away from other nodes than the central node.\nFinally, note that the central node in the star will also always have the highest betweenness because each of the paths of length two connecting every pair of peripheral nodes to one another has to include the central node. So it serves as the intermediary between any communication between peripheral nodes.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nDegree\n6.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nCloseness\n8.2\n4.5\n4.5\n4.5\n4.5\n4.5\n4.5\n\n\nBetwenness\n15.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\nTable 20.12: Centralities in a star graph of order 7.\n\n\nThe mathematical sociologist Linton Freeman (1979) thus thinks that the “big three” centrality measures are the big three precisely because they are maximized for the central node in a star graph."
  },
  {
    "objectID": "lesson-centrality.html#references",
    "href": "lesson-centrality.html#references",
    "title": "20  Centrality",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBavelas, Alex. 1950. “Communication Patterns in Task-Oriented Groups.” The Journal of the Acoustical Society of America 22 (6): 725–30.\n\n\nBeauchamp, Murray A. 1965. “An Improved Index of Centrality.” Behavioral Science 10 (2): 161–63.\n\n\nBoldi, Paolo, and Sebastiano Vigna. 2014. “Axioms for Centrality.” Internet Mathematics 10 (3-4): 222–62.\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nBorgatti, Stephen P, and Martin G Everett. 2006. “A Graph-Theoretic Perspective on Centrality.” Social Networks 28 (4): 466–84.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41.\n\n\n———. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39.\n\n\n———. 1980. “The Gatekeeper, Pair-Dependency and Structural Centrality.” Quality and Quantity 14 (4): 585–92.\n\n\nMarsden, Peter V. 1983. “Restricted Access in Networks and Models of Power.” American Journal of Sociology 88 (4): 686–717.\n\n\nNieminen, Juhani. 1974. “On the Centrality in a Graph.” Scandinavian Journal of Psychology 15 (1): 332–36.\n\n\nSabidussi, Gert. 1966. “The Centrality Index of a Graph.” Psychometrika 31 (4): 581–603."
  },
  {
    "objectID": "lesson-status.html#status-as-indegree-centrality",
    "href": "lesson-status.html#status-as-indegree-centrality",
    "title": "21  Status",
    "section": "21.1 Status as Indegree Centrality",
    "text": "21.1 Status as Indegree Centrality\nConsider a network which could be composed of asymmetric ties indicating some kind of positive regard or esteem that node i has for node j, represented by the directed graph in Figure 21.1, with result adjacency matrix shown in Table 21.1.\nThe directed edges could be “thinks the other person is great,’’ or”respects the other person” or “would take advice from tha person.” Note that all these relations are asymmetric you can think that person A is great, but that does not mean they think the same thing about you.\n\n\n\n\n\nFigure 21.1: A directed graph.\n\n\n\n\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 21.1:  Adjacency matrix corresponding to a directed graph. \n\n\nAn easy approach is to measure the status of each node by counting the number of direct nominations they get from others. This would be trying to measure the status of each node by using their indegree centrality. The results are shown in Table 21.2 (a). According to the table, nodes \\(F\\) and \\(I\\) are the highest status nodes in Figure 21.1 because they each receive five and four nominations respectively They are followed by nodes \\(\\{A, B, E, J\\}\\) who receive three nominations each. Node \\(C\\) is the lowest status, as no one thinks they are important.\nHowever, the problem with using the number of incoming nominations as a measure of status is that the indegree centrality only measures the number of ties that are incoming to each node, but it does not differentiate between who sends each tie. Every nomination counts as the same.\nBut as we noted, the whole point of the idea of status is that you gain status when you receive ties from high-status others, and their status is established by their receiving ties from high status others, and so forth. So indegree centrality won’t do as a measure of status in social networks, if we aim to capture the full idea behind the concept."
  },
  {
    "objectID": "lesson-status.html#using-exogeneous-status-information",
    "href": "lesson-status.html#using-exogeneous-status-information",
    "title": "21  Status",
    "section": "21.2 Using Exogeneous Status Information",
    "text": "21.2 Using Exogeneous Status Information\nAnother possibility is that we check some measure of status that comes from outside the network (the fancy word for this is exogenous). This could be for instance, the position of each node in the organizational chart, with ten indicating a top position and zero indicating an entry-level position. We could record this information using a \\(1 \\times 12\\) column vector where the exogenous status of each node i is given by each entry \\(\\mathbf{b}_i\\). Such an exogenous status score vector is shown in Table 21.2. In the table, larger numbers indicate higher status.\n\n\n\n\n\nTable 21.2: Example of estimating status using exogeneous scores for nodes.\n\n\n\n\n(a) Indegree Centrality \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    3 \n    3 \n    0 \n    1 \n    3 \n    5 \n    1 \n    2 \n    4 \n    3 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Exogenous Status Scores \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    2 \n    5 \n    4 \n    6 \n    8 \n    8 \n    2 \n    4 \n    9 \n    4 \n    9 \n    6 \n  \n\n\n\n\n\n\n\n\n(c) Status Scores Based on Exogeneous Information \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    17 \n    14 \n    0 \n    8 \n    22 \n    24 \n    5 \n    10 \n    17 \n    14 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n\n(d) Status Scores Based on Endogeous Information (In-degree) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    6 \n    11 \n    0 \n    5 \n    12 \n    7 \n    3 \n    1 \n    5 \n    8 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(e) Status Scores Based on All Indirect Paths (Katz) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    12 \n    25 \n    0 \n    8 \n    27 \n    17 \n    12 \n    2 \n    14 \n    19 \n    2 \n    4 \n  \n\n\n\n\n\n\n\n\n(f) Status Scores Based on All Indirect Paths and Exogeneous Status Information (Hubbell) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    74 \n    143 \n    4 \n    56 \n    160 \n    111 \n    66 \n    20 \n    84 \n    119 \n    23 \n    32 \n  \n\n\n\n\n\n\nNow the status score for each person \\(\\mathbf{s}\\) can be determined by taking each of their incoming nominations and weighting them by the exogenous status score of each of the other people, so that nominations from lower status nodes (like node \\(K\\) in Table 21.2 (b)) count for more than those coming from higher status nodes (like node \\(A\\) in Table 21.2 (b)). To calculate the status of each node we add up the status of each of the nodes that point to it.\nHow do we do this? Recall from Chapter 16, that it is always possible to multiply a square matrix times a column vector of the same length as the matrix’s row and column dimensions and that the result is always another column vector of the same length as the original.\nAccordingly, we can get each person’s weighted status score by taking network’s adjacency matrix \\(\\mathbf{A}\\) and multiplying it by the (in this case, \\(12 \\times 1\\)) column vector of exogenous status scores \\(\\mathbf{b}\\). However, because we want to add up the status scores of the nodes that point to a given node, what we want is the product of the transpose of the network adjacency matrix times the vector of status scores:\n\\[\n  \\mathbf{s}^{ex} = \\mathbf{A}' \\mathbf{b}\n\\tag{21.1}\\]\nWhen we do that, we end up with the status scores shown in Table 21.2 (c).\nAs we can see, the status order is a bit different once we take into account the exogenous status of the other people who nominate each node. Yes, node \\(F\\) is still the highest ranked node, and node \\(C\\) is the lowest ranked. However, node \\(I\\) is no longer the second highest status node, that honor now goes to node \\(E\\). The reason is that while \\(I\\) has a larger indegree than \\(E\\), node \\(I\\)’s in-neighbors, as shown in Figure Figure 21.1 and Table 21.1, \\(N_{in}(I) = \\{A, C, G, K\\}\\) are relatively low status (except for \\(K\\)). \\(E\\)’s in-neighbors, by way of contrast, \\(N_{in}(E) = \\{B, F, I\\}\\) are all high to mid-status."
  },
  {
    "objectID": "lesson-status.html#using-endogeneous-network-information",
    "href": "lesson-status.html#using-endogeneous-network-information",
    "title": "21  Status",
    "section": "21.3 Using Endogeneous Network Information",
    "text": "21.3 Using Endogeneous Network Information\nIt turns out, that in many cases, we don’t have exogenous status information on each node in the network to rely on. In that case, we must rely on endogenous network information to determine the status of each of the other nodes.\nOne approach is just to use original indegree centrality scores shown in Table 21.2 (a) as the status of each other the nodes. We can then say that a node is high status if it is pointed to by other nodes who are also pointed to by many other nodes. Conversely, a node is low status if it is pointed to by other nodes that are not pointed to by many other nodes.\n\\[\n  s^{en} = \\mathbf{A}'d_{in}\n\\tag{21.2}\\]\nWhere \\(\\mathbf{d}_{in}\\) is the \\(12 \\times 1\\) column vector of indegree centralities shown in Table 21.2 (a) and \\(\\mathbf{A}'\\) is the transpose of the network’s adjacency matrix shown in Table 21.1. The results are shown in Table 21.2 (d). As we can see, considering only endogenous network information gives us a completely different picture of the status order than using exogeneous information. Now \\(E\\) is definitely the highest status node, and \\(F\\) which was the highest status node based on exogenous considerations drops to fourth place, behind \\(B\\) and \\(J\\).\nLooking at Figure 21.1, we can see why this happened. Take the set of \\(F\\)’s in-neighbors \\(\\{C, D, H, J, L\\}\\). It is easy to see from Table 21.2 (a), that most of these nodes also have low indegree centrality (except for \\(J\\)). So even though \\(F\\) has five nodes pointing toward them, all of them are not very high-status people. By comparison \\(E\\) only has three in-neighbors \\(\\{B, F, I\\}\\), and all three are towards the top in terms of in-degree centrality. \\(E\\) has higher status than \\(F\\) according to \\(s^{en}\\) because the people that choose \\(E\\) are also chosen by many others, which is exactly what we want in a status measure.\nWhile \\(s^{en}\\) seems like a good measure of status, it does have one big drawback. It only counts direct connections. However, it is possible that you get status not just from the nodes that point directly toward you, but from the nodes that point to those other people even if they don’t point toward you (e.g., two step connections), and perhaps from the nodes that point to those two-step alters, and the ones that point to those three-steps away, and so forth. A good status measure should be able to take into account the status of your indirect connections in computing your own staus score. How do we do this?"
  },
  {
    "objectID": "lesson-status.html#a-mathy-interlude",
    "href": "lesson-status.html#a-mathy-interlude",
    "title": "21  Status",
    "section": "21.4 A Mathy Interlude",
    "text": "21.4 A Mathy Interlude\nConsider any number \\(x\\), where \\(x < |1|\\) (remember that \\(|a|\\) means “the absolute value of \\(a\\)), and thus \\(-1 > x < 1\\) (this reads”\\(x\\) is between -1 and +1”). Thus, \\(x\\) can be 0.43, or -0.62, or whatever in that interval. Recall that when we take a number in this interval and we raise it to a power, we end up with a smaller number than we begin with. The bigger the power, the smaller the result. For instance, take \\(x = 0.75\\). For instance:\n\\[\n  x^2 = 0.75^2 = 0.562\n\\] \\[\n  x^5 = 0.75^5 = 0.237\n\\] \\[\n  x^{10} = 0.75^{10} = 0.056\n\\] Because mathematicians are strange people, they like to say things like, “since the result gets closer to zero the bigger the power, then that means that when I raise the number to an infinite power, then the result should approach zero.” In equation terms:\n\\[\nx^{\\infty} = 0.75^{\\infty} \\approx 0\n\\] Since raising a number between -1 and 1 to a big power gets you closer to zero the bigger the power, mathematicians then go on to wonder whether adding up the powers, gets to the point where the sum does not grow anymore. For instance, what is the end point of:\n\\[\n  1 + x + x^2 + x^3 + x^4 + x^5 \\ldots + x^{\\infty}\n\\tag{21.3}\\]\nThe idea is that as we move to the right and add a number between -1 and 1 raised to a bigger and bigger power, we add a smaller and smaller number, such that as we approach infinity, we end up adding such an infinitesimally small number that it might as well be zero. For instance, table Table 21.3 shows the result of raising \\(x\\) to the powers between 2 and 20 for \\(x = 0.75\\).\n\n\n\n\n\n \n  \n    Power \n    Result \n  \n \n\n  \n    2 \n    0.562 \n  \n  \n    3 \n    0.422 \n  \n  \n    4 \n    0.316 \n  \n  \n    5 \n    0.237 \n  \n  \n    6 \n    0.178 \n  \n  \n    7 \n    0.133 \n  \n  \n    8 \n    0.100 \n  \n  \n    9 \n    0.075 \n  \n  \n    10 \n    0.056 \n  \n  \n    11 \n    0.042 \n  \n  \n    12 \n    0.032 \n  \n  \n    13 \n    0.024 \n  \n  \n    14 \n    0.018 \n  \n  \n    15 \n    0.013 \n  \n  \n    16 \n    0.010 \n  \n  \n    17 \n    0.008 \n  \n  \n    18 \n    0.006 \n  \n  \n    19 \n    0.004 \n  \n  \n    20 \n    0.003 \n  \n\n\n\nTable 21.3:  Incresing powers of the number 0.75. \n\n\nNow let us sum \\(1 + 0.75\\) and add the result to the sum of all the numbers in the third column of Table 21.3, to get an approximation to the sum shown in Equation 21.3. The result is 3.99.\nIn turns out, by some bit of mathematical magic, that this number is pretty close to: \\[\n  (1-0.75)^{-1} = \\frac{1}{1 - 0.75} = 4\n\\]\nAs we noted, as the power that we raise the number to approaches \\(\\infty\\), we will be adding a number that is closer to zero, so the result of the infinity sum when \\(x = 0.75\\) will converge towards:\n\\[  \n  1 + x + x^2 + x^3 + x^4 + x^5 \\ldots + x^{\\infty} =\n\\] \\[\n  1 + 0.75 + 0.75^2 + 0.75^3 + 0.75^4 + 0.75^5 \\ldots + 0.75^{\\infty} \\approx 4\n\\] In general terms, for any number \\(x\\) the sum of the following infinite series converges to:\n\\[\n  1 + x + x^2 + x^3 + x^4 + x^5 +...x^\\infty =\n\\] \\[\n(1-x)^{-1} = \\frac{1}{1 - x}\n\\tag{21.4}\\]\nWhy is this bit of math important? We will see next!"
  },
  {
    "objectID": "lesson-status.html#katz-status-score",
    "href": "lesson-status.html#katz-status-score",
    "title": "21  Status",
    "section": "21.5 Katz Status Score",
    "text": "21.5 Katz Status Score\nIn the mid-twentieth century, the great statistician Leo Katz set out to develop a measure of status in social networks that took into account indirect connections. He first observed that the new matrix that results from taking the original adjacency matrix and raising it to a power (using matrix multiplication) has a clear interpretation, as we saw in Chapter 17. For instance, if we raise the adjacency matrix to the third power (\\(\\mathbf{A}^3\\)) the resulting matrix will contain, as cell entries, all the walks of \\(l = 3\\) that have node \\(i\\) as the starting node and node \\(j\\) as the destination node, the same goes for any number \\(\\mathbf{A}^k\\).\nKatz saw this as a way to incorporate indirect connections to construct a measure of status using only endogenous information. The idea would be to say that your total status is the sum of number of other people who choose you (or think you are great, or a great source or advice or whatever). However, among the people that choose you the ones that are chosen by many others should count for more. Those are people who are two-steps away from you. But the same should apply to the people who choose those others (people three-steps away from you). Overall, you should get more status from people who choose the people who choose the people, who choose the people…who choose you and you should get more status from the more people who are most likely to be chosen by those others, across any number of steps.\nTo accomplish this, we need to construct a new matrix \\(A^*\\) that incorporates all this information about people’s one step, two-step, three-step, connections to others, then sum rows of that matrix. The resulting vector (\\(s^{Katz}_i\\)) would contain the desired score for each node \\(i\\).\nOne way to proceed would be:\n\\[\nA^* = A + A^2 + A^3 + A5 + \\ldots A^{\\infty}\n\\tag{21.5}\\]\nThere are a couple of problems here. First this sum keeps getting bigger and bigger and it does not have a natural end point (keeps going forever). This is because it is counting the direct connections (\\(A\\)) as much as the very indirect connections, like \\(A^5\\), or the number of indirect links connecting you to others five steps away.\nWhat we want is a way to count the first-step links the most, and then discount the longer-step links, with the discount getting larger the longer the chain. So that three-steps links count for less than two-steps links but count for more than four step links to others and so forth.\nKatz’s great idea is to multiply the original adjacency matrix and its powers by a number \\(\\alpha\\) that was larger than zero, but less than one. This leads to:\n\\[\nA^* = \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty}\n\\tag{21.6}\\]\nNow the difference between Equation 21.5 and Equation 21.6, is that as we saw before (see Equation 21.4), while the sum in Equation 21.5 keeps getting bigger and bigger (the technical term is “diverges”), the one in Equation 21.6, will stop growing, because raising a number less than one and more than zero to a big power will result in a tiny number. The sum will converge rather than diverge.\nMoreover, Katz knew his math, and noted that there is a version of {Equation 21.4} that applies to matrices. This is:\n\\[\nA^* = \\alpha A(I + \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty})\n\\]\n\\[\nA^* = \\alpha A(I-A)^{-1}\n\\tag{21.7}\\]\nWhere \\(I\\) refers to the “identity matrix.” This is simply a matrix with the same dimensions as \\(A\\), but containing ones along the diagonal and zeros everywhere. It functions just like the number “\\(1\\)” does in regular (scalar) multiplication. Thus, for an adjacency matrix \\(A\\) and its respective identity matrix \\(I\\) of the same dimensions:\n\\[\nA \\times I = A\n\\] \\[\nI \\times A = A\n\\] \\[\nA \\times A^{-1} = I\n\\] \\[\nA^{-1} \\times A = I\n\\]\nWhat Katz (1953) proposed is that we can turn the infinite sum part of Equation 21.7 (\\(I + \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty})\\) into just \\((I-A)^{-1}\\) following the principle outlined earlier in Equation 21.4. Just like the endless sum of squares of a number \\(x\\) between \\(-1\\) and and \\(1\\) just turns into just \\(\\frac{1}{1-x}\\), the endless sums of a matrix containing numbers between \\(-1\\) and \\(1\\) as its entries \\(\\alpha A\\) turns into \\((I-\\alpha A)^{-1}\\), with \\(I\\) playing the role of \\(1\\) and raising \\(I-\\alpha A\\) to the power of \\(-1\\) playing the role of taking the reciprocal.1\nKatz showed that the new matrix, \\(A^* = \\alpha A(I-\\alpha A)^{-1}\\) contains all the information we need, as it condenses the sums of all the status that a persons gets from all their connections both direct and indirect, regardless of how indirect, and it weighs each person’s contribution to each other’s persons status by the status of those people (which is calculated in the same way). Math magic to the rescue!\nLet’s see how it works, step by step:\n\nFirst we create the \\(12 \\times 12\\) identity matrix \\(I_{12 \\times 12}\\), show in Table 21.4 (a). As noted, this matrix has twelve ones across the diagonals and zeroes everywhere else. Then we choose a value for alpha. There are obscure mathematical reasons for why this value cannot be too big (depending on \\(A\\)), but for this example \\(\\alpha = 0.45\\) will work.\nSecond, we multiply \\(\\alpha\\) times the original adjacency matrix (shown in Figure 21.1) to get \\(\\alpha A\\). This new matrix is shown in Table 21.4 (b). In the new \\(\\alpha A\\) matrix, for every cell in which there is one in \\(A\\), the value 0.45 now appears in \\(\\alpha A\\).\nThird, we subtract \\(I\\) from \\(\\alpha A\\) , to get \\(I-\\alpha A\\). This new matrix is shown in Table 21.4 (c). Note that what this does is to add ones to the diagonals of \\(\\alpha A\\) and change all the other non-zero entries from positive to negative.\nFourth, we find the matrix that equals the reciprocal of \\(I-\\alpha A\\) (also called the matrix inverse of \\(I-\\alpha A\\)) to get \\((I-\\alpha A)^{-1}\\). The matrix inverse is somewhat involved to calculate for larger matrices like \\(I-\\alpha A\\), so, for now, chalk the numbers in Table 21.4 (d) up to math magic. Essentially you are trying to find a new matrix \\(W\\) such that when you multiply it by \\((I-\\alpha A)\\) you get \\(I\\) as the result.\nFifth we multiply \\(\\alpha A\\) (shown in Table 21.4 (b)) times the new matrix \\((I-\\alpha A)^{-1}\\) (shown in Table 21.4 (d)) to get the answer to \\(\\alpha A(I-\\alpha A)^{-1}\\). This new matrix, called the Katz status similarity matrix is shown in Table 21.4 (e).2 In this matrix, the larger the number in the cell, the more node \\(i\\) is connected to node \\(j\\) via indirect connections.\nFinally, we compute the column sums of the Katz status similarity matrix. In equation form:\n\n\\[\ns^{katz}_i = \\sum_jA^*_{ij}\n\\tag{21.8}\\]\nWith \\(A*\\) computed using Equation 21.7. The resulting scores are shown in Table 21.2 (e) for each node of Figure 21.1.\nAs we can see, according to the Katz’s status score, node \\(E\\) is still the highest status node in the network. They are followed by nodes \\(B\\) and \\(J\\), closely agreeing with the endogenous status scores obtained using the in-degree (Table 21.2 (d)). This makes sense, since the Katz scores can be seen as a generalization of the endogenous degree measure, with the latter taken into account only the first step links, and Katz’s taking into account all the indirect links regardless of lengths (but counting the really long ones very little, and counting the first step ones the most). In this way, the Katz approach is the most comprehensive way to compute the status of nodes using only endogenous network information.\n\n\nTable 21.4: Example of estimating status in social networks using exogeneous and endogeneous information for nodes.\n\n\n\n\n(a) Twelve by twelve identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Adjacency matrix multiplied by alpha \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0.00 \n    0.45 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    B \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n  \n  \n    E \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    F \n    0.00 \n    0.45 \n    0 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    H \n    0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    J \n    0.45 \n    0.45 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    K \n    0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    L \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n  \n\n\n\n\n\n\n\n\n(c) Adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1.00 \n    -0.45 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    B \n    0.00 \n    1.00 \n    0 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    1 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n  \n  \n    E \n    0.00 \n    0.00 \n    0 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    F \n    0.00 \n    -0.45 \n    0 \n    -0.45 \n    -0.45 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    H \n    -0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    J \n    -0.45 \n    -0.45 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n  \n  \n    K \n    -0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    1.00 \n    0.00 \n  \n  \n    L \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    -0.45 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(d) Inverse of adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2.0 \n    2.5 \n    0 \n    0.6 \n    2.4 \n    1.4 \n    1.1 \n    0.1 \n    1.5 \n    2.1 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    1.8 \n    0 \n    0.2 \n    1.3 \n    0.5 \n    0.8 \n    0.0 \n    0.6 \n    0.8 \n    0.0 \n    0.1 \n  \n  \n    C \n    1.6 \n    3.1 \n    1 \n    1.2 \n    3.5 \n    2.8 \n    1.4 \n    0.7 \n    1.9 \n    2.6 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.4 \n    2.8 \n    0 \n    2.2 \n    3.0 \n    2.6 \n    1.2 \n    0.4 \n    1.4 \n    2.2 \n    0.4 \n    1.0 \n  \n  \n    E \n    0.7 \n    1.4 \n    0 \n    0.4 \n    2.3 \n    1.0 \n    0.6 \n    0.1 \n    0.6 \n    1.4 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.1 \n    2.7 \n    0 \n    1.3 \n    3.0 \n    2.8 \n    1.2 \n    0.3 \n    1.2 \n    2.0 \n    0.3 \n    0.6 \n  \n  \n    G \n    0.1 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    1.1 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.1 \n    3.7 \n    0 \n    1.3 \n    3.8 \n    2.9 \n    1.7 \n    1.3 \n    1.8 \n    3.2 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.3 \n    0.6 \n    0 \n    0.2 \n    1.1 \n    0.4 \n    0.3 \n    0.0 \n    1.3 \n    0.6 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.6 \n    3.1 \n    0 \n    1.0 \n    3.0 \n    2.1 \n    1.4 \n    0.2 \n    1.4 \n    3.2 \n    0.2 \n    0.4 \n  \n  \n    K \n    1.1 \n    1.4 \n    0 \n    0.4 \n    1.6 \n    0.8 \n    0.6 \n    0.1 \n    1.2 \n    1.2 \n    1.1 \n    0.2 \n  \n  \n    L \n    1.9 \n    3.5 \n    0 \n    1.3 \n    3.7 \n    2.9 \n    1.6 \n    0.7 \n    1.9 \n    2.9 \n    0.7 \n    1.6 \n  \n\n\n\n\n\n\n\n\n(e) Katz similarity matrix (original adjacency matrix multiplied by alpha times the inverse of the adjacency matrix multiplied by alpha subtracted from the identity matrix) \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0.0 \n    2.5 \n    0 \n    0.6 \n    2.4 \n    1.4 \n    1.1 \n    0.1 \n    1.5 \n    2.1 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    0.0 \n    0 \n    0.2 \n    1.3 \n    0.5 \n    0.8 \n    0.0 \n    0.5 \n    0.8 \n    0.0 \n    0.1 \n  \n  \n    C \n    1.6 \n    3.1 \n    0 \n    1.3 \n    3.6 \n    2.7 \n    1.4 \n    0.7 \n    1.9 \n    2.6 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.4 \n    2.8 \n    0 \n    0.0 \n    3.0 \n    2.6 \n    1.3 \n    0.4 \n    1.4 \n    2.2 \n    0.4 \n    1.0 \n  \n  \n    E \n    0.7 \n    1.4 \n    0 \n    0.4 \n    0.0 \n    0.9 \n    0.6 \n    0.1 \n    0.6 \n    1.4 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.1 \n    2.7 \n    0 \n    1.3 \n    3.0 \n    0.0 \n    1.2 \n    0.2 \n    1.2 \n    2.0 \n    0.2 \n    0.6 \n  \n  \n    G \n    0.1 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    0.0 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.1 \n    3.7 \n    0 \n    1.3 \n    3.8 \n    2.8 \n    1.7 \n    0.0 \n    1.8 \n    3.3 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.3 \n    0.6 \n    0 \n    0.2 \n    1.0 \n    0.4 \n    0.3 \n    0.0 \n    0.0 \n    0.6 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.6 \n    3.2 \n    0 \n    0.9 \n    3.0 \n    2.1 \n    1.4 \n    0.2 \n    1.5 \n    0.0 \n    0.2 \n    0.5 \n  \n  \n    K \n    1.0 \n    1.4 \n    0 \n    0.4 \n    1.6 \n    0.8 \n    0.6 \n    0.0 \n    1.3 \n    1.2 \n    0.0 \n    0.2 \n  \n  \n    L \n    1.9 \n    3.5 \n    0 \n    1.4 \n    3.8 \n    2.9 \n    1.6 \n    0.8 \n    1.9 \n    2.9 \n    0.8 \n    0.0"
  },
  {
    "objectID": "lesson-status.html#hubbells-tweak-on-katzs-score",
    "href": "lesson-status.html#hubbells-tweak-on-katzs-score",
    "title": "21  Status",
    "section": "21.6 Hubbell’s Tweak on Katz’s Score",
    "text": "21.6 Hubbell’s Tweak on Katz’s Score\nSo far, we have discussed two main ways to measure the status of node in a social network, both based on the similar principle that people gain status from being (directly or indirectly) connected to high-status others (and get less status from being directly or indirectly connected to low status others). There are two ways to get a sense of the status of others. On the exogeneous approach, we use some kind of prior ranking or knowledge (see Table 21.2 (b)) on the endogenous approach, we use only information on network connectivity (in-degree or the Katz approach). What if there was a way to combine both approaches and get the best of both worlds?\nThis is exactly what was proposed by Hubbell (1965). It revolves around a relatively small tweak on Katz’s approach. The trick is to take the part of Equation 21.7 that computes the endogenous status based on all indirect links to others (\\((I-\\alpha A)^{-1}\\)) and multiply not by \\(\\alpha A\\), but by the external vector of status \\(b^T\\).\nIn equation form:\n\\[\ns^{hubbell} = A'^*\\mathbf{b}\n\\tag{21.9}\\]\nWhere \\(\\mathbf{b}\\) is the column vector containing the exogenous status information shown in Table 21.2 (b), and \\(A'^*\\). Is the transpose of the matrix \\(A^*\\) which is given by:\n\\[\nA^* = (I - \\alpha A)^{-1}\n\\] The resulting Hubbell status scores are shown in Table 21.2 (f) for each node in Figure 21.1. As we can see, incorporating both endogenous and exogenous status information changes the picture, creating more separation between high and low status nodes.\nNow, node \\(E\\) is the indisputable highest status node in the network, followed, at a distant second and third place, by nodes \\(B\\) and \\(F\\). Combining both endogenous and exogenous sources of information does reveal a deeper status inequalities in social networks."
  },
  {
    "objectID": "lesson-status.html#references",
    "href": "lesson-status.html#references",
    "title": "21  Status",
    "section": "References",
    "text": "References\n\n\n\n\nHubbell, Charles H. 1965. “An Input-Output Approach to Clique Identification.” Sociometry, 377–99.\n\n\nKatz, Leo. 1953. “A New Status Index Derived from Sociometric Analysis.” Psychometrika 18 (1): 39–43.\n\n\nVigna, Sebastiano. 2016. “Spectral Ranking.” Network Science 4 (4): 433–45."
  },
  {
    "objectID": "lesson-groups-cliques.html#categories-everywhere",
    "href": "lesson-groups-cliques.html#categories-everywhere",
    "title": "22  Clique Analysis",
    "section": "22.1 Categories Everywhere",
    "text": "22.1 Categories Everywhere\nA common feature of social life is to divide people into categories. For instance, there are ethnoracial categories (like Black, Asian, Hispanic, Pacific Islander), there are ethnonational categories (Philipino, Puerto Rican, Taiwanese, Nigerian, etc.), ethnolinguistic categories (based on the language(s) you speak), and ethnoreligious categories (based on the religion you practice). People also divide themselves into gender identity or sexual orientation categories. Sociologists like to divide up people based on things like categories based on such markers of “social position” like occupation, education, and social class. In the news, sometimes you hear about battles between “Millennials” (who kill everything) and out of touch “Boomers” (who hate everything), with sarcastic “Gen-Exers” in between. This is nothing but a division of people into categories based on age and generation (sometimes also referred to as cohort). The possibilities are endless!\nWhy are sociologists (and people) so obsessed with dividing up people into categories? Well, the basic idea is that category labels provide a mechanism to explain group formation, and they allow for sociologists to get a sense of the position people occupy in society. For instance, following the principle of homophily discussed in the lesson of ego networks, we may surmise that race, gender, and generation serve as principles of group formation (so that groups tend to be homogeneous with respect to age, gender, and generation). We may also say that a stockbroker who makes 500K a year and lives in Malibu occupies a very different social position than a high-school teacher who makes 50K a year and lives in the valley."
  },
  {
    "objectID": "lesson-groups-cliques.html#the-anti-categorical-imperative",
    "href": "lesson-groups-cliques.html#the-anti-categorical-imperative",
    "title": "22  Clique Analysis",
    "section": "22.2 The Anti-Categorical Imperative",
    "text": "22.2 The Anti-Categorical Imperative\nSocial network analysis rejects the idea that the best way to divide up people into categories is to use pre-existing labels. This penchant has been labeled (riffing on the philosopher Immanuel Kant) the anti-categorical imperative (Emirbayer and Goodwin 1994). It is not that social network analysts necessarily reject the idea that people can be divided up into groups or that we should try to get a sense of the social position that people occupy. Instead, what network analysts insist on is that we should use the pattern of interconnections in social networks to come up with groups and assign people to social positions.\nAt its simplest, then, finding groups and assigning nodes to social position boil down to the same thing, which is to take the initial set of nodes in the graph and divide them up into (some times mutually exclusive, sometimes overlapping) subsets, so that nodes in the same subset belong to the same group or occupy the same social position in the network.\nIn network terms, then, what is the difference between being in the same group and occupying the same social position? Just like with the traditional sociological conceptions, the key difference is that belonging to the same group logically implies that you share a lot of connections with the other people in your group (e.g., a group of friends is necessarily linked to one another). However, being in the same social position does not require that you are connected to others in your same position. For instance, you can be doctor in New York City and share the same social position with a doctor in Los Angeles without necessarily being connected to that person. So in network analysis, nodes in a network who share the same position have similar patterns of connectivity with others without necessarily being connected to one another (although they could be!).\nSo the group and the social position imagery leads to two distinct ways of partitioning (a fancy word for “splitting”) the nodes in a graph (representing a social network) into subsets. One based on whether the subsets are strongly interconnected among themselves (the group approach) or whether the subsets have similar patterns of connectivity with others (the position approach). While the group approach is global (it uses information from the connectivity structure of the whole graph), the position approach is local (it uses information from the local neighborhoods of each node to assign them to categories). They are many versions of the group approach, but only two main versions of the position approach. In this lesson, we will cover the main varieties of the group approach. The next two lessons deals with varieties of the position approach."
  },
  {
    "objectID": "lesson-groups-cliques.html#the-group-approach",
    "href": "lesson-groups-cliques.html#the-group-approach",
    "title": "22  Clique Analysis",
    "section": "22.3 The Group Approach",
    "text": "22.3 The Group Approach\nThe basic idea behind the group approach is to find densely connected subgraphs in the original graph. Densely connected subgraphs are the social network equivalent of “groups” in the real world (Freeman 1992). Just like in the real world, people can belong to more than one group at a time.\nThe most densely connected subgraph in a graph is a subgraph with density equal to one. This is a complete subgraph of the larger graph. A complete subgraph of a larger graph is called a clique (Luce and Perry 1949). The identity of a clique is given by the nodes that are inside the clique. So when say that the set of nodes \\(\\{A, B, C, D\\}\\) is a clique of size four with nodes A, B, C, and D inside of it. For instance, Figure 22.1 shows a clique of size four with nodes \\(\\{A, B, C, D\\}\\) as members.\n\n\n\n\n\nFigure 22.1: A clique of size four.\n\n\n\n\nCliques come in different sizes. A clique of size four is a maximally complete subgraph with four nodes in it. A complete subgraph of order five would yield a clique of size five and so forth. A subgraph is maximal for a given property (like being complete) if adding one more node to the subgraph gets rid of the property. For instance, a maximally complete subgraph of order four (a clique of size four) means that adding on more node to the subgraph would make its density drop below one, and thus the subgraph of order five is no longer complete.\nNote that, technically, the closed triad we considered in the lesson on dyads and triads is a clique of size three!\n\n\n\n\n\nFigure 22.2: An undirected graph with four cliques of size 4.\n\n\n\n\nFigure 22.2 shows a graph containing four separate cliques of size four, with nodes who belong to these four cliques highlighted in different colors. Nodes \\(\\{A, B, C, D\\}\\) belong to one clique, nodes \\(\\{E, F, G, H\\}\\), to another, nodes \\(\\{I, J, K, L\\}\\) to yet another clique, and nodes \\(\\{M, N, O, P\\}\\) to a final clique of size four.\nNodes can belong to multiple cliques at once. This is for two reasons:\n\nFirst, cliques of smaller size are nested within cliques of larger size. So inside a clique of size four like those formed by nodes \\(\\{A, B, C, D\\}\\) in Figure 22.2), there are four separate cliques of size three: \\(\\{A, B, C\\}\\), \\(\\{B, C, D\\}\\), \\(\\{A, B, D\\}\\), and \\(\\{A, C, D\\}\\). So that means that each node in a clique of size four technically belongs four distinct cliques! The larger clique of size four and the three smaller cliques of size three.\nSecond, nodes can be in multiple cliques is that the same node can be shared by multiple cliques even if the cliques are not nested. This situation is shown in Figure 22.3 where node D is shared by cliques \\(\\{A, B, C, D\\}\\) and \\(\\{D, E, F, G, H\\}\\). Using the set theory lingo we resorted to talk about node neighborhoods, another way of saying this is that the nodes shared by two cliques is given by the intersection of their set of members. So in the example shown in Figure 22.3:\n\n\\[\n\\{D, E, F, G, H\\} \\cap \\{A, B, C, D\\} = D\n\\]\n\n\n\n\n\nFigure 22.3: An undirected graph with two cliques, one of size four and the other one of size 5.\n\n\n\n\nWhen studying a social network, we may be interested in finding how many cliques are inside of it. This means enumerating the full set of cliques above a certain minimum clique size. For instance, the total number of cliques of size four or larger in the graph shown in Figure 22.4 goes as follows:\n\n\\(\\{D, E, F, G, H\\}\\)\n\\(\\{D, E, G, H\\}\\)\n\\(\\{I, J, K, L\\}\\)\n\\(\\{A, B, C, D\\}\\)\n\\(\\{E, F, G, H\\}\\)\n\\(\\{D, F, G, H\\}\\)\n\\(\\{D, E, F, G\\}\\)\n\\(\\{D, E, F, H\\}\\)\n\nSo there are eight cliques of size four or larger in the graph!\n\n\n\n\n\nFigure 22.4: An undirected graph. How many cliques of size four or larger are there?"
  },
  {
    "objectID": "lesson-groups-cliques.html#cliques-as-affliation-networks",
    "href": "lesson-groups-cliques.html#cliques-as-affliation-networks",
    "title": "22  Clique Analysis",
    "section": "22.4 Cliques as Affliation Networks",
    "text": "22.4 Cliques as Affliation Networks\nThere is a connection between cliques and affiliation networks. Recall from the lesson affiliation networks that we use these types of networks, represented using bipartite graphs whenever we want to highlight the linkages between people and groups. Well, cliques are groups, which means that the network formed by arranging people by their membership in cliques is an affiliation network!\n\n\n\n\n\n \n  \n     \n    C1 \n    C2 \n    C3 \n    C4 \n    C5 \n    C6 \n    C7 \n    C8 \n  \n \n\n  \n    A \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    E \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n  \n  \n    G \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    H \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n  \n  \n    I \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\nTable 22.1:  Person by clique affiliation matrix. \n\n\n\n\n\n\n\nFigure 22.5: Bipartite graph of clique affliations.\n\n\n\n\nTable 22.1 shows the Clique Affiliation Matrix (\\(C\\)) with people in the rows (i) and all the cliques of size four or larger in Figure 22.4 we listed previously (j) in the columns. Each cell entry \\(C_{ij}\\) in the clique affiliation matrix is set to one if node i belongs to clique j. Otherwise it is set to zero.\nAs we can see, some nodes (like node A) belong to only one clique, but other nodes, like node D belongs to six cliques! This is consistent with the idea that while some people belong to just a few groups, other over-committed people belong to multiple groups. Figure 22.5) shows the corresponding bipartite graph displaying the clique affiliations in the network. People are shown as circles and cliques are shown as triangles.\n\n\n\n\n\n \n  \n     \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    1 \n    1 \n    6 \n    4 \n    4 \n    4 \n    4 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    4 \n    5 \n    4 \n    4 \n    4 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    4 \n    4 \n    5 \n    4 \n    4 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    4 \n    4 \n    4 \n    5 \n    4 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    4 \n    4 \n    4 \n    4 \n    5 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\nTable 22.2:  Clique co-membership matrix. \n\n\nJust like with regular affiliation networks, we can compute the matrix multiplication product of the clique affiliation matrix \\(C\\) times its transpose \\(C'\\). This gives us the clique co-membership matrix (\\(M\\)).\n\\[\n  M = C \\times C'\n\\tag{22.1}\\]\nThe off-diagonal cells of the clique co-membership matrix tell us the number of common cliques nodes i and j share, and the diagonal cells tell us the number of clique that each node belongs to. This is shown in Table 22.2).\n\n\n\n\n\n \n  \n     \n    C1 \n    C2 \n    C3 \n    C4 \n    C5 \n    C6 \n    C7 \n    C8 \n  \n \n\n  \n    C1 \n    5 \n    4 \n    0 \n    1 \n    4 \n    4 \n    4 \n    4 \n  \n  \n    C2 \n    4 \n    4 \n    0 \n    1 \n    3 \n    3 \n    3 \n    3 \n  \n  \n    C3 \n    0 \n    0 \n    4 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C4 \n    1 \n    1 \n    0 \n    4 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    C5 \n    4 \n    3 \n    0 \n    0 \n    4 \n    3 \n    3 \n    3 \n  \n  \n    C6 \n    4 \n    3 \n    0 \n    1 \n    3 \n    4 \n    3 \n    3 \n  \n  \n    C7 \n    4 \n    3 \n    0 \n    1 \n    3 \n    3 \n    4 \n    3 \n  \n  \n    C8 \n    4 \n    3 \n    0 \n    1 \n    3 \n    3 \n    3 \n    4 \n  \n\n\n\nTable 22.3:  Clique overlap matrix. \n\n\nWe can compute the matrix multiplication product of the transpose of the clique affiliation matrix \\(C'\\) times the original matrix \\(C\\). This gives us the clique overlap matrix (\\(O\\)).\n\n\n\n\n\nFigure 22.6: Clique graph.\n\n\n\n\n\\[\nO = C' \\times C\n\\tag{22.2}\\]\nThe off-diagonal cells of the clique overlap matrix tell us the number of common members cliques i and j share, and the diagonal cells tell us size of each clique. This is shown in Table 22.3). Note that clique three is an isolate in the clique graph, as it shares no members with other cliques.\nFrom this information we can construct what Everett and Borgatti (1998) call the Clique Graph. This is a weighted graph with cliques as the nodes and the edges weighted by the number of people shared by each clique. The clique graph corresponding to the clique affiliation matrix in Table 22.1) is shown in Figure 22.6)."
  },
  {
    "objectID": "lesson-groups-cliques.html#n-cliques",
    "href": "lesson-groups-cliques.html#n-cliques",
    "title": "22  Clique Analysis",
    "section": "22.5 N-Cliques",
    "text": "22.5 N-Cliques\nConsider the network represented by the graph shown in Figure 22.7). In this figure, it is clear that the set of nodes \\(\\{E, F, G, H\\}\\) are members of a clique of size four. But what about the set of nodes \\(\\{A, B, C, D\\}\\)? I mean, they do look “groupy” but they technically do not meet the requirements of a sociometric clique. The reason is that the subgraph formed by nodes \\(\\{A, B, C, D\\}\\) is not complete (it is missing the edge \\(AC\\)) and thus has a density below one. However, we still have a strong intuition that there are pretty close to being a group.\n\n\n\n\n\nFigure 22.7: A graph featuring n-cliques.\n\n\n\n\nThe mathematician Duncan Luce Luce (1950) had the same intuition and that’s why he developed the notion of an n-clique. The basic idea is simple. Instead of defining a group based on a complete subgraph, we can define a group based on a subgraph with a (minimum) desired level of indirect connectivity (see the lesson on indirect connectivity), which we denote as n. For instance, we can say that any subset of nodes where every node in the subset is connected to every other node in the subset by a path of length two or smaller (which means a direct connection as an edge is a “path” of length one) form a 2-clique (\\(n = 2\\)).\nNote that the subgraph formed by the nodes \\(\\{A, B, C, D\\}\\) meets this criterion: While node A is not connected to node C, it can reach node C by two paths of length two (either \\(\\{AD, DC\\}\\) or \\(\\{AB, BC\\}\\)). Note that adding any other node to the subgraph (e.g., J, or H) breaks this property and no longer makes it a 2-clique. This means that the subgraph \\(\\{A, B, C, D\\}\\) is maximal for the property of being a 2-clique\nCan you see other subset of nodes in the graph that also form a 2-clique?1\nOf course we can keep on going an make up even weaker and more relaxed definitions of a group based on the idea of indirect connectivity (e.g., \\(n = 3\\), \\(n = 4\\), and so forth). For instance, we can define a 3-clique as any subgraph in which nodes are separated by a minimum of three steps (a path of length three). For instance, the hexagonal “ring” formed by the subgraph containing nodes \\(\\{A, B, G, H, J, L\\}\\) in Figure 22.7) is such a 3-clique. Every actor can reach every other actor in the subgraph via a path of length 3 or smaller. Note that while this set of actors still looks kind of groupy (in a ring around the rosie kind of way) it does not look as much like a group as a 2-clique.\nOf course, it would be silly to keep on going with larger values of n, since the subgraphs so defined would be so loosely connected as to no longer count as groups. That’s why when defining n-cliques, values of \\(n\\) in the range of two or three are the most commonly used."
  },
  {
    "objectID": "lesson-groups-cliques.html#references",
    "href": "lesson-groups-cliques.html#references",
    "title": "22  Clique Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nEmirbayer, Mustafa, and Jeff Goodwin. 1994. “Network Analysis, Culture, and the Problem of Agency.” American Journal of Sociology 99 (6): 1411–54.\n\n\nEverett, Martin G, and Stephen P Borgatti. 1998. “Analyzing Clique Overlap.” Connections 21 (1): 49–61.\n\n\nFreeman, Linton C. 1992. “The Sociological Concept of\" Group\": An Empirical Test of Two Models.” American Journal of Sociology 98 (1): 152–66.\n\n\nLuce, R Duncan. 1950. “Connectivity and Generalized Cliques in Sociometric Group Structure.” Psychometrika 15 (2): 169–90.\n\n\nLuce, R Duncan, and Albert D Perry. 1949. “A Method of Matrix Analysis of Group Structure.” Psychometrika 14 (2): 95–116."
  },
  {
    "objectID": "lesson-positions-struct-equiv.html#the-position-approach",
    "href": "lesson-positions-struct-equiv.html#the-position-approach",
    "title": "23  Equivalence and Similarity",
    "section": "23.1 The Position Approach",
    "text": "23.1 The Position Approach\nThe basic idea behind the position approach to dividing up the nodes in a graph is to come up with a measure of how similar two nodes are in terms of their patterns of connectivity with others. This measure then can be used to partition the nodes into what are called equivalence or similarity classes. Nodes in the same equivalence class are said to occupy the same position in the social structure described by the network.\nThere are two main ways to partition nodes into equivalence classes. The first is based on the idea that two nodes occupy the same position is they have similar patterns of connectivity to the same other nodes in the graph. This is called structural equivalence.\nThe second is based on the idea that two nodes are equivalent if they are connected to people who are themselves equivalent, even if these are not literally the same people. This is called regular equivalence.\nThis lesson will deal mainly with various ways of partitioning the nodes in a network based on structural equivalence (Section 23.2) and its more relaxed cousin, structural similarity."
  },
  {
    "objectID": "lesson-positions-struct-equiv.html#sec-equiv",
    "href": "lesson-positions-struct-equiv.html#sec-equiv",
    "title": "23  Equivalence and Similarity",
    "section": "23.2 Structural Equivalence",
    "text": "23.2 Structural Equivalence\nTwo nodes are structurally equivalent if they are connected to the same others. Thus, their patterns of connectivity (e.g., their row in the adjacency matrix) is exactly the same.\n\n\n\n\n\nFigure 23.1: An undirected graph with nodes colored by membership in the same structural equivalence class.\n\n\n\n\nFor instance in Figure 23.1, nodes C and D are structurally equivalent because they are connected to the same neighbors \\(\\{A, B, E\\}\\). In the same way, nodes A and B are structurally equivalent because they are connected to the same neighbors \\(\\{C, D\\}\\). Finally, nodes E and F occupy unique positions in the network because their neighborhoods are not equivalent to that of any other nodes. Node E is the only node that has a neighborhood composed of nodes \\(\\{C, D, F\\}\\), and node F is the only node that has a neighborhood composed of node \\(\\{E\\}\\) only. Perhaps F is the main boss, and E is the second in command.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    D \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 23.1:  Adjancency Matrix of an Undirected Graph \n\n\nWe can also see by looking at Table 23.1) that, indeed, the rows corresponding to the structurally equivalent nodes \\(\\{A, B\\}\\) and \\(\\{C, D\\}\\) in the corresponding adjacency matrix are indistinguishable from one another. The nodes that have unique positions in the network \\(\\{E, F\\}\\), also have a unique pattern of 0s and 1s across the rows of the adjacency matrix.\n\n23.2.1 Structural Similarity\nIn most real-world applications, the standard definition of structural equivalence is much too strong. In answer to the question of whether two nodes occupy the same position in the network it only allows for a “yes/no” answer. Yes, if their neighborhoods are exactly the same, and “no” if there aren’t.\nWhat we need is a measure of position that allows for “more or less” rather than “yes” and “no.” This is what is called structural similarity (Leicht, Holme, and Newman 2006). Two nodes are structurally similar if they have similar patterns of connectivity with the same others. There are various versions of structural similarity between nodes. Here we will consider some popular ones."
  },
  {
    "objectID": "lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-euclidian-distance",
    "href": "lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-euclidian-distance",
    "title": "23  Equivalence and Similarity",
    "section": "23.3 Measuring Structural Similarity Using the Euclidian Distance",
    "text": "23.3 Measuring Structural Similarity Using the Euclidian Distance\nGiven an adjacency matrix for a graph, how can we find out which nodes are structurally similar without staring at a picture for a long time? A classic way of measuring structural similarity, developed by the sociologist Ronald Burt Burt (1976) is to use the Euclidean Distance between the row vectors corresponding to each node in an adjacency matrix.\nTake for instance Table 23.1. The row vector for node A is \\(a_{(A)j} = (0, 0, 1, 1, 0, 0)\\), and so is the row vector for node B \\(a_{(B)j}\\), because we already know they are structurally equivalent! Remember, the row vector is just the entries in each row corresponding to each node in Table 23.1. So the row vector for node C (\\(a_{(C)j}= (1, 1, 0, 0, 1, 0)\\) and so forth. Here the subscript \\(j\\) refers to each column entry of the adjacency matrix \\((A, B, C...F)\\).\nThe Euclidean Distance between the row row-vectors of two nodes \\(k\\) and \\(l\\) is given by:\n\\[\nd^{Euclid}_{k,l} = \\sqrt{\\sum_j (a_{(k)j}-a_{(l)j})^2}\n\\tag{23.1}\\]\nWhat equation Equation 23.1 says is that we take each corresponding entry of the row vectors, subtract them from one another, square them, sum them, and take the square root of the resulting sum. As noted, structurally equivalent nodes will receive a score of zero, while structurally similar nodes will receive scores that are close to zero. The larger the Euclidean distance between two nodes, the less structurally similar they are.\nSo let’s say we wanted to find out the structural similarity between between nodes A and C in Table 23.1 using the Euclidean distance. We would proceed as follows:\n\nFirst, get the row vector for node A that’s \\(a_{(A)j} = (0, 0, 1, 1, 0, 0)\\), as we saw earlier.\nSecond, get the row vector for node C that’s \\(a_{(C)j}= (1, 1, 0, 0, 1, 0)\\), as we saw earlier.\nThird, line them up, so that you can compute the differences and then square them:\n\n\n\n\n\n\n\n\nA\n0\n0\n1\n1\n0\n0\n\n\nC\n1\n1\n0\n0\n1\n0\n\n\nA - C\n(0 - 1)\n(0 - 1)\n(1 - 0)\n(1 - 0)\n(0 - 1)\n(0 - 0)\n\n\nA - C\n-1\n-1\n1\n1\n-1\n0\n\n\n(A - C)^2\n1\n1\n1\n1\n1\n0\n\n\n\nTable 23.2: Euclidean distance calculation.\n\n\nThe Euclidean distance between A and C is thus the square root of the sum of the numbers in the last row of Table 23.2: \\(\\sqrt{1+1+1+1+1+0} = \\sqrt{5}= 2.2\\).\nAs we noted, for structurally equivalent node pairs (which have identical row vectors), the Euclidean distance should reach its minimum value of zero. We can check that by computing the Euclidean distance of nodes A and B in Table 23.1:\n\n\n\n\n\n\n\nA\n0\n0\n1\n1\n0\n0\n\n\nB\n0\n0\n1\n1\n0\n0\n\n\nA-C\n0\n0\n0\n0\n0\n0\n\n\n(A-C)^2\n0\n0\n0\n0\n0\n0\n\n\n\nTable 23.3: Euclidean distance calculation.\n\n\nIndeed since the sum of the last row of the numbers in Table 23.3 is zero, then so will the square root!\n\n23.3.1 The Structural Similarity Matrix\nTable 23.4 shows the structural similarity matrix (\\(\\mathbf{S}\\)) produced by computing euclidean distance between of each pair of nodes in the adjacency matrix shown in Table 23.1 (corresponding to the graph shown in Figure 23.1) according to Equation 23.1.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n0\n2.2\n2.2\n1\n1.7\n\n\nB\n0\n–\n2.2\n2.2\n1\n1.7\n\n\nC\n2.2\n2.2\n–\n0\n2.4\n1.4\n\n\nD\n2.2\n2.2\n0\n–\n2.4\n1.4\n\n\nE\n1\n1\n2.4\n2.4\n–\n2\n\n\nF\n1.7\n1.7\n1.4\n1.4\n2\n–\n\n\n\nTable 23.4: Structural Equivalence matrix for an undirected graph.\n\n\nIn the \\(\\mathbf{S}\\) matrix, each cell \\(\\mathbf{s}_{ij}\\) gives us the Euclidean distance between nodes i and j. Note that the \\(\\mathbf{S}\\) matrix is symmetric, meaning that the same information is contained in the lower and upper triangles (\\(\\mathbf{s}_{ij}= \\mathbf{s}_{ji}\\)). This makes sense, because the distance between point a and point b should be the same as the distance between point b and point a. In the same way, if you are similar to your friend, then your friend should be equally similar to you!\nChecking the values in Table 23.4, we can see that structurally equivalent pairs of nodes—are connected to exactly the same others, like nodes A and B or nodes C and D—have similarity \\(\\mathbf{d}_{ji}= 0\\) in the matrix based on the Euclidian distnce. As nodes become less similar—are connected to different others like nodes E and C—their Euclidean distance becomes larger.\nSo, we can say that structurally similar nodes occupy the same position in the network. So A and B occupy the same position, and so do C and D.\n\n\n23.3.2 Euclidian Distance in Directed Graphs\nSo far we have considered the case of structural similarity for undirected graphs composed of symmetric ties.\nBut what happens when the network we are studying is composed of asymmetric ties?\nWell, in the directed graph case, we have to distinguish between two ways nodes in a graph can be structurally similar to one another based on the directionality of the ties we are considering.\n\nIn the first case, two nodes are structurally similar if they send ties to the same others.\nIn the second case, two nodes are structurally similar if they receive ties from the same others.\n\nThese don’t necessarily have to go together. One node (A) can send ties to the same others as another node (B) (and thus A and B can be structurally similar when it comes to their out-neighbors), but receive ties from a different set of others (and thus A and B can fail to be structurally similar in terms of their in-neighbors)\nThis means that in the directed graph case, two nodes are structurally equivalent if and only if they send ties to the same others and receive ties from the same others.\nThis added complication means that we have to modify the way we measure structural similarity in the directed graph case when using the euclidian distance. Particularly, to consider the distance between pairs of nodes, we now have to consider both their row-vectors (capturing the pattern of sending ties) and their column vectors (capturing the patterns of their receiving ties) in calculating the distance.\nThis means Equation 23.1 now turns into:\n\\[\n  d^{Euclidean}_{k,l} = \\sqrt{\\sum_j (a_{(k)j}-a_{(l)j})^2 + \\sum_i (a_{(k)i}-a_{(l)i})^2}\n\\tag{23.2}\\]\nThe first part of Equation 23.2 inside the square root operator is just like Equation 23.1: \\(\\sum_j (a_{(k)j}-a_{(l)j})^2\\). In this case, this tracks the Euclidean distance between the respective row vectors of nodes k and l (which is we sum across the columns j), which captures the extent to which they send links to the same others. When this part of the equation equals zero, it means k and l are structurally equivalent when it comes to sending ties.\nThe second part that is added is \\(\\sum_j (a_{(k)i}-a_{(l)i})^2\\), which measures the Euclidean distance between the column vectors of nodes k and l in the asymmetric adjacency matrix (which is why we sum across the rows i); this captures the extent to which k and l receive ties from the same others. When this part of the equation equals zero, it means k and l are structurally equivalent when it comes to receiving ties.\n\n\n\n\n\nFigure 23.2: A directed graph\n\n\n\n\nLet’s look at an example. Consider the graph shown in Figure 23.2. In the graph, nodes rendered in the same color are structurally equivalent according to Equation 23.2. The corresponding asymmetric adjacency matrix for the graph in Figure 23.2 is shown in Table 23.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n1\n1\n1\n0\n1\n0\n\n\nB\n1\n–\n1\n1\n0\n1\n0\n\n\nC\n1\n0\n–\n0\n0\n0\n0\n\n\nD\n1\n1\n0\n–\n1\n0\n1\n\n\nE\n0\n0\n1\n0\n–\n1\n0\n\n\nF\n1\n0\n0\n0\n0\n–\n0\n\n\nG\n0\n0\n1\n0\n0\n1\n–\n\n\n\nTable 23.5: Asymmetric adjacency matrix for a directed graph.\n\n\nLet’s say we wanted to figure out whether nodes E and G are structurally equivalent (they are). First, we would compare their respective row-vectors in the adjacency matrix:\n\n\n\n\nTable 23.6: Row vectors for nodes E and G.\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nE\n0\n0\n1\n0\n–\n1\n0\n\n\nG\n0\n0\n1\n0\n0\n1\n–\n\n\n\n\n\n\nThen we would compare their respective column vectors:\n\n\n\n\nTable 23.7: Column vectors for nodes E and G.\n\n\n\nE\nG\n\n\n\n\nA\n0\n0\n\n\nB\n0\n0\n\n\nC\n0\n0\n\n\nD\n1\n1\n\n\nE\n–\n0\n\n\nF\n0\n0\n\n\nG\n0\n–\n\n\n\n\n\n\nAnd indeed, they are both the same! When we compute the structural similarity matrix based on the Euclidean distance for the directed graph shown in Figure 23.2 using Equation 23.2, we end up with:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n3.4\n4.6\n4.4\n3.1\n4.6\n3.1\n\n\nB\n3.4\n–\n3.7\n3.8\n2.4\n3.7\n2.4\n\n\nC\n4.6\n3.7\n–\n3.1\n3.9\n0\n3.9\n\n\nD\n4.4\n3.8\n3.1\n–\n4.1\n3.1\n4.1\n\n\nE\n3.1\n2.4\n3.9\n4.1\n–\n3.9\n0\n\n\nF\n4.6\n3.7\n0\n3.1\n3.9\n–\n3.9\n\n\nG\n3.1\n2.4\n3.9\n4.1\n0\n3.9\n–\n\n\n\nTable 23.8: Structural Equivalence matrix for an undirected graph.\n\n\nWhich tells us that nodes E and G in Figure 23.2 are structurally equivalent \\(d_{E,G} = 0\\), but so are nodes C and F. These two pair of nodes send ties to the same out-neighbors and receive ties from the same in-neighbors.1 So, we can say that nodes E and G occupy one position in the network and nodes C and F occupy another position. Perhaps if this were an office, and the relation was one of advice, this would reveal two set of actors who have a similar role in the office network."
  },
  {
    "objectID": "lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-neighborhood-overlap",
    "href": "lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-neighborhood-overlap",
    "title": "23  Equivalence and Similarity",
    "section": "23.4 Measuring Structural Similarity Using the Neighborhood Overlap",
    "text": "23.4 Measuring Structural Similarity Using the Neighborhood Overlap\nAs we noted in the original graph theory lesson, it is possible for the neighborhood of two nodes in a graph to overlap. Recall that for each node, we define its neighborhood as the set of other nodes that they are adjacent to. That means the neighborhood between two nodes can have members in common. The more members they have in common the more structurally similar two nodes are.\n\n\n\n\n\nFigure 23.3: An undirected graph.\n\n\n\n\nFor instance, imagine you have a friend and that friend knows all your friends and you know all their friends. In which case we would say that the overlap between your node neighborhoods is pretty high; in fact the two neighborhoods overlap completely, which makes you structurally equivalent! But even if your friend knows 90% of the people in your network (and you know 90% of the people in their network) that would make you very structurally similar to one another.\nNow imagine you just met a new person online who lives in a far away country, and as far as you know, they know none of your friends and you know none of their friends. In which case, we would say that the overlap if the two neighborhoods is nil or as close to zero as it can get. You occupy completely different positions in the network.\n\n23.4.1 Jaccard Similarity\nWe can use this reasoning to construct a measure of structural similarity between two nodes called Jaccard’s Similarity Coefficient (\\(J_{ij}\\)). It goes like this (Jaccard 1901). Let’s say \\(n_{ij}\\) is the number of friends that nodes i and j have in common, and the total number of i’s friends if \\(k_i\\) (i’s degree) and the total number of j’s friends if \\(k_j\\) (j’s degree). Then the structural similarity of i and j is given by:\n\\[\n  J_{ij} = \\frac{n_{ij}}{k_i + k_j - n_{ij}}\n\\tag{23.3}\\]\nIt says that the structural similarity of two nodes is equivalent to the number of friends that the two persons know in common, divided by the sum of their degrees minus the number of people they know in common. Jaccard’s coefficient ranges from zer (when \\(n_{ij}=0\\) and the two nodes have no neighbors in common) to 1.0 (when \\(n_{ij} = k_i\\) and \\(n_{ij} = k_j\\) and the two nodes are structurally equivalent).\n\n\n23.4.2 Dice Similarity\nA second measure of structural similarity between nodes based on the neighborhood overlap is the Dice Similarity Index. It goes like this (Dice 1945):\n\\[\n  D_{ij} = \\frac{2n_{ij}}{k_i + k_j}\n\\tag{23.4}\\]\nWhich says that the structural similarity between two nodes is equivalent to the twice the number of people the know in common, divided by the sum of their degrees.\n\n\n23.4.3 Cosine Similarity\nA third and final measure of structural similarity between two nodes based on the neighborhood overlap is the cosine similarity between their respective neighborhoods (\\(C_{ij}\\)). This is given by:\n\\[\n  C_{ij} = \\frac{n_{ij}}{\\sqrt{k_ik_j}}\n\\tag{23.5}\\]\nWhich says that the structural similarity between two nodes is equivalent to the number of people they know in common divided by the square root of the product of their degrees (which is also referred to as the geometric mean of their degrees).\nA lot of the times, these three measures of structural similarity will tend to agree. Table 23.9), Table 23.10), and Table 23.11) show the similarities between each pair of nodes in the graph depicted in Figure 23.3.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n–\n0.17\n0.17\n0\n0.17\n0\n0.17\n0.25\n0.2\n0.17\n0\n0.25\n\n\nB\n–\n–\n0\n0\n0.33\n0.2\n0.14\n0\n0.4\n0.14\n0.17\n0\n\n\nC\n–\n–\n–\n0.4\n0.33\n0\n0.33\n0\n0\n0\n0.17\n0.2\n\n\nD\n–\n–\n–\n–\n0.17\n0\n0.17\n0\n0\n0\n0.5\n0\n\n\nE\n–\n–\n–\n–\n–\n0\n0\n0\n0.17\n0\n0.17\n0\n\n\nF\n–\n–\n–\n–\n–\n–\n0.2\n0\n0.25\n0.5\n0\n0.33\n\n\nG\n–\n–\n–\n–\n–\n–\n–\n0\n0\n0.14\n0.17\n0.5\n\n\nH\n–\n–\n–\n–\n–\n–\n–\n–\n0.25\n0.5\n0\n0\n\n\nI\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.4\n0\n0\n\n\nJ\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0\n0.2\n\n\nK\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0\n\n\nL\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\n\nTable 23.9: Structural similarity matrix based on Jaccard’s index.\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n–\n0.29\n0.29\n0\n0.29\n0\n0.29\n0.4\n0.33\n0.29\n0\n0.4\n\n\nB\n–\n–\n0\n0\n0.5\n0.33\n0.25\n0\n0.57\n0.25\n0.29\n0\n\n\nC\n–\n–\n–\n0.57\n0.5\n0\n0.5\n0\n0\n0\n0.29\n0.33\n\n\nD\n–\n–\n–\n–\n0.29\n0\n0.29\n0\n0\n0\n0.67\n0\n\n\nE\n–\n–\n–\n–\n–\n0\n0\n0\n0.29\n0\n0.29\n0\n\n\nF\n–\n–\n–\n–\n–\n–\n0.33\n0\n0.4\n0.67\n0\n0.5\n\n\nG\n–\n–\n–\n–\n–\n–\n–\n0\n0\n0.25\n0.29\n0.67\n\n\nH\n–\n–\n–\n–\n–\n–\n–\n–\n0.4\n0.67\n0\n0\n\n\nI\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.57\n0\n0\n\n\nJ\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0\n0.33\n\n\nK\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0\n\n\nL\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\n\nTable 23.10: Structural similarity matrix based on Dice’s index.\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n–\n0.54\n0.15\n0\n0.45\n0\n0.09\n0.21\n0.52\n0.15\n0\n0.21\n\n\nB\n–\n–\n0.22\n0\n0.45\n0.32\n0.52\n0\n0.52\n0.22\n0.26\n0.32\n\n\nC\n–\n–\n–\n0.58\n0.5\n0\n0.29\n0\n0\n0\n0.29\n0.35\n\n\nD\n–\n–\n–\n–\n0.29\n0\n0.17\n0\n0\n0\n0.67\n0\n\n\nE\n–\n–\n–\n–\n–\n0\n0\n0\n0.29\n0\n0.29\n0\n\n\nF\n–\n–\n–\n–\n–\n–\n0.2\n0\n0.41\n0.71\n0\n0.5\n\n\nG\n–\n–\n–\n–\n–\n–\n–\n0\n0\n0.14\n0.5\n0.41\n\n\nH\n–\n–\n–\n–\n–\n–\n–\n–\n0.41\n0.71\n0\n0\n\n\nI\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.58\n0\n0\n\n\nJ\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0\n0.35\n\n\nK\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0\n\n\nL\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\n\nTable 23.11: Structural similarity matrix based on the cosine distance.\n\n\n\n\n23.4.4 Neighborhood Overlap in Directed Graphs\nSimilarity works in similar (pun intended) ways when studying asymmetric ties in directed graph. The main difference, as usual, is that in a directed graph pairs of nodes can structurally similar in two different ways. Fist, pairs of nodes can be similar with respect to their out-neighborhoods, in which case we say that nodes are structural similar if they point to the same set of neighbors. This is called the out-similarity. Second, pairs of nodes can be similar with respect to their in-neighborhoods, in which case we say that nodes are structural similar if they receive ties or nominations from the same set of neighbors. This is called the in-similarity.\nSpecial cases of the out and in-similarities between nodes show up in particular types of networks. For instance, consider an information network composed of scientific papers. Here a directed tie emerges when paper A cites or refers to paper B. This is called a citation network.\nIn a citation network out-similar papers are papers that cite the same other papers. Out-similar papers are said to exhibit bibliographic coupling (essentially the overlap or set intersection between their reference lists). A weighted network of similarities between papers, where the weight of the edge is the number of other papers that that they both cite in common is called a bibliographic coupling network. A bibliographic coupling network is essentially a network of out-similarities between papers in a scientific information network.\nIn a citation network, in-similar papers are papers that get cited by the same set of others. In this case, we say that the two papers are co-cited a third paper. A weighted network of similarities between papers, where the weight of the edge is the number of other papers that cite both of them is called co-citation network. A co-citation network is essentially a network of in-similarities between papers in a scientific information network.\nThe two measures of out and in-similarities can be defined in the same way as before. If \\(n^{out}_{ij}\\) is the number of common out-neighbors of nodes i and j and \\(n^{in}_{ij}\\) is the number of their common out-neighbors, \\(k_{out}\\) is the total number of out-neighbors of a particular node, and \\(k_{in}\\) is the total number of in-neighbors, then the structural out and in-similarities between pairs of nodes i and j are given by (using the cosine distance measure) by:\n\\[\n  C_{ij}^{out} = \\frac{n_{ij}^{out}}{\\sqrt{k_i^{out}k_j^{out}}}\n\\tag{23.6}\\]\n\\[\n  C_{ij}^{in} = \\frac{n_{ij}^{in}}{\\sqrt{k_i^{in}k_j^{in}}}\n\\tag{23.7}\\]"
  },
  {
    "objectID": "lesson-positions-struct-equiv.html#references",
    "href": "lesson-positions-struct-equiv.html#references",
    "title": "23  Equivalence and Similarity",
    "section": "References",
    "text": "References\n\n\n\n\nBurt, Ronald S. 1976. “Positions in Networks.” Social Forces 55 (1): 93–122.\n\n\nDice, Lee R. 1945. “Measures of the Amount of Ecologic Association Between Species.” Ecology 26 (3): 297–302.\n\n\nJaccard, Paul. 1901. “Distribution of the Alpine Flora in the Dranse’s Basin and Some Neighbouring Regions.” Bulletin de La Societe Vaudoise Des Sciences Naturelles 37 (1): 241–72.\n\n\nLeicht, Elizabeth A, Petter Holme, and Mark EJ Newman. 2006. “Vertex Similarity in Networks.” Physical Review E 73 (2): 026120."
  },
  {
    "objectID": "lesson-positions-blockmod.html#the-correlation-distance",
    "href": "lesson-positions-blockmod.html#the-correlation-distance",
    "title": "24  Blockmodeling",
    "section": "24.1 The Correlation Distance",
    "text": "24.1 The Correlation Distance\nIt turns out there is an even fancier way to find out whether two nodes in a graph are structurally similar. It relies on a more complicated measure of distance called the correlation distance. This measure compares the row (or column) vectors of nodes in a graph and returns a number between \\(-1\\) and \\(+1\\). When it comes to structural equivalence, the correlation distance works like this:\n\nPairs of structurally equivalent nodes get a \\(+1\\). Nodes that are almost structurally equivalent but not quite (they are structurally similar) get a positive number larger than zero. The closer that number is to \\(+1\\) the more structurally similar the two nodes are.\nNodes that are completely different from one another (that is connect to completely disjoint sets of neighbors) get a \\(-1\\). In this case, nodes are opposites: Every time one node i has a \\(1\\) in their row vector in the adjacency matrix, the other has a \\(0\\) and vice versa. Nodes that are structurally dissimilar thus get a negative number between zero and \\(-1\\). The closer that number is to \\(-1\\), the more structurally dissimilar the two nodes are.\nNodes that have an even combination of similarities and differences in their pattern of connectivity to others get a number close to zero, with \\(0\\) indicating that two nodes have an even number of commonalities and differences.\n\nThe correlation distance between two nodes k and l is computed using the following formula:\n\\[\n    d^{corr}_{k, l} =\n    \\frac{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j}) \\times\n    (a_{(l)j} - \\overline{a}_{(l)j})\n    }\n    {\n    \\sqrt{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j})^2 \\times\n    \\sum_j\n    (a_{(l)j} - \\overline{a}_{(l)j})^2\n        }\n    }\n\\tag{24.1}\\]\nEquation 24.1 looks like a monstrously complicated one, but it is actually not that involved.\nLet’s go through each components that we have already encountered in the lesson structural equivalence and structural similarity:\n\n\\(a_{(k)j}\\) is the row vector for node k in the adjacency matrix.\n\\(a_{(l)j}\\) is the row vector for node l in the adjacency matrix.\n\nNow let’s introduce ourselves to some new friends. For instance, what the heck is \\(\\overline{a}_{(k)j}\\)? The little “bar” on top the \\(a\\) indicates that we are taking the mean or the average of the elements of the row vector.\nIn equation form:\n\\[\n\\overline{a}_{(k)j} = \\frac{\\sum_j a_{kj}}{N}\n\\tag{24.2}\\]\nIn Equation 24.2, \\(\\sum_i a_{kj}\\) is the sum of all the elements in the vector, and \\(N\\) is the length of the vector, which is equivalent to the order of the graph from which adjacency matrix came from (the number of nodes in the network).\nSo for instance, in Table 23.1, the row vector for node A is:\n\n\n\n\nTable 24.1: Row vector for node A.\n\n\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\nWhich implies:\n\\[\n\\sum_i a_{(A)j} = 0 + 0 + 1 + 1 + 0 + 0 = 2\n\\]\nAnd we know that \\(N = 6\\), so that implies:\n\\[\n\\overline{a}_{(A)j} = \\frac{\\sum_i a_{Aj}}{N} = \\frac{2}{6}=0.33\n\\] The term \\(\\overline{a}_{(k)j}\\) is called the row mean for node k in the adjacency matrix. Just like we can compute row means, we can also compute column means by using the elements of the column vector \\(\\overline{a}_{(k)i}\\)\nNow that we know what the row means are, we can make sense of the term \\((a_{(k)j} - \\overline{a}_{(k)j})\\) in Equation 24.1. This is a vector composed of the differences between the row vector entries in the adjacency matrix and the row mean for that node. So in the case of node A and the row vector in Table 24.1 that would imply:\n\n\nTable 24.2: Row vector of mean differences for node A.\n\n\n\n\n(a) Vector of row entries minus the row mean.\n\n\n\n\n\n\n\n\n\n\n(0 - 0.33)\n(0 - 0.33)\n(1 - 0.33)\n(1 - 0.33)\n(0 - 0.33)\n(0 - 0.33)\n\n\n\n\n\n\n\n\n(b) Result of subtracting row mean from row entries.\n\n\n-0.33\n-0.33\n0.67\n0.67\n-0.33\n-0.33\n\n\n\n\n\n\nWhich implies: \\[\n\\sum_j (a_{(k)j} - \\overline{a}_{(k)j}) = -0.33 -0.33 + 0.67 + 0.67 -0.33 -0.33 = 0.02\n\\]\nThe numerator of Equation 24.1, just says: “Take the entries in the row vector for the first node, and create a new vector composed of the those entries minus the row means and sum the vector. Then do the same for the other node and multiply the two numbers” And in the denominator of the equation we just square the same vectors sum them, multiply each of the two numbers and take the square root of the result product. Once we have the numerator and denominator we can evaluate the fraction and compute the correlation distance between those two nodes.\nWhen we do that for each pair of nodes in Table 23.1, we end up with the structural similarity matrix shown in Table 24.3, based on the correlation distance.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.71\n-0.71\n0.71\n-0.32\n\n\nB\n1\n–\n-0.71\n-0.71\n0.71\n-0.32\n\n\nC\n-0.71\n-0.71\n–\n1\n-1\n0.45\n\n\nD\n-0.71\n-0.71\n1\n–\n-1\n0.45\n\n\nE\n0.71\n0.71\n-1\n-1\n–\n-0.45\n\n\nF\n-0.32\n-0.32\n0.45\n0.45\n-0.45\n–\n\n\n\nTable 24.3: Correlation distance matrix for an undirected graph.\n\n\nIn the Table 24.3, the structurally equivalent pairs of nodes, A and B and C and D have \\(d^{corr} = 1.0\\). Nodes that are completely non-equivalent like C and E and D and E have \\(d^{corr} = -1.0\\). Nodes that are structurally similar, but not equivalent, like nodes C and F (\\(d^{corr} = 0.45\\)) get a positive number that is less than \\(1.0\\)."
  },
  {
    "objectID": "lesson-positions-blockmod.html#iterated-correlational-distances-concor",
    "href": "lesson-positions-blockmod.html#iterated-correlational-distances-concor",
    "title": "24  Blockmodeling",
    "section": "24.2 Iterated Correlational Distances: CONCOR",
    "text": "24.2 Iterated Correlational Distances: CONCOR\nWhat happens if we were to try to use Equation 24.1 to find the correlation distance of a correlation distance matrix? If we were to do this and use Table 24.3 as our input matrix, we end up with Table 24.4 (a).\n\n24.2.1 We Need to go Deepah\nNow, as Leo (when stuck in a dream, about a dream, about a dream…) always says: “We need to go deeper.”1 And, indeed, we can. We can take the correlation distance of the nodes based on Table 24.4 (a). If we do that, we end up with the entries in Table 24.4 (b). If we keep on going, we end up with the entries in Table 24.4 (c). Note that in Table 24.4 (c), there are only two values for all the entries: \\(+1\\) and \\(-1\\)!\n\n\nTable 24.4: Iterated correlational distances.\n\n\n\n\n(a) Second Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.97\n-0.97\n0.97\n-0.84\n\n\nB\n1\n–\n-0.97\n-0.97\n0.97\n-0.84\n\n\nC\n-0.97\n-0.97\n–\n1\n-1\n0.84\n\n\nD\n-0.97\n-0.97\n1\n–\n-1\n0.84\n\n\nE\n0.97\n0.97\n-1\n-1\n–\n-0.84\n\n\nF\n-0.84\n-0.84\n0.84\n0.84\n-0.84\n–\n\n\n\n\n\n\n\n\n(b) Third Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-0.99\n\n\nB\n1\n–\n-1\n-1\n1\n-0.99\n\n\nC\n-1\n-1\n–\n1\n-1\n0.99\n\n\nD\n-1\n-1\n1\n–\n-1\n0.99\n\n\nE\n1\n1\n-1\n-1\n–\n-0.99\n\n\nF\n-0.99\n-0.99\n0.99\n0.99\n-0.99\n–\n\n\n\n\n\n\n\n\n(c) Fourth Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-1\n\n\nB\n1\n–\n-1\n-1\n1\n-1\n\n\nC\n-1\n-1\n–\n1\n-1\n1\n\n\nD\n-1\n-1\n1\n–\n-1\n1\n\n\nE\n1\n1\n-1\n-1\n–\n-1\n\n\nF\n-1\n-1\n1\n1\n-1\n–\n\n\n\n\n\n\nMore importantly, as shown in Table 24.5, the structurally equivalent nodes have exactly the same pattern of \\(+1\\)s and \\(-1\\)s across the rows. This procedure of iterated correlations, invented by a team of sociologists and psychologists at Harvard University in the mid-1970s (Breiger, Boorman, and Arabie 1975), is called CONCOR—and acronym for the hard to remember title of “convergence of iterate correlations’’—and is designed to extract structurally equivalent positions from networks even when the input matrix is just based on structural similarities.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    -- \n    1 \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    B \n    1 \n    -- \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    C \n    -1 \n    -1 \n    -- \n    1 \n    -1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    1 \n    -- \n    -1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    -1 \n    -1 \n    -- \n    -1 \n  \n  \n    F \n    -1 \n    -1 \n    1 \n    1 \n    -1 \n    -- \n  \n\n\n\nTable 24.5:  Structurally Equivalent Positions in an undirected graph."
  },
  {
    "objectID": "lesson-positions-blockmod.html#blockmodeling",
    "href": "lesson-positions-blockmod.html#blockmodeling",
    "title": "24  Blockmodeling",
    "section": "24.3 Blockmodeling",
    "text": "24.3 Blockmodeling\nThe example we considered previously concerns the relatively small network shown in Figure 23.1. What happens when we apply the method of iterated correlations (CONCOR) to a bigger network, something like the one shown in Figure 24.1?\n\n\n\n\n\nFigure 24.1: An undirected graph.\n\n\n\n\nWell, we can begin by computing the correlation distance across all ,the \\(V=22\\) nodes in that network using Equation 24.1. The result of that looks like Table 24.6 (a). Note that even before we do any iterated correlations of correlation matrices we can see that the peripheral, single-connection nodes \\(E, F, G, H\\), \\(I, J, K, L, M\\) and \\(N, O, P, Q, R\\) are perfectly structurally equivalent. This makes sense, because all the nodes in each of these three groups have identical neighborhoods, since they happen to be connected to the same central node \\(A\\) for the first group, \\(B\\) for the second group and \\(C\\) for the third group. Note also that \\(U\\) and \\(V\\) are structurally equivalent, since their neighborhoods are the same: Their single connection is to node \\(S\\).\n\n\nTable 24.6: Correlation Distance Matrices Corresponding to an Undirected Graph.\n\n\n\n\n(a) Original Correlation Distance Matrix. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1.00 \n    -0.15 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    0.05 \n    -0.13 \n    -0.13 \n  \n  \n    B \n    -0.15 \n    1.00 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    C \n    -0.15 \n    -0.15 \n    1.00 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    D \n    -0.24 \n    -0.24 \n    -0.24 \n    1.00 \n    0.34 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    -0.16 \n    -0.09 \n    -0.09 \n  \n  \n    T \n    -0.19 \n    -0.19 \n    -0.19 \n    0.34 \n    1.00 \n    0.69 \n    0.69 \n    0.69 \n    0.69 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.13 \n    0.69 \n    0.69 \n  \n  \n    E \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    F \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    G \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    H \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    I \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    J \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    K \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    L \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    M \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    N \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    O \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    P \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    Q \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    R \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    S \n    0.05 \n    -0.24 \n    -0.24 \n    -0.16 \n    -0.13 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    1.00 \n    -0.09 \n    -0.09 \n  \n  \n    U \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n  \n    V \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(b) Original Correlation Distance Matrix After Ten Iterations. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(c) Correlation Distance Matrix in (a) with Rows and Columns Reshuffled to Show Hidden Pattern. \n \n  \n      \n    V \n    U \n    S \n    H \n    G \n    F \n    E \n    T \n    C \n    A \n    B \n    R \n    Q \n    P \n    O \n    N \n    M \n    L \n    K \n    J \n    D \n    I \n  \n \n\n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWhat happens when we take the correlation distance of the correlation distance matrix shown in Table 24.6 (a), and the correlation distance of the resulting matrix, and keep going until we only have zeros and ones? The results is Table 24.6 (b). This matrix seems to reveal a much deeper pattern of commonalities in structural positions across the nodes in Figure 24.1. In fact, if we were a conspiracy theorist like Charlie from Always Sunny in Philadelphia, we might even surmise that there is a secret pattern that can be revealed if we reshuffled the rows and the columns of the matrix (without changing any of the numbers of course!).2\nIf we do that, we end up with Table 24.6 (c). So it turns out that there is indeed a secret pattern! The reshuffling shows that the nodes in the network can be divided into two blocks such within blocks all nodes are structurally similar (and some structurally equivalent) and across blocks, all nodes are structurally dissimilar. Thus \\(V, U, S, H, G, F, E, T, C, A, B\\) are members of one structurally similar block (let’s called them “Block 1”), and nodes \\(R, Q, P, O, N, M, L, K, J, D, I\\) are members of another structurally similar block (let’s called them “Block 2”). Nodes in “Block 1” are structurally dissimilar from nodes in “Block 2,” but structurally similar to one another and vice versa. To illustrate, Figure 24.2 is the same as Figure 24.1, but this time nodes are colored by their memberships in two separate blocks.\n\n\n\n\n\nFigure 24.2: An undirected graph with block membership indicated by node color.\n\n\n\n\nNote that we haven’t changed any of the information in Table 24.6 (b) to get Table 24.6 (c). If you check, the row and column entries for each node in both figures are identical. It’s just that we changed the way the rows ordered vertically and the way the columns are ordered horizontally. For instance, node \\(A\\)’s pattern of connections is negatively correlated with node \\(I\\)’s in Table 24.6 (b), and has the same negative correlation entry in Table 24.6 (c). The same goes for each one of node \\(A\\)’s other correlations, and the same for each node in the table. Table 24.6 (b) and Table 24.6 (c) contain the same information it’s just that Table 24.6 (c) makes it easier to see a hidden pattern.\nThis property of the method of iterated correlations is the basis of a strategy for uncovering blocks of structurally similar actors in a network developed by a team of sociologists, physicists, and mathematicians working at Harvard in the 1970s. The technique is called blockmodeling (White, Boorman, and Breiger 1976). Let’s see how it works.\n\n24.3.1 We Need to go Deepah\nOf course, as Leo always says: “We need to go deeper.” And indeed we can. What happens if we do the same analysis as above, but this time in the two node-induced subgraphs defined by the set of structurally similar nodes in each of the two blocks we uncovered in the original graph? Then we get Table 24.7 (a) and Table 24.7 (b).\n\n\nTable 24.7: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    B \n    A \n    C \n    S \n    V \n    U \n    T \n    E \n    F \n    H \n    G \n  \n \n\n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    I \n    J \n    K \n    M \n    L \n    D \n    N \n    O \n    P \n    R \n    Q \n  \n \n\n  \n    I \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWe can see that Table 24.6 (a) separates our original Block 1 into two further sub-blocks. Let’s call them “Block 1a” and “Block 1b.” Block 1a is composed of nodes \\(A, B, C, S, U, V\\) and Block 1b is composed of nodes \\(E, F, G, H, T\\). Table 24.6 (b) separates our original Block 2 into three further sub-blocks. There’s the block composed of nodes \\(I, J, K, L, M\\). Let’s call this “Block 2a”, the block composed of nodes \\(N, O, P, Q, R\\). Let’s call this “Block 2b.” Then, there’s node \\(D\\). Note that this node is only structurally similar to itself and is neither similar nor dissimilar to the other nodes in the subgraph \\(d^{corr} = 0\\), so it occupies a position all by itself! Let’s call it “Block 2c.”\n\n\nTable 24.8: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    S \n    C \n    B \n    A \n    V \n    U \n  \n \n\n  \n    S \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    V \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    U \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    B \n    C \n    A \n    S \n  \n \n\n  \n    B \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    S \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\nNow let’s do a couple of final splits of the subgraph composed of nodes \\(A, B, C, S, U, V\\). This is shown in Table 24.8. The first split separates nodes in block \\(A, B, C, S\\) from those in block \\(U, V\\) (Table 24.8 (a)). The second splits the nodes in subgraph \\(A, B, C, S\\) into two blocks composed of \\(A, S\\) and \\(B, C\\), respectively (Table 24.8 (b)).\n\n\n\n\n\nFigure 24.3: An undirected graph with block membership indicated by node color.\n\n\n\n\nFigure 24.3 shows the nodes in Figure 24.1 colored according to our final block partition. It is clear that the blockmodeling approach captures patterns of structural similarity. For instance, all the single-connection nodes connected to more central nodes get assigned to their own position: Block 1b: \\(E, F, G, H, T\\), Block 2a: \\(I, J, K, L, M\\), and Block 2b: \\(N, O, P, Q, R\\). The most central node \\(D\\) (in terms of Eigenvector centrality) occupies a unique position in the graph. Two of the three central nodes (in terms of degree centrality) \\(B, C\\) get assigned to their own position. Meanwhile \\(A, S\\) form their own structurally similar block. Finally, \\(U, V\\) also form their own structurally similar block as both are structurally equivalent in the orignal graph.\n\n\n24.3.2 The Blocked Adjancency Matrix\nWhat happens if we were to go back fo the adjacency matrix corresponding to Figure 24.1, and then reshuffle the rows and columns to correspond to all these wonderful blocks we have uncovered? Well, we would en up with something like Table 24.9. This is called the blocked adjacency matrix. In the blocked adjacency matrix, the division between the nodes corresponding to each block of structurally similar nodes in Table 24.7 and Table 24.8 is marked by thick black lines going across the rows and columns.\nEach diagonal rectangle in Table 24.9 corresponds to within-block connections. Each off-diagonal rectangle corresponds to between block connections. There are two kinds of rectangles in the blocked adjacency matrix. First, there are rectangles that only contains zero entries. These are called zero blocks. For instance the top-left rectangle in Table 24.9 is a zero block. Then there rectangles that have some non-zero entries in them (ones, since this is a binary adjacency matrix). These are called one blocks. For instance, the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) is a one block.\n\n\n\n\n\n \n  \n      \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    T \n    E \n    F \n    G \n    H \n    B \n    C \n    A \n    S \n    U \n    V \n    D \n  \n \n\n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    M \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    O \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    P \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    Q \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    R \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    T \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    S \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n  \n  \n    U \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    V \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\nTable 24.9:  Blocked adjancency matrix. \n\n\nZero-blocks indicate that the members of the row block don’t have any connections with the members the column block (which can include themselves!). For instance, the zero-block in the top-left corner of the blocked adjacency matrix in Table 24.9 indicates that the members of this block are not connected to one another in the network (and we can verify from Figure 24.3 that this is indeed the case).\nOne blocks indicate that the members of the column block share some connections with the members of the column block (which can also include themselves!). For instance, the one-block in the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) tells us that members of this block are connected to at least one member of the \\(I, J, K, L, M\\) block (and we can verify from Figure 24.3 that this is indeed the case, since \\(B\\) is connected to all of them).\n\n\n24.3.3 The Image Matrix\nFrom this reshuffled adjacency matrix, we can get to a reduced image matrix containing the relations not between the nodes in the graph, but between the blocks in the graph. The way we proceed to construct the image matrix is as follows:\n\nFirst we create an empty matrix \\(\\mathbf{B}\\) of dimensions \\(b \\times b\\) where \\(B\\) is the number of blocks in the blockmodel. In our example, \\(b = 7\\) so the image matrix has seven rows and seven columns. The \\(ij^{th}\\) cell in the image matrix \\(\\mathbf{B}\\) records the relationship between row block i and column block j in the blockmodel.\nSecond, we put a zero in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 24.9 is a zero-block.\nThird, we put a one in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 24.9 is a one-block.\n\nThe result is Table 24.10.\n\n\n\n\n\n \n  \n      \n    I, J, K, L \n    N, O, P, Q, R \n    T, E, F, G, H \n    B, C \n    A, S \n    U, V \n    D \n  \n \n\n  \n    I, J, K, L \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    N, O, P, Q, R \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    T, E, F, G, H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B, C \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A, S \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    U, V \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\nTable 24.10:  Image matrix Corresponding to the blockmodel of an Undirected Graph. \n\n\nSo the big blocked adjacency matrix in Table 24.6 (c) can be reduced to the image matrix shown Table 24.10, summarizing the relations between the blocks in the graph. This matrix, can then even be represented as a graph, so that we can see the pattern of relations between blocks! This is shown in Figure 24.4\n\n\n\n\n\nFigure 24.4: Graph representation of reduced image matrix from a blockmodel.\n\n\n\n\nThis is how blockmodeling works!"
  },
  {
    "objectID": "lesson-positions-blockmod.html#references",
    "href": "lesson-positions-blockmod.html#references",
    "title": "24  Blockmodeling",
    "section": "References",
    "text": "References\n\n\n\n\nBreiger, Ronald L, Scott A Boorman, and Phipps Arabie. 1975. “An Algorithm for Clustering Relational Data with Applications to Social Network Analysis and Comparison with Multidimensional Scaling.” Journal of Mathematical Psychology 12 (3): 328–83.\n\n\nWhite, Harrison C, Scott A Boorman, and Ronald L Breiger. 1976. “Social Structure from Multiple Networks. I. Blockmodels of Roles and Positions.” American Journal of Sociology 81 (4): 730–80."
  }
]