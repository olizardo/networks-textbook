[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Networks",
    "section": "",
    "text": "Welcome\nThis is an introductory textbook on social networks to be used in conjunction with an undergraduate lecture class on social networks (SOCIOL 111) taught in the department of sociology at UCLA."
  },
  {
    "objectID": "lesson-what-are-networks.html#what-is-a-network",
    "href": "lesson-what-are-networks.html#what-is-a-network",
    "title": "1  What Are Networks?",
    "section": "1.1 What is a Network?",
    "text": "1.1 What is a Network?\nSo what is a network? Minimally, a network is composed of a set of units or entities. These are some times called nodes or vertices, however what makes these units into a network is the fact that at least some pairs of them are joined by a set of links or ties. These are also sometimes called edges. So a network is essentially a set of nodes some of which are linked together, usually because the units interact in some ways, and the come to be connected via some kind of relationship. In social networks, the nodes can be people and the connections can be any type of social tie (e.g., friendship, enmity, co-working), whether positive or negative, between them.\n\n\n\n\n\nFigure 1.1: Point and line plot of a network.\n\n\n\n\nIn future lessons, we will discuss the different types of ties that can exist in social networks as well as the major social network theories that have been developed by sociologists, anthropologists, and organization theorists to explain why these types of ties exist, how they work, and what benefits (or drawbacks) they have for people and organizations.\nIn a point and line plot such as the one shown in Figure 1.1, networks are represented as pictures. These kinds of pictures are also sometimes called sociograms or network diagrams (the people who don’t like them call them “hairballs”). The convention in these kind of pictures of networks is that the nodes (e.g., people) are drawn as circles (or some other polygon such as a triangles or squares) and the connections, ties, or links between people are drawn as lines or sometimes, if some kind of direction is implied, as arrows.\nThis way of representing networks is occasionally called a “graph”, although we will see that the idea of a graph is a little more abstract than just a picture. So in this class we will make a strong differentiation between pictorial network representations, and the mathematical concept of graph. When referring to the former, we will use the term plot or network diagram, reserving the term graph for the abstract mathematical object. In future lessons, we will get into more details about pictorial (and non-pictorial) ways of thinking about networks and how they connect to other ways of representing them."
  },
  {
    "objectID": "lesson-what-are-networks.html#types-of-networks",
    "href": "lesson-what-are-networks.html#types-of-networks",
    "title": "1  What Are Networks?",
    "section": "1.2 Types of Networks",
    "text": "1.2 Types of Networks\n\n\n\n\n\nFigure 1.2: Types of Networked Systems.\n\n\n\nGiven the very broad definition of networks given in preceding, it should start to dawn on you why people think that networks are everywhere. And, come to think of it, they kind of are! Any system of interacting parts or entities can be depicted and analyzed as a network. That is why the idea of a network cuts across the information (e.g., networks of words in a text), social (networks of students in a school), physical (networks of servers on the internet), biological (networks of neurons in the brain) sciences, and psychological (networks of symptoms of mental disorders). The diagram in Figure 1.2 gives you an idea of the different types of networked systems that exist in the world.\n\n\n\nFigure 1.3: Network of Characters in Game of Thrones. Image created by Marcos Martins Marchetti via https://www.kaggle.com/code/mmmarchetti/game-of-thrones-network-analysis\n\n\nFor instance, some social networks do not even have to be made up of real people! In fictional worlds, such as the Game of Thrones HBO series created from the books written by George R. R. Martin, some characters meet and get to know one another, but others never meet (or more likely in this show, (spoiler!) die before they get to meet). So we can construct a social network made up of acquaintance relations between the characters (who ended up meeting whom) such as the one depicted in Figure 1.3. This is a social network because the links are composed of a psychological or emotional relation between people. In this case the relation “knowing somebody” in the GoT world is an example of a symmetric tie between people (we will see a symmetric tie is in #sec-ties). In Figure 1.3, the size of the name of each character is proportional to the centrality of the actor in the network. As we will see in Chapter 20, centrality is an index of how important a given actor is in a network. So, even if you’ve never watched the show, you’ll know that Tyrion, Daenerys, and Jon are pretty important people in this show!\n\n\n\nFigure 1.4: The U.S. Airport Network. Image created by Jose M Salln via https://jmsallan.netlify.app/blog/plotting-us-airline-airport-networks/\n\n\nOther networks are composed of links between physical technological systems not people. Take, for instance, the various airports (large and small, international and regional) in the United States. Everyday, some number of flights departs from one airport and lands on another. This means that airports in the US are linked via directed (e.g., from/to) ties in a socio-Technical network. This network is “socio-technical because it involves both social and technological links built from the flights that go from airport to the next (see Figure 1.4). The network is social because it is composed of people traveling. The network is also technological because its links are made possible by a complex web of air-flight technology, including planes, radars, in-flight computer systems and so forth. As we will in Chapter 4, the directed links between airports are an example of an asymmetric tie; some airports fly a lot to other airports (e.g., Small Regional to LAX), but other airports (e.g., LAX) only schedule flights to other big airports).\n\n\n\nFigure 1.5: Network of Connectivity Between Brain Regions. Image from https://www.cam.ac.uk/research/news/wiring-the-brain\n\n\nAll of us carry around a very complex and staggeringly large network, composed of millions of nodes and billions of connections. The network is called the brain (see Figure 1.5). The nodes are a special type of biological cell called a neuron. Neurons are special because they have these filaments called dendrites and this long body called an axon. The axon of one neuron links up to the dendrites of another one, generating a large-scale complex network that allows you to breath, eat, drink, think, see, smell and read these pages. The brain is a biological network, because the nodes are biological units (neurons) as are the connections. Other biological networks include ecosystems were the nodes are species and the links are various types of relations between, some antagonistic (predator/prey) and others mutualistic.\n\n\n\nFigure 1.6: Network of Symptoms in the Diagnostic and Statistical Manual of Mental Disorders. Image from https://doi.org/10.1371/journal.pone.0137621\n\n\nThe nodes in a network can also be composed of psychological entities like attitudes, feelings, norms, types of thoughts, or emotions. In these psychological networks two entities are connected if they tend to occur in the same people. For instance, people who are have social anxiety, also experience loneliness, which is linked to depression. Figure 1.6 shows just such a network of symptoms, taken from [Diagnostic and Statistical Manual of Mental Disorders] created by psychological scientists who study psychopathology. In these network, the symptoms that cluster together (shown as nodes of different color) come to define specific types of mental disorders, like “Social Phobia” or “Panic Disorders.”"
  },
  {
    "objectID": "lesson-what-are-networks.html#what-is-a-social-network",
    "href": "lesson-what-are-networks.html#what-is-a-social-network",
    "title": "1  What Are Networks?",
    "section": "1.3 What is A Social Network?",
    "text": "1.3 What is A Social Network?\nAs you can see from the tree chart shown in Figure 1.2, only a subset of networks in the world as social networks. The key difference between social networks and other networks is that social networks have to involve people (or groups of people) and their perceptions, thoughts, interactions, and behaviors. Sometimes interactions between people are mediated by technologies. For instance, people can connect to one another by texting on their phone or traveling by plane, in which case the differentiation between what is a technological and a social network becomes a matter of degree.\nIn this class we will deal with networks that are closer to the “purely social” end of the scale: Those involving people, their perceptions, interactions, sentiments, exchanges, memberships, and relations. The primary perspective that we will take is that of social network analysis as developed in the discipline of sociology since the 1970s.\n\n\n\n\n\nFigure 1.7: Social Science Disciplines that Contribute to and Use Social Network Analysis.\n\n\n\nAs noted, the inter-disciplinary field in charge of studying social networks is called social network analysis (SNA), and is composed of insights from a variety of other social science disciplines such as sociology, anthropology, psychology, communication, and others (see Figure 1.7). SNA in its turn, is part of an even larger interdisciplinary field in charge of studying all types of networks called network science which includes work in physics, computer science, data science, biology, engineering, mathematics, and other fields. The relationship between these different scientific fields is depicted in Figure 1.8.\n\n\n\n\n\nFigure 1.8: The Network of Network Science."
  },
  {
    "objectID": "lesson-what-are-networks.html#the-two-faces-of-social-network-analysis",
    "href": "lesson-what-are-networks.html#the-two-faces-of-social-network-analysis",
    "title": "1  What Are Networks?",
    "section": "1.4 The Two Faces of Social Network Analysis",
    "text": "1.4 The Two Faces of Social Network Analysis\nSocial network analysis has two broad aspects. One, generally referred to as network theory is about figuring out how networks work and what networks do to and for people. In essence, social network theories are general statements about how people behave in networks and how networks themselves “behave”“; that is where network relations come from, what they do, and what consequences they have for the people involved.\nFor instance, the idea of social capital that is, that the connections that you have to others can bring you certain types of benefits, is part of network theory. In fact, as we will see later, a good chunk of network theory (but not all!), such as the theory of structural holes, or the strength of weak ties theory, can be thought of as theories of social capital (Borgatti and Halgin 2011). Other types of network theory deal with how networks of sentiment relations (e.g. likes and dislikes) form, while other tell us about how things flow through networks.\n\n\n\n\n\nFigure 1.9: The two faces of social network analysis.\n\n\n\nAnother branch of social network analysis deals with how to measure various network properties. This branch of social network analysis, called network measurement links social network concepts to some type of mathematical or quantitative representation. Since this branch of network analysis deals with measurement, it is where mathematics and other forms of quantitative representation of networks (such as matrices) come in handy.\nIf math scares you, don’t worry. Our job is to walk you slowly through it. But you still may be asking: Why math though? The beauty of math, is that it allows us to take some fuzzy social science concepts, stated in natural language, such as the idea of “popularity” or “social position” or “strength of connection” and give it a precise representation. That way we can use networks to learn about what makes the social world go round or predict why some people, organizations, or even whole countries are successful and others are not (among other things).\n\n\n\n\n\nFigure 1.10: Levels of analysis in social networks.\n\n\n\nThe two “faces” of SNA (network theory and network measurement) as well as some choice examples are depicted in Figure 1.9. Don’t get nervous if you do not know what the things at the bottom of the diagram (e.g., “density”), means. We will explain them to you in the forthcoming lessons We can develop theories or measure network properties at multiple levels of analysis. Like other complex systems, social networks feature dynamics at multiple nested levels. We will deal with four such leves in what follows. At the node level we may be interested in what properties nodes have by virtue of the connections they have within the network. Both the idea of an ego network and various measures of social position based on centrality are defined at this level.\nAt the dyad and triad levels, we may be interested in the properties that the edges or the links have by virtue of settling into certain configurations. Both the idea of tie strength and various theories dealing with triples of nodes such as Balance Theory (Davis 1963), Strength of Weak Ties Theory (Granovetter 1973), the theory of Structural Holes (Burt 1995), and Simmmelian Tie Theory (Krackhardt 1999) are defined this level. At the level of motifs may be interested in the network substructures or the “lego building blocks” that make up the larger network. For instance, how many configurations of three, four, or five actors can we observe? At the subgroup or community We may be interested in properties that subsets or clusters or nodes have by virtue of the set of connections they share. Here, theories and measures of group cohesion, and community structure in networks have been developed.Finally, we may be interested in measuring properties and theorizing the structure and dynamics of the whole network. This may includes quantities that are sums or averages of features computed at lower levels, or they may include properties applicable to the system as a whole (e.g., whether it would take a short or a long time to get something from one randomly selected person in the network to another). Ideas of whether human networks constitute Small Worlds (Milgram 1967) are defined at this level.\nIn Figure 1.10 we can see how the nested structure of social networks can be depicted. At all levels we can develop specific theories to understand what is happening at that slice or develop special measures designed to link the concepts of those theories to a precise quantitative representation."
  },
  {
    "objectID": "lesson-what-are-networks.html#networks-graphs-and-matrices",
    "href": "lesson-what-are-networks.html#networks-graphs-and-matrices",
    "title": "1  What Are Networks?",
    "section": "1.5 Networks, Graphs, and Matrices",
    "text": "1.5 Networks, Graphs, and Matrices\n’Social network analysis is an influential, and now increasingly widespread, methodological approach for analyzing the social world. Traditionally, sociologists have studied relationships using a variety of observational strategies, both qualitative, such as ethnography and interviews, and quantitative, such as those based on the social survey. However, beginning in earnest in the 1950s, sociologists began to make concerted use of mathematical techniques from a branch of pure mathematics called graph theory and a branch of applied mathematics called matrix algebra to develop scientific models of social relationships and to come up with measures connecting key concepts from social theory, such as roles, prominence, and prestige, to tangible empirical evidence.\nSocial Network Analysis (SNA) is the use of graph-theoretic and matrix algebraic techniques to study social structure and social relationships, which exist in real world networks. While much of this activity has to do with the measurement of social network concepts, Social Network Theory is the branch of social networks that tells us what social networks are, what they do, how they make a difference (negative or positive) in the world, and where networks come from and how they change over time.\n\n\n\nFigure 1.11: The ‘three-step shuffle’ in Social Network Analysis\n\n\nA key skill you will gain by taking this class is to transition swiftly from these three ways of talking about networks, namely, networks as real world systems of social interactions, networks as represented mathematically as graphs, and networks represented quantitatively as matrices. This three-step transition is represented in Figure 1.11. Another skill you will gain by taking this class is how to apply social network theory to understand how real world networks work and change."
  },
  {
    "objectID": "lesson-what-are-networks.html#references",
    "href": "lesson-what-are-networks.html#references",
    "title": "1  What Are Networks?",
    "section": "References",
    "text": "References\n\n\n\n\nBorgatti, Stephen P, and Daniel S Halgin. 2011. “On Network Theory.” Organization Science 22 (5): 1168–81.\n\n\nBurt, Ronald S. 1995. Structural Holes. Harvard University Press.\n\n\nDavis, James A. 1963. “Structural Balance, Mechanical Solidarity, and Interpersonal Relations.” American Journal of Sociology 68 (4): 444–62.\n\n\nGranovetter, Mark S. 1973. “The Strength of Weak Ties.” American Journal of Sociology 78 (6): 1360–80.\n\n\nKrackhardt, David. 1999. “The Ties That Torture: Simmelian Tie Analysis in Organizations.” Research in the Sociology of Organizations 16 (1): 183–210.\n\n\nMilgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67."
  },
  {
    "objectID": "lesson-graphs-theory.html#from-ties-to-graphs",
    "href": "lesson-graphs-theory.html#from-ties-to-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.1 From Ties to Graphs",
    "text": "2.1 From Ties to Graphs\nThis is likely not your first sociology course. But even if it is, relationships are relatively intuitive for people. They are all around us: you and your parents, you and your siblings, your siblings and your parents, you and your classmates, your classmates to each other. Affiliation, communication, friendship, hatred: these are the content of social relationships. Two people either have a relationship of some kind, or they do not. Essentially, a relationship is a connection between at least two social actors. We will see that sometimes, when relations clump together into networks, two sets of relations can have different content, but still share the same form (e.g., arrangement, or pattern).\nThe technical social networks term for “relations clumping together into networks” is concatenation (Martin 2009). For instance, every time you introduce two previously unrelated acquaintances to one another, you concatenate two previous disconnected ties into a connected triple."
  },
  {
    "objectID": "lesson-graphs-theory.html#social-ties",
    "href": "lesson-graphs-theory.html#social-ties",
    "title": "2  Introduction to Graphs",
    "section": "2.2 Social Ties",
    "text": "2.2 Social Ties\nFor example, you are likely enrolled in a social network’s class if you are reading this, and have people you know such as your friends and people you’ve taken prior classes with, but also people you’ve never seen before. It might be obvious that you have a relationship with those people that you know, but do you have a relationship with those you do not know?\n\n\n\nFigure 2.1: “One of the first pictures of a social network, then called a ‘sociogram’ was drawn by Jacob Moreno in 1934. It consisted of the relationships between 19 boys (triangles) and 18 girls (circles) in a 5th grade classroom”\n\n\nThe answer is maybe. It depends on how you define the term social relationship. If you were asked who your friends are, you would tell me that some of your classmates are your friends, and the rest of your classmates are not your friends. If you were asked which of these people in your classroom are your classmates however, everyone would be your classmate, except for your professor or teaching assistants. You share a particular type of relationship with these other people, your classmates, even if you’ve never met them. The word “classmate” even implies a relationship type, one with a different social meaning than “friend.” When analyzing a social network, it is important to first understand what type of social relationship you are analyzing, as it relates directly to what type of conclusions or generalizations you can make about the social world."
  },
  {
    "objectID": "lesson-graphs-theory.html#network-boundaries",
    "href": "lesson-graphs-theory.html#network-boundaries",
    "title": "2  Introduction to Graphs",
    "section": "2.3 Network Boundaries",
    "text": "2.3 Network Boundaries\nOnce you have a type of social relationship you would like to examine, the next step is to bound the context. If you want to map out all the social relationships in the world, well that’s impossible. Imagine how difficult it would be to map out all the people at your school who are friends with one another. That might be feasible if you have only 1,000 undergraduates, but at a school of 30,000, it would be a nightmare. That is in part why it is so important to bound the social context. The other is to exclude relationships that are not meaningful for your study. Bounding, or to draw boundaries, is to have a rule about what will or will not be included in the study (Laumann, Marsden, and Prensky 1989).\n\n\n\nFigure 2.2: “The Zachary karate club network study was one of the first data collection probjects in the history of SNA. The data are famous for showing how networks could be used to find groups based on the relations between actors.”\n\n\nFor example, if you are interested in who is friends with who in your social networks class, you have bound your study to look at only people who are in your social networks class. One of the most famous social network studies was performed by the anthropologist Wayne W. Zachary (see Figure 2.2 at a college karate club in the 1970s (Zachary 1977). Thus, the 34 members of the karate club and the outside teacher were the actors included in the study because they were the people who were involved in the day to day operations of the karate club at the time data were collected.\nWith a type of social relationship in some bounded context, you can begin to map the social world as a graph. In its most basic form, a graph is essentially a picture of the relationships between different types of social actors. This picture becomes incredibly powerful when we begin to use mathematical concepts to understand how actors relate to each other (mostly what this book is concerned with) or upon what social principles the network may have been formed.\nWhile this class will mostly use the terms node and edge when referring to graphs, these are not the only terms in use among those who use network analysis techniques. Additional names for nodes include, vertex or point. Relationships between two nodes are, in addition to being called edges, are referred to as ties or links. Table Table 2.1 shows the different network lingo people use across disciplines.\n\n\nTable 2.1: Network terminology across disciplines.\n\n\nAcademic Origin\nSocial Actor\nRelationship\n\n\n\n\nGraph Theory\nPoint\nLine\n\n\nNetwork Science\nVertex\nEdge\n\n\nSociology\nActor\nTie\n\n\nComputer Science\nNode\nLink"
  },
  {
    "objectID": "lesson-graphs-theory.html#the-building-blocks-of-graphs-edges-and-nodes",
    "href": "lesson-graphs-theory.html#the-building-blocks-of-graphs-edges-and-nodes",
    "title": "2  Introduction to Graphs",
    "section": "2.4 The Building Blocks of Graphs: Edges and Nodes",
    "text": "2.4 The Building Blocks of Graphs: Edges and Nodes\nThere is a mathematical definition of a graph which is slightly more technical. A graph is a set, usually represented by the capital letter G.\nFrom high school math, you may remember that the mathematical definition of a set is simply a collection of entities, some of which may be ordered and some of which may themselves be other sets (a set can have sets as its members). In the case of graphs, the entities inside the collection are a set of vertices (also called nodes) and a separate set of edges (also called links).\nA graph is thus a set containing two sets as its members: a set of nodes (usually represented by the capital letter V) and a set of edges (usually represented by the capital letter E).\nIn set theory notation:\n\\[    \n  G = \\{V, E\\}\n\\tag{2.1}\\]\nWhich says that the members of the set defined by the graph, which we call G, are two other sets, called E and V (who themselves have a series of members inside). The usual notation, like in Equation 2.1, is to enclose the members of a set in brackets \\(\\{\\}\\).\n\n2.4.1 Nodes\nThe set of nodes usually represents actors in the real world social network. Point and line diagrams (such as the ones shown in Figure 2.2) are used to represent graphs, these in their turn represent the real social network.\nIn these diagrams nodes (representing actors) are usually drawn as a circle, but they could be any shape or symbol. In social network analysis, actors are often either an individual or an organization, but, as we have seen, in wider applications of the network imagery in the physical and biological sciences (usually going under the banner of network science), nodes can represent anything that links up to other similar entities in a larger system. These include power generation stations and homes, servers and computers, animals in an ecosystem, towns, really anything of substance that we can define some kind of relation on, or from which some type of content can be said to be exchanged.\n\n\n2.4.2 Edges\nEdges represent the presence of a connection or a social tie between two nodes. As we will see this can be a permanent relationship (e.g., “brother of”) or a more fleeting interaction (e.g., a text message, being in the same place at the same time). We will define what social ties are, how many types exist, and what their properties are, in lecture. For now, we can say that in social network analysis, connections are relationships, or links between nodes, and edges in a graph are meant to represent these connections.\nIn graph theory, the set of edges is best thought of as a collection of pairs of nodes, where the two members of the pair are the nodes involved in the social tie. So if node A is linked to node B via some social tie (friendship, study group, coworkers), then AB is a member of the edge set of the relevant graph. In set theory notation, this is usually written as \\(AB \\in E\\) which is read as “the edge AB is a member of the set \\(E\\).” Edges can also be referred to by juxtaposing the two nodes that are connected by the edge. Thus, the edge AB can also be written as \\(V_A V_B\\).\nIn the case of power generation stations and homes, the edges can represent power lines. Meanwhile, servers and computers are connected by internet cables and wi-fi access, while towns are connected by roads. The existence of edges signal the potential for content to flow, whether that’s power, computer data, or people in cars. In the case of social networks, the content that flows between two nodes are such things as influence, advice, information, and support. But it may also be disease, bullying, or gossip."
  },
  {
    "objectID": "lesson-graphs-theory.html#what-is-a-graph",
    "href": "lesson-graphs-theory.html#what-is-a-graph",
    "title": "2  Introduction to Graphs",
    "section": "2.5 What is a Graph?",
    "text": "2.5 What is a Graph?\nFigure 2.3(a) shows an example of a point and line network diagram of a graph with four nodes and two edges. Nodes A, B, C and D are circles representing actors A, B, C and D, whose real world social relationships we are interested in studying. The lines drawn between A and B and likewise between B and C represent the edges, indicating the presence of a social tie. Thus the edges, AB and BC appear in the network diagram. The lack of an edge between nodes B and C reflects the absence of a relationship between actors named B and C in the real world.\nSo if we were to write out the graph shown in Figure 2.3(a) in terms of the sets that define the graph, we would say:\n\\[\n  G = \\{E, V\\}\n\\tag{2.2}\\]\n\\[  \nE = \\{AB, AC\\}\n\\tag{2.3}\\]\n\\[\nV = \\{A, B, C, D\\}\n\\tag{2.4}\\]\nThis says that the graph G shown in Figure 2.3(a) is a set with two members, E and V, each of which is its own set. The edge set of G has two members, AB and AC. The node set of G has four members, A, B, C, and D. The number of members in a set is typically referred to as the cardinality of the set, written \\(|N|\\) where \\(N\\) is the name of the set.\nSo, in the Figure 2.3(a) case, the cardinality of the edge set is two \\(|E| = 2\\), and the cardinality of the vertex set is four \\(|V| = 4\\). These two basic graph properties are so important, that they can serve as a “signature” for the graph. Sometimes people will refer to a graph as \\(G(n, m)\\) where \\(n\\) is the cardinality of the vertex set (number of nodes) and \\(m\\) is the cardinality of the edge set (number of links). Thus, the graph in Figure 2.3(a), can be referred to as a \\(G(4, 2)\\) graph. Note that Figure 2.3(a) this is not the only possible \\(G(4, 2)\\) graph. Figure 2.3(b) and Figure 2.3(c) show two other possible variations of a \\(G(4, 2)\\) graph.\n\n\n\n\n\n\n\n(a) A simple graph.\n\n\n\n\n\n\n\n(b) A G(4, 2) graph\n\n\n\n\n\n\n\n\n\n(c) Another G(4,2) graph.\n\n\n\n\n\n\n\n(d) An unlabeled G(4, 2) graph.\n\n\n\n\nFigure 2.3: Four simple graphs with four nodes and two edges."
  },
  {
    "objectID": "lesson-graphs-theory.html#graph-labeling",
    "href": "lesson-graphs-theory.html#graph-labeling",
    "title": "2  Introduction to Graphs",
    "section": "2.6 Graph Labeling",
    "text": "2.6 Graph Labeling\nIn Figure 2.3(a-c), the nodes have letters which function as “names” for each. Thus, we can refer to node \\(A\\), or node \\(B\\) and so forth. These names are arbitrary, we can as well use numbers \\(\\{1, 2, 3, ...\\}\\) or a combination of letters and numbers \\(\\{V_1, V_2, V_3, ....\\}\\).\nWhen the nodes of a graph are distinguished from one another using names, the graph is said to be labeled. Sometimes, the names don’t matter, so we don’t specify them, as in Figure 2.3(d). In this case, the graph is said to be unlabeled.\nGraph metrics like the ones we will compute starting on Chapter 5, are the same regardless of whether the graph is labeled or not. Most of the example graphs we will be looking at are of the labeled kind."
  },
  {
    "objectID": "lesson-graphs-theory.html#trivial-and-non-trivial-graphs",
    "href": "lesson-graphs-theory.html#trivial-and-non-trivial-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.7 Trivial and Non-Trivial Graphs",
    "text": "2.7 Trivial and Non-Trivial Graphs\nThe \\(G(1, 0)\\) graph, essentially that formed by an isolated person sitting alone in a room with no connections to others (which can represent the case of the Japanese Hikikomori) is sometimes called the trivial graph. Obviously, for purposes of social network analysis, the graphs that are be used are non-trivial.\nThe smallest non-trivial graph, and thus the smallest unit of social analysis, is the \\(G(2, 1)\\) graph, the social unit formed by two people connected by a single link, sometimes referred to as a connected dyad (the \\(G(2, 0\\) graph is a disconnected dyad, like two people stuck in a deserted island who decided to no longer speak to one another). Connected dyads are the “hydrogen atom” of society, the building block from which all other larger networks are built."
  },
  {
    "objectID": "lesson-graphs-theory.html#basic-graph-properties",
    "href": "lesson-graphs-theory.html#basic-graph-properties",
    "title": "2  Introduction to Graphs",
    "section": "2.8 Basic Graph Properties",
    "text": "2.8 Basic Graph Properties\nGraphs have some basic properties that we need to learn about:\n\nIn a graph, if two nodes are joined together by an edge, they are said be adjacent. So in Figure 2.3(a), nodes A and B are adjacent, as are nodes A and C. Pairs of nodes that are not linked by an edge, like nodes B and C, are said be nonadjacent.\nThe nodes at the two ends of each existing edge are said to be the end vertices of that edge. Each edge has two end vertices. As we have been doing, edges are named by typing together the names of their two end vertices. So the edge that has nodes A and B as its endpoints is called AB.\nIf an edge “touches” a node (e.g., connects it to another node) we say that that edge is incident on that node. So in Figure 2.3(a), the edge AB is incident on both nodes A and B. The relation of incidence will be important in a later lesson when we discuss network metrics computed at the node level like degree centrality.\nIn a graph, nodes that are not connected to any other nodes are called isolates. This means that in Figure 2.3(a), node D is an isolate because it has zero edges incident upon it.\nIf a node is connected to \\(n-1\\) nodes in a graph—basically, every other node but themselves—it is called a star node or a dominant node. The star node is the oppossite of an isolate, having the maximum number of connections that could be observed in a social network.\n\n\n\n\n\n\n\n\n(a) A G(6, 5) graph.\n\n\n\n\n\n\n\n(b) The complement of a G(6, 5) graph.\n\n\n\n\nFigure 2.4: A graph and its complement."
  },
  {
    "objectID": "lesson-graphs-theory.html#sec-complement",
    "href": "lesson-graphs-theory.html#sec-complement",
    "title": "2  Introduction to Graphs",
    "section": "2.9 The Graph Complement",
    "text": "2.9 The Graph Complement\nSometimes when examining a graph, we may be interested in the graph’s evil twin. This is called the graph complement.\nMore technically, for any graph \\(G\\), its complement \\(G'\\) is a graph that meets the following two conditions:\n\nEvery pair of nodes adjacent in \\(G\\) are non-adjacent in \\(G'\\).\nEvery pair nodes that are non-adjacent in \\(G\\) and are adjacent in \\(G'\\).\n\nThat is, the disconnected nodes in graph \\(G\\) are connected in its complement \\(G'\\), and the connected nodes in \\(G\\) are disconnected in \\(G'\\).\nFor instance, Figure 2.4 shows a \\(G(6, 5)\\) graph in (a), namely a graph with six nodes and five edges, and the complement of that graph in (b). As we can see, every pair of nodes that is connected in (a) is disconnected in (b) and vice versa.\nWhat’s the graph complement useful for? As we will see later on, we sometimes will be interested in counting the non-relations that exist in a network in addition to the relations. The complement (and its matrix representation) is useful for that."
  },
  {
    "objectID": "lesson-graphs-theory.html#simple-graphs",
    "href": "lesson-graphs-theory.html#simple-graphs",
    "title": "2  Introduction to Graphs",
    "section": "2.10 Simple Graphs",
    "text": "2.10 Simple Graphs\nGraphs like that shown in Figure 2.3 are called simple graphs. There are two requirements for a graph to count as a simple graph:\n\nFirst, there can only be one edge joining two nodes at any time. That is, there cannot be multiple lines linking together the same pair of nodes. We will see in a later lesson that there are types of graphs that relax this restriction.\nThe second requirement is that graph does not contain any edges that have the same node as its two end points. These kind of edges are called loops, and they are edges that connect a node to itself! Clearly this does not make sense for most sociological applications.\n\nWith some exceptions, noted in subsequent lessons, simple graphs can represent most social networks."
  },
  {
    "objectID": "lesson-graphs-theory.html#references",
    "href": "lesson-graphs-theory.html#references",
    "title": "2  Introduction to Graphs",
    "section": "References",
    "text": "References\n\n\n\n\nLaumann, Edward O, Peter V Marsden, and David Prensky. 1989. “The Boundary Specification Problem in Network Analysis.” In Research Methods in Social Network Analysis, edited by Linton C. Freeman, Douglas R. White, and Antone Kimball Romney, 61–79. New Brunswick: Transaction Publishers.\n\n\nMartin, John Levi. 2009. Social Structures. Princeton University Press.\n\n\nZachary, Wayne W. 1977. “An Information Flow Model for Conflict and Fission in Small Groups.” Journal of Anthropological Research 33 (4): 452–73."
  },
  {
    "objectID": "lesson-graphs-subgraphs.html#graphs-and-subgraphs",
    "href": "lesson-graphs-subgraphs.html#graphs-and-subgraphs",
    "title": "3  Graphs and their Subgraphs",
    "section": "3.1 Graphs and Subgraphs",
    "text": "3.1 Graphs and Subgraphs\nConsider the graph shown in Figure 3.1 (a). If all the actors that you are interested in studying are included here, we would refer to it as the whole network. However, sometimes, even when we collect data on a large number of actors we may be interested in analyzing not the whole network, but only some parts of it. How do we do that?\n\n\n\n\n\n\n\n(a) Original graph\n\n\n\n\n\n\n\n(b) A subgraph of the original graph.\n\n\n\n\n\n\n\n\n\n(c) Another subggraph of the original graph.\n\n\n\n\n\n\n\n(d) An edge deleted subgraph of the original graph.\n\n\n\n\nFigure 3.1: A graph (a) and a subgraph (b).\n\n\nWell, good thing that a graph is actually a set of two sets. If you remember your high school set theory, you can always take a set and consider only a subset of the original members.\nSince graphs are sets, we can do the same thing. A subset of the original nodes (or edges) of a graph, is called a subgraph. So if \\(G =\\{E,V\\}\\) is the original graph, the subgraph \\(G' = \\{E',V'\\}\\) is a subset of \\(G\\), which is written \\(G \\subset G'\\), with the understanding that \\(E' \\subset E\\) and \\(V' \\subset V\\). In mathematics, “\\(\\subset\\)” is the symbol for subset. Thus, \\(A \\subset B\\) is read as “set A is a subset of set B.”\nFor instance, let us say we are interested in just analyzing actors A, B, C, D, and E in the graph shown in Figure 3.1 (a). They seem to be a close-knit group of people. In that case, as noted earlier, if we call the original graph \\(G\\) with vertex and edge sets \\(\\{E, V\\}\\)we can define a new subgraph \\(G'\\), whose node subset \\(V'\\) only includes the actors we are interested in studying, in this case \\(V' = \\{A, B, C, D, E\\}\\), where \\(V' \\subset V\\).\nThe subgraph \\(G'\\) is shown in Figure 3.1 (b). It looks exactly like we wanted, capturing the relations between an inter-connected subgroup of actors in the original graph. Note that the edge set of the subgraph \\(E'\\) only includes those edges that are incident to the other nodes in the subgraph (as defined in Chapter 2) and omits those in the original graph that are incident to nodes that are not in the subgraph, so \\(E' \\subset E\\). As we will see in a later lesson, well-connected subgroups of actors of an original graph are called a cohesive subset."
  },
  {
    "objectID": "lesson-graphs-subgraphs.html#vertex-and-edge-induced-subgraphs",
    "href": "lesson-graphs-subgraphs.html#vertex-and-edge-induced-subgraphs",
    "title": "3  Graphs and their Subgraphs",
    "section": "3.2 Vertex and Edge-Induced Subgraphs",
    "text": "3.2 Vertex and Edge-Induced Subgraphs\nFor any graph, we can define a subgraph based on any old random subset of the original node set. It is completely up to us. For instance, we could define a new subgraph \\(G''\\) of the original graph shown in Figure 3.1 (a), that includes the node set \\(V'' = \\{D, E, G, I\\}\\). That is shown in Figure 3.1 (c). That subgraph is weird (composed of two standalone connected dyads) and probably not very useful, but it is a subgraph of the original graph anyways!\nJust like we can define subgraphs based on the node set of a graph, we can define subgraphs based on subsets of the original edge set. For instance, we could pick the edges \\(E' = \\{AB, AC, AJ\\}\\) and define a subgraph based on them, which will necessarily include node set \\(V' = \\{A, B, C, J\\}\\).\nWhen a subgraph is defined by selecting a subset of nodes to keep (the common case) it is called a vertex-induced subgraph of the original graph, like Figure 3.1 (b). When a subgraph is defined by picking a subset of edges from the original graph to keep, it is called (you guessed it) an edge-induced subgraph of the original graph."
  },
  {
    "objectID": "lesson-graphs-subgraphs.html#vertex-and-edge-deleted-subgraphs",
    "href": "lesson-graphs-subgraphs.html#vertex-and-edge-deleted-subgraphs",
    "title": "3  Graphs and their Subgraphs",
    "section": "3.3 Vertex and Edge-Deleted Subgraphs",
    "text": "3.3 Vertex and Edge-Deleted Subgraphs\nA common reason for defining subgraphs in social network analysis is when we wonder what a network would look like if were to get rid of an actor or a set of actors. Sometimes this is useful, when we want to get a sense of how important that set of actors is for holding the network together.\nSo it is possible to define a subgraph by deleting nodes. This is written \\(G' = G - \\{a, b, c\\}\\), where \\(\\{a, b, c\\}\\) is the set of nodes we are deleting. So this says “give me a subgraph \\(G'\\) that is equal to the original graph \\(G\\) minus nodes \\(\\{a, b, c\\}\\).” In our previous example, Figure 3.1 (b) is the subgraph that results when we delete nodes \\(\\{F, G, H, I, J\\}\\) from the graph in Figure 3.1 (a): \\(G' = G - \\{F, G, H, I, J\\}\\). This is called a vertex-deleted subgraph of the original graph.\nThe subgraph that results from removing all the nodes of the original graph (so that the cardinality of the node set is now zero) is called the null graph. The subgraph that results from removing all the nodes of the original graph except for one (so that the cardinality of the node set of the resulting subgraph is equal to one) is called the singleton graph, also called the trivial graph as we saw in Chapter 2 (one is indeed a lonely number in social networks).\nJust like we can create subgraphs by deleting nodes, we can also create subgraphs by removing edges. For instance Figure 3.1 (d) is the subgraph that results from removing edges \\(\\{AB, AE, AC, CI, CE, GJ\\}\\) from the graph shown in Figure 3.1 (a). This is written \\(G' = G - \\{AB, AE, AC, CI, CE, GJ\\}\\), which says “give me a new graph \\(G'\\) which is equal to the original graph \\(G\\) minus edges \\(\\{AB, AE, AC, CI, CE, GJ\\}\\). This is called the edge-deleted subgraph of the original graph.\nA subgraph created by removing only edges, but leave all the nodes of the original graph intact, like in Figure 3.1 (d), is also called a spanning subgraph of the original graph.\nThe subgraph that results from removing all the edges in a graph, such that the cardinality of the edge set turns to zero, is called the empty graph.\nAs we will see later, subgraphs (as well as vertex and edge deletion) are a useful concept for discussing levels at an “in-between” levels, above the node level but “below” the whole network level: subgroups. However, subgraphs are also useful for network concepts at the node level, because there is a special type of subgraph, called the ego graph that is defined by picking a central node and the nodes that are connected to it, along with the edges connecting the nodes surrounding ego. We will cover ego graphs in ?sec-egonets."
  },
  {
    "objectID": "lesson-graphs-ties.html#symmetric-ties-and-undirected-graphs",
    "href": "lesson-graphs-ties.html#symmetric-ties-and-undirected-graphs",
    "title": "4  Types of Ties and Their Graphs",
    "section": "4.1 Symmetric Ties and Undirected Graphs",
    "text": "4.1 Symmetric Ties and Undirected Graphs\nAs noted in Chapter 2, Nodes and edges are indeed the building blocks of a graph. However, types of relationships that the edges represent can change both how we understand the network conceptually and also what mathematical techniques we can apply to the graph when we compute graph metrics (the subject of Chapter 5). The basic idea is that when we do network analysis, we want to map our understanding of the nature of the social relationships we are studying to the types of graphs we use to represent the network formed by the concatenation of those relationships (Martin 2009).\n\n\n\n\n\nFigure 4.1: A undirected graph\n\n\n\n\nLet us assume that Figure 4.1 represents a network of people who spend time together. One way of building this network would be to ask people on your dorm room floor who are the people that they spend some amount of time (e.g., more than an hour a week) hanging out with. By definition the relation “spending time together” lacks any inherent directionality. Mutuality (or reciprocity) is built in by construction. It would be nonsensical for a person (say A) to claim that they spend time with another person (say B) and for B to say that they do not spend time with A. In social network analysis these types of ties are called symmetric ties (Heider 1946). So if \\(AB\\) means that A spends time with B if we know \\(AB\\) is true, then we know that \\(BA\\) is also true.\nIn the same way, two people being in the same place at the same time (co-location), even if they do not one another, is an example of a symmetric tie. You also have the symmetric tie “being in the same class as” every other student that is also taking your Social Networks seminar this term. Note that, in this sense, all co-memberships create symmetric ties among all actors involved (we will revisit this topic when talking about affiliation networks in Chapter 22). For instance, if I am a member of your family, you are also my family member; if we are both members of the soccer club, we are considered teammates.\nSocial networks composed of symmetric ties are represented using undirected graphs like the one shown in Figure 4.1. When an undirected graph has no loops (edges connecting a node to itself), and there is only one edge connecting adjacent vertices to one another (the graph has no multiedges), it is called a simple undirected graph.\nNetworks composed of symmetric ties have some interesting properties. If we know that the relationship (R) linking two nodes A and B is symmetric, then only a single edge exists that links them, and it does not matter whether we call this edge AB or BA. The order does not matter. In this way, we can formally define as symmetric tie as one that lacks directionality; if a tie is symmetric, then if we know that A is related to B (the AB edge is part of the edge set of the graph), then we know by necessity that B is related to A.\nCan you think of other examples of symmetric ties? Is friendship, as culturally defined in the contemporary world, a symmetric tie?"
  },
  {
    "objectID": "lesson-graphs-ties.html#asymmetric-ties-and-directed-graphs",
    "href": "lesson-graphs-ties.html#asymmetric-ties-and-directed-graphs",
    "title": "4  Types of Ties and Their Graphs",
    "section": "4.2 Asymmetric Ties and Directed Graphs",
    "text": "4.2 Asymmetric Ties and Directed Graphs\nIn contrast to spending time together, being members of the same family, or being in the same place at the same time, some social ties allow for inherent directionality. Edges in these graphs are are called asymmetric ties (Heider 1946). That is, one member of the pair can claim to have a particular type of social relationship with the other, but it is possible (although not necessary) that the other person fails to have the same relationship with the first.\nHelping or social support relations, are like this. For instance, you can help someone with their homework, or given them personal advice, but this does not necessarily mean that that person will return the favor. They may, or they may not. The point is that, in contrast to symmetric tie, mutuality or reciprocity is not built in by definition, but must happen as an empirical event in the world. We need to ask the other person to find out (or check their email logs). Can you think of other examples of asymmetric social ties?\n\n\n\n\n\nFigure 4.2: A directed graph\n\n\n\n\nReciprocity is an important concept in social network analysis. Some have said it is perhaps the most important concept for understanding human society (Gouldner 1960), which may be a bit of an exaggeration. Only asymmetric ties may have the property of being non-reciprocal or having more or less reciprocity. If I think you are my friend, I very much hope that you also think you are my friend.\nThat said, sociologists have found that in many natural social settings this is not the case. Sometimes people think they are friends with others, but those other people disagree (Carley and Krackhardt 1996). For this reason, sociologists typically ask: if I do you a favor, would you do me a favor in the future? Additionally, sociologists often ask: if I treat you with respect, will you also treat me with respect? If I text you, will you text me back? If this is true, we have a level of reciprocity in our relationship.\nFor some ties, such advice or support, or friendship relations, reciprocity is all or none; it either exists or it does not. For instance, the friendship offer you extend to someone may be reciprocated (or not). In the same way, you can like someone and they may like you back (or not), like the notes you passed around in middle school. For other ties, such as communication ties (e.g., those defined by the amount of texting, or calling), reciprocity is a matter of degree, there may be more or less. For instance, you can text someone 10 times a day, but they may text you back only half of those instances. In all cases, reciprocity is at a maximum when the content of the relationship is equally exchanged between actors.\nCan you think of relationships in your life characterized by more or less reciprocity?\nJust like symmetric ties are represented using a particular type of graph (namely, an undirected graph), social networks composed of asymmetric ties are best represented by a type of graph called a directed graph (or digraphs for short). Figure 4.2 shows the point and line diagram picture of a digraph. What were simple lines for in the undirected graph shown in Figure 4.1 have been replaced with arrows indicating directionality.\nA node sends a relationship to the node that the arrow points to, which in turn receives the relationship. In a digraph, up to two directed arrows may link nodes going in both directions. When an undirected graph has no loops (edges connecting a node to itself), and there is only one edge connecting a sender node to a receiving node (the graph has no multiedges) like Figure 4.2, it is called a simple directed graph.\nIn a directed graph, for every edge, there is a source node and a destination node. So in the case of “A helps B” the source node is A and the destination node is B. In the case of “B helps A” the source node is B and the destination node is A. This means that in a directed graph, in contrast to a undirected one, the order in which you list the nodes when you name the edges matters. Thus, the edge AB is a different one from the edge BA. The first one may exist but the second one may not exist (edges in a directed graph are sometimes also called arcs).\nFor instance, if Figure 4.2 were an advice network (Cross, Borgatti, and Parker 2001), on the other hand, we could say that H seeks advice from D, but D does not seek advice from H. This may be because D is higher in the office hierarchy or is more experienced than H, in which case lack of reciprocity may be indicative of an authority relationship between the two nodes.\nOne must always be careful when examining a directed network to make sure one properly understands the direction of the underlying social relationships!\n\n4.2.1 Types of Nodes in Directed Graphs\nIn a directed graph, there will typically be three types of (non-isolate) nodes (Harary, Norman, and Cartwright 1965):\n\nFirst, there will be nodes that receive ties but don’t send them. These are called receivers (like node C in Figure 4.2). For receiver nodes \\(k_{in} > 0\\) and \\(k_{out} = 0\\).\nSecond, there will be nodes that receive ties and also send out ties. These are called carriers (like nodes A and B in Figure 4.2. For carrier nodes, \\(k_{in} > 0\\) and \\(k_{out} > 0\\).\nFinally, there will be nodes that send ties but don’t receive them. These are called transmitters (like nodes E and G in Figure 4.2). For transmitter nodes, \\(k_{in} = 0\\) and \\(k_{out} > 0\\)."
  },
  {
    "objectID": "lesson-graphs-ties.html#references",
    "href": "lesson-graphs-ties.html#references",
    "title": "4  Types of Ties and Their Graphs",
    "section": "References",
    "text": "References\n\n\n\n\nCarley, Kathleen M, and David Krackhardt. 1996. “Cognitive Inconsistencies and Non-Symmetric Friendship.” Social Networks 18 (1): 1–27.\n\n\nCross, Rob, Stephen P Borgatti, and Andrew Parker. 2001. “Beyond Answers: Dimensions of the Advice Network.” Social Networks 23 (3): 215–35.\n\n\nGouldner, Alvin W. 1960. “The Norm of Reciprocity: A Preliminary Statement.” American Sociological Review, 161–78.\n\n\nHarary, Frank, Robert Z Norman, and Dorwin Cartwright. 1965. Structural Models: An Introduction to the Theory of Directed Graphs. Wiley.\n\n\nHeider, Fritz. 1946. “Attitudes and Cognitive Organization.” The Journal of Psychology 21 (1): 107–12.\n\n\nMartin, John Levi. 2009. Social Structures. Princeton University Press."
  },
  {
    "objectID": "lesson-graphs-metrics.html#computing-metrics-on-graphs",
    "href": "lesson-graphs-metrics.html#computing-metrics-on-graphs",
    "title": "5  Basic Graph Metrics",
    "section": "5.1 Computing Metrics on Graphs",
    "text": "5.1 Computing Metrics on Graphs\nAs graphs are really just representations of real world-networks, they can be as varied as different networks are from one another. Some social networks are small (composed of just a few people), while others are large. In some social networks there is not much connectivity, while other networks feature thousands if not millions of ties between people.\nWe can quantify a lot of these properties of real world networks by representing them as graphs and then computing graph metrics. This is fancy word for counting various numbers most of which are a function of the number of nodes and edges in a graph. In this lesson, we will review the most important graph metrics in social networks.\n\n5.1.1 Graph Order\nThe order of a graph (typically written as \\(n\\)), is the number of nodes in the graph. Technically speaking, keeping in mind that the graph is really a set of a set of nodes and edges, the order is the cardinality of node set \\(n = |V|\\). Cardinality is simply a technical term from set theory indicating how many elements there are in a set. Thus, the graph in Figure 4.1 has an order of 9, while Figure 4.2 has an order of 7.\n\n\n5.1.2 Graph Size\nLikewise, the size of a graph (typically written as \\(m\\)), is the number of edges in the graph, which is (like with nodes) the cardinality of the edge set \\(m = |E|\\). Thus, the size of the graph in Figure 4.1 is 16, while the size of the graph in Figure 4.2 it is 11.\nThis means that if you say the size of the graph in Figure 4.1 is 16, we will all understand that you are referring to the number of edges, as you would use the term order if you were talking about the number of nodes."
  },
  {
    "objectID": "lesson-graphs-metrics.html#the-graph-maximum-size",
    "href": "lesson-graphs-metrics.html#the-graph-maximum-size",
    "title": "5  Basic Graph Metrics",
    "section": "5.2 The Graph Maximum Size",
    "text": "5.2 The Graph Maximum Size\nIn some cases we may be interested to know how many edges there could be in a graph if everyone had a relationship with everyone else in the network. This is the maximum possible number of edges that could exist in a network of order \\(n\\).\nRecall that, as we discussed previously, the number of edges in a graph is called the graph size. Additionally, a graph in which all the edges that could exist are actually present is called the complete graph. The maximum size of a graph of order \\(n\\) is the number of edges that would exist in that graph is the graph was complete. A graph in which no edges exist (every node is an isolate), is called an empty graph.\nWhile this may seem like a complicated thing, it is actually given by a simple (and important) formula. For an undirected graph this is:\n\\[\n  Max(E)^{u}=\\frac{n(n-1)}{2}\n\\tag{5.1}\\]\nFor a directed graph, the same computation is even simpler. The maximum number of possible edges in this case is:\n\\[\n  Max(E)^d=n(n-1)\n\\tag{5.2}\\]"
  },
  {
    "objectID": "lesson-graphs-metrics.html#graph-maximum-size-explainer",
    "href": "lesson-graphs-metrics.html#graph-maximum-size-explainer",
    "title": "5  Basic Graph Metrics",
    "section": "5.3 Graph Maximum Size Explainer",
    "text": "5.3 Graph Maximum Size Explainer\nWhere do these formulas for maximum graph size come from? Let’s consider the directed case first. A graph would be complete if every node was connected to every other node. It is easy to see that for each node, this would be every other node in the graph except themselves. So in a complete graph of order \\(n\\), the degree of each node has to be \\(n-1\\). So if we call the degree of the first node in the graph \\((n-1)_1\\), then the degree set (\\(\\mathbf{k}^c\\)) of a complete graph would be:\n\\[\n  \\mathbf{k}^{c} = [(n-1)_1, (n-1)_2, (n-1)_3 \\dots (n-1)_n]  \n\\tag{5.3}\\]\nIt is easy to see the that the sum of degrees (\\(\\sum k_i^c\\)) of the complete graph, will give us the maximum size for any graph of that order. So we can write that sum as:\n\\[\n  \\sum k_i^{c} = \\sum_i^{n} n-1\n\\tag{5.4}\\]\nEssentially we add up \\(n-1\\) to \\(n-1\\) to \\(n-1\\), etc. \\(n\\) times, once for each node in the graph. From basic arithmetic, we know that adding the same number \\(n\\) number of times is the same as multiplying that number by \\(n\\). So therefore, the maximum size of a complete graph of order \\(n\\) is \\(n \\times n-1 = n(n-1)\\), the formula written as Equation 5.2!\nIn the undirected case (Equation 5.1, the procedure is the same, except that now each edge shows up twice in the degree sum, but one of those is redundant. Therefore we divide the whole sum of degrees by 2 to eliminate double-counting, resulting in \\(\\frac{n(n-1)}{2}\\) as the formula for the maximum size of an undirected graph or order \\(n\\).\n\n5.3.1 Applying your knowledge\nYou can now look a genius. If somebody were to ask you:\nQ: In classroom with 10 kindergartners, how many total pairs of friends would exist if every kid was friends with every other kid?\nUsing your graph theory metrics knowledge, you can now pull out your phone calculator and compute:\n\\[\n  \\frac{10(10-1)}{2} =\n  \\frac{10(9)}{2} =\n  \\frac{90}{2} = 45\n\\]\nImpressing your audience immensely!"
  },
  {
    "objectID": "lesson-graphs-metrics.html#graph-density",
    "href": "lesson-graphs-metrics.html#graph-density",
    "title": "5  Basic Graph Metrics",
    "section": "5.4 Graph Density",
    "text": "5.4 Graph Density\nIn a classic study, the British social anthropologist Elizabeth Bott, made a classic distinction between two types of network structure. According to Bott, some networks are tight-knit and others are loose-knit (Bott 1957). Tight-knit networks feature a lot of connections and actors can reach others via multiple pathways. Loose-knit networks are more sparsely connected and actors are only reachable to one another via a very restricted set of pathways.\nIt was soon realized that a very simple graph metric, called the network density could be a useful index of Bott’s ideas of tight versus loose-knit networks. Tight-knit networks are dense featuring a lot of inter-connections between actors, while loose-knit networks are less dense (Barnes 1969). How can we think of this in terms of graph theory concepts?\n\n5.4.1 Density in Undirected Graphs\nThe density \\(d(G)\\) of a graph is a measure of how many ties between actors exist compared to how many ties between actors are possible, given the graph size (number of nodes) and the graph order (number of links). As such, the density of an undirected graph is quite simply calculated as, the ratio between the observed number of edges \\(m\\) (the cardinality of the edge set), and the graph maximum size as defined using Equation 5.1.\n\\[\n  d(G)^u=\\frac{m}{\\frac{n(n-1)}{2}}=m \\times \\frac{2}{n(n-1)}=\\frac{2m}{n(n-1)}\n\\tag{5.5}\\]\nWhere \\(m\\) is the number of edges (graph size) and \\(n\\) is the number of nodes (graph order) in the network.\nFor the graph shown in Figure 4.1, of size \\(m = 16\\) and order \\(n=9\\), we can use Equation 5.5 to compute the graph density as follows:\n\\[\n  d(G)^u = \\frac{2m}{n(n-1)} =\n  \\frac{2 \\times 16}{9 \\times (9-1)}=\n  \\frac{32}{9 \\times 8} =\n  \\frac{32}{72} =\n  0.44\n\\]\nThe estimated density of the network, \\(d = 0.44\\) tell us that 44% of the total possible number of edges are actually observed. Another way to think about density, is as giving the probability that, if we were to choose two random nodes in the network, this random dyad will have probability \\(p = 0.44\\) of being connected (as opposed to null).\n\n\n5.4.2 Density in Directed Graphs\nTo compute the density of a directed graph, there is no need to multiply the numerator by two, as each edge does single duty. As such, the equation for computing the density of a directed graph is:\n\\[\n  d(G)^d=\\frac{m}{n(n-1)}\n\\tag{5.6}\\]\nWhich is even simpler than in the directed case!\n\n\n5.4.3 Why is density important?\nThe density of a network property is important to consider for two reasons. First, (which is the definition of density!) is that it can help us understand how connected the network is compared to how connected it might be. Second, when comparing two networks with the same number of nodes and the same type of relationships, it can tell us how the networks are different.\nFor example, let us imagine that there are two organizations, each with 10 people in them. The one organization has high density and the other has low density in terms of the interactions among the members. What might be some of the underlying social differences between the two organizations? While we would need more information, we could posit that the one issue might be that information does not transmit very efficiently across the low density organization because it has to go from member to member, rather than diffusing from one member rapidly to all the others. Another issue might be the “hit by a bus” problem, where if one or two members are taken out of the network, you can suffer breakdown because they are no longer there to coordinate the different parts that don’t talk to each other. Denser networks are less vulnerable to disruption due to remove of key nodes."
  },
  {
    "objectID": "lesson-graphs-metrics.html#references",
    "href": "lesson-graphs-metrics.html#references",
    "title": "5  Basic Graph Metrics",
    "section": "References",
    "text": "References\n\n\n\n\nBarnes, John A. 1969. “Graph Theory and Social Networks: A Technical Comment on Connectedness and Connectivity.” Sociology 3 (2): 215–32.\n\n\nBott, Elizabeth. 1957. Family and Social Network: Roles, Norms, and External Relationships in Ordinary Urban Families. Tavistock Publications."
  },
  {
    "objectID": "lesson-graphs-neighborhoods.html#node-neighborhoods",
    "href": "lesson-graphs-neighborhoods.html#node-neighborhoods",
    "title": "6  Nodes and their Neighborhoods",
    "section": "6.1 Node Neighborhoods",
    "text": "6.1 Node Neighborhoods\nAs we have seen, each node in a graph or order \\(N\\), given by the set \\(V = \\{v_1, v_2, v_3, \\dots v_N\\}\\) may be adjacent to a certain set of other nodes. In graph theory, these are called the node’s neighbors.The neighborhood of a node in a graph is written as \\(\\mathcal{N}(v)\\), where \\(v\\) is the node’s name in the graph. For instance, if we are referring to the neighbors of node A in the graph shown as Figure 6.1 we would write \\(\\mathcal{N}(A)\\).\nThe neighborhood of each node is a proper subset of the larger set of nodes in the graph \\(V\\). This is written as \\(\\forall v: \\mathcal{N}(v) \\subset V\\), which translates from math to English as “for all nodes \\(v\\), the neighborhood of \\(v\\) is a subset of the larger node set \\(V\\).” In Figure 6.1 for instance, \\(\\mathcal{N}(A) = \\{B, C, D, F\\}\\), and \\(\\mathcal{N}(A) \\subset V\\).1\n\n\n\n\n\nFigure 6.1: Another simple graph.\n\n\n\n\n\n6.1.1 Node Neighborhood Intersection\nNote that the neighbor sets of two nodes can have members in common. For instance, in Figure 6.1 we have \\(\\mathcal{N}(A) = \\{B, C, D, F\\}\\) and we also have \\(\\mathcal{N}(D) = \\{A, B, F\\}\\). These two sets share common members!\nSometimes we may be interested in the total number of other people that two nodes share a connection with. Like when you wonder how many people you and your friend are both friends with (or a social media algorithm lets you know). This is called the intersection of the two node neighborhood sets.\nSo if A and D are both nodes in a graph, the intersection of their neighborhood sets gives us a list of the other nodes in the graph they are both connected to. Using set theory notation, this can be written as: \\(\\mathcal{N}(A) \\cap \\mathcal{N}(D) = \\{B, F\\}\\), which says that nodes A and D have B and F as common neighbors.2\nThe cardinality of the sets formed by the intersection of the neighborhoods of all the nodes in the graph gives us the number of common neighbors between each pair of nodes, which may be zero if two neighborhoods sets are disjoint. As we saw in Chapter 2, in set theory, the cardinality of a set is the number of members in that set. Thus, the cardinality of the set \\(\\{A, B, C, D\\}\\) is four. Two sets are disjoint if they have no members in common, which means that their intersection is the empty set. We will see in a later lesson that this quantity has applications for deriving important matrices from graphs and computing some key network metrics in the network.\nNote that two nodes can have common neighbors even if they are not directly connected in the network! So the number of common neighbors is defined for both connected and null dyads.\nFor instance, in Figure 6.1, the intersection of the neighborhoods of nodes D and E exists and it is given by \\(\\mathcal{N}(D) \\cap \\mathcal{N}(E) = \\{B, F\\}\\) even though nodes D and E are not linked (they are nonadjacent).\n\n\n6.1.2 Node Neighborhood Union\nSometimes we may be interested in the total number of other people that two nodes are connected to, regardless of whether both of them are connected to them. Think of this as adding the set of people that you know with the set of people one of your friends knows, counting the people that your friend knows but you don’t, and the people you know but your friend doesn’t. This is called the union of the two node neighborhood sets.\nSo if A and D are both nodes in a graph, the union of their neighborhood sets gives us a list of the total number of other nodes in the graph either one is connected to.\nUsing set theory notation, this can be written as:3\n\\[\n    \\mathcal{N}(A) \\cup \\mathcal{N}(D) = \\{B, C, F\\}\n\\tag{6.1}\\]\nWhich says that nodes A and D have B C, and F as neighbors, but not necessarily common neighbors.\nAs we will see later, the intersection and union of the neighborhood sets can be used as a basis to construct measures of (structural) similarity between nodes in a graph."
  },
  {
    "objectID": "lesson-graphs-neighborhoods.html#node-neighborhoods-in-directed-graphs",
    "href": "lesson-graphs-neighborhoods.html#node-neighborhoods-in-directed-graphs",
    "title": "6  Nodes and their Neighborhoods",
    "section": "6.2 Node Neighborhoods in Directed Graphs",
    "text": "6.2 Node Neighborhoods in Directed Graphs\nJust like in undirected (simple) graphs, each node in a directed graph has a node neighborhood. However, because now each node can be the source or destination for a asymmetric edges, this means that we have to differentiate the neighborhood of a node depending on whether the node is the sender or the recipient of a given link.\nSo, we say that a node j is an an in-neighbor of a node i if there is a directed link with j as the source and i as the destination node. For instance, in Figure 4.2, E is an in-neighbor of C, because there’s a asymmetric edge with E as the source and C as the destination.\nIn the same way, we say that a node i is an out-neighbor of a node j if there is a directed link with i as the source and j as the destination. For instance, in Figure 4.2, F is an in-neighbor of G, because there’s a asymmetric edge with G as the source and F as the destination\nFor each node, the full set of in-neighbors forms the in-neighborhood of that node. This is written \\(N^{in}(v)\\), where \\(v\\) is the label corresponding to the node. For instance, in Figure 4.2, the node set \\(N^{in}(D) = \\{B, E, G\\}\\) is the in-neighborhood of node D.\nIn the same way, the full set of in-neighbors defines the out-neighborhood of that node. This is written \\(N^{out}(v)\\), where \\(v\\) is the label corresponding to the node. For instance, in Figure 4.2, the node set \\(N^{out}(B) = \\{A, C, D\\}\\) is the out-neighborhood of node B.\nNote that typically, the set of in-neighbors and out-neighbors of a given node will not be exactly the same, and sometimes the two sets will be completely disjoint (they won’t share any members).\nNodes will only show up in both the in and out-neighborhood set when there are reciprocal or mutual ties between the nodes. For instance, in Figure 4.2, the out-neighborhood of node F is \\(\\{A\\}\\) and the in-neighborhood is \\(\\{A, G\\}\\). Here node A shows up in both the in and out-neighborhood sets because A has a reciprocal tie with F."
  },
  {
    "objectID": "lesson-graphs-neighborhoods.html#references",
    "href": "lesson-graphs-neighborhoods.html#references",
    "title": "6  Nodes and their Neighborhoods",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lesson-graphs-degree.html#node-degree-in-undirected-graphs",
    "href": "lesson-graphs-degree.html#node-degree-in-undirected-graphs",
    "title": "7  Nodes and their Degrees",
    "section": "7.1 Node Degree in Undirected Graphs",
    "text": "7.1 Node Degree in Undirected Graphs\nIn an undirected graph, a given node’s degree can be defined in two ways, both of which lead to the same answer.\nOne way to think about the degree of a given node \\(i\\) in a graph (written \\(k_i\\)) is as the cardinality of the set of neighbors of that node as defined earlier:\n\\[\n  k_i = |\\mathcal{N}(i)|\n\\tag{7.1}\\]\nSo in the graph shown in Figure 6.1:\n\\[\nk_A = |\\mathcal{N}(A)| = |\\{B, C, D, F\\}|=4\n\\tag{7.2}\\]\nAnother way to think about node degree is not as the cardinality of the node neighborhood set, but as a count of edges. In this case, we count the number of edges that have a given node \\(i\\) as one of their endpoints. Recall, that an edge that has a given node as one of their endpoints is said to be incident upon that node. So in the graph shown in Figure 6.1, the set of edges that have node A as one of their endpoints is:\n\\[\nk_A = \\{AB, AC, AD, AF\\}\n\\]\nWhich means that:\n\\[\n|k_A| = 4\n\\] Either way, computing degree as the cardinality of the node’s neighbor set or as the number of edges incident upon the node, gives us the number of other people that a given node is connected to in the network. We will see in Chapter 20, that this is an important measure of node position called degree centrality (Freeman 1977).\nIn a graph, nodes that have a degree equal to one, and thus have just a single neighbor in the graph, are called endpoints of the graph. Thus, in Figure 6.1, node \\(C\\) is an endpoint."
  },
  {
    "objectID": "lesson-graphs-degree.html#node-degree-in-directed-graphs",
    "href": "lesson-graphs-degree.html#node-degree-in-directed-graphs",
    "title": "7  Nodes and their Degrees",
    "section": "7.2 Node Degree in Directed Graphs",
    "text": "7.2 Node Degree in Directed Graphs\nBecause in a directed graph, each node has two distinct set of neighbors, we can compute two versions of degree for the same node.\n\nin a directed graph, for any node i, we can count the number of edges that have a given node \\(v\\) as their destination node. This is also the cardinality of the in-neighborhood set of that node. This is called a node’s indegree and it is written \\(k^{in}_i\\), where i is the label corresponding to that node.\nAdditionally, in a directed graph, for any node i, we can count the number of edges that have a given node \\(i\\) as their source node. This is also the cardinality of the out-neighborhood set of that node. This is called that node’s outdegree and it is written as \\(k^{out}_i\\), where i is the label corresponding to that node.\n\nFor instance, in Figure 4.2, \\(k^{out}_B = 3\\) and \\(k^{in}_B = 2\\). Node B has three outgoing ties (from nodes A, C, and D) and three incoming ties (from nodes A and D).\nCan you calculate what the indegree and outdegree of node D in Figure 4.2 is?\nThe graph theoretic ideas of indegree and outdegree have clear sociological interpretations. In a social network, for instance, a node having a large outdegree could indicate a sociable person (a person that likes to connect with others), while having a large indegree can indicate a popular person (e.g., a person lots of other people want to be friends with). In a later lesson we will see how to use a directed graph’s asymmetric adjacency matrix to readily compute the outdegree and indegree in real social networks."
  },
  {
    "objectID": "lesson-graphs-degree.html#references",
    "href": "lesson-graphs-degree.html#references",
    "title": "7  Nodes and their Degrees",
    "section": "References",
    "text": "References\n\n\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#sec-degset",
    "href": "lesson-graphs-degree-metrics.html#sec-degset",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.1 The Graph Degree Set",
    "text": "8.1 The Graph Degree Set\nComputing the degree of each node in the network gives us a vector (called k), containing the degree of each node. The vector is of “length” n where this is the number of nodes in the network. This is called the graph’s degree set, written k.\nWhat is a vector? A vector is a sequence of numbers. Thus, \\((1, 2, 3, 4)\\) is a vector, and so is \\((0, 1, 1, 0, 1, 1, 1)\\) and \\((0.23, 0.39, 0.89)\\). The length of a vector is the numbers of elements in it. Thus, the length of the vector \\((1, 2, 3, 4)\\) is 4 and the the length of the vector \\((0, 1, 1, 0, 1, 1, 1)\\) is 7. When vectors are considered as sets of numbers, the length of the vector is equivalent to the cardinality of the set defined by the vector.\nAs you may have already figured out, there are as many members of this set as there are nodes in the graph, so the cardinality of the degree set is the same as that of graph’s node set \\(|\\mathbf{k}| = |V|\\).\n\n\n\n\n\nFigure 8.1: A undirected graph\n\n\n\n\nLet’s consider the graph shown in Figure 8.1 again. The graph’s degree set k is shown in Table 8.1.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 8.1: The degree set of an undirected graph."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#the-graph-degree-sequence",
    "href": "lesson-graphs-degree-metrics.html#the-graph-degree-sequence",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.2 The Graph Degree Sequence",
    "text": "8.2 The Graph Degree Sequence\nWhen we list the members of the degree set (each node’s degree) in decreasing order from bigger to smaller, this is called the graph’s degree sequence, and it is written d. Every graph has its own degree sequence, but graphs with very different structure (in terms of other graph metrics) can have the same degree sequence (Kim et al. 2009).\nIt is easy to see that if we order the values of the degree set from higher to lower, we would obtain the following degree sequence:\n\n\n\n\n\n\n\n5\n4\n4\n4\n3\n3\n3\n3\n3\n\n\n\nTable 8.2: The degree sequence of an undirected graph."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#minimum-and-maximum-degree",
    "href": "lesson-graphs-degree-metrics.html#minimum-and-maximum-degree",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.3 Minimum and Maximum Degree",
    "text": "8.3 Minimum and Maximum Degree\nWhen studying a network, we are usually interested in the range of connectivity of people. What is the most connections somebody has in the network? What is the smallest set of neighbors someone has? These two graph metrics are called the maximum and minimum degree respectively. And are written \\(k_{max}\\) and \\(k_{min}\\). They are given by taking the maximum or the minimum values of the graph degree set describing the network. Thus if \\(\\mathbf{k}\\) is the vector containing each node’s degree, then the minimum and maximum degrees are given by:\n\\[\n  k_{max} = max(\\mathbf{k})\n\\tag{8.1}\\]\n\\[\n  k_{min} = min(\\mathbf{k})\n\\tag{8.2}\\]\nWhere \\(max(\\mathbf{k})\\) says “give me the largest number in the vector k, and \\(min(\\mathbf{k})\\) says”give me the smallest number in the vector k. To go back to our running example of the graph shown in Figure 8.1, it is easy to see from the degree set shown in Table 8.1, that for this network, \\(k_{max} = 5\\), and \\(k_{min} = 3\\)."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#degree-range",
    "href": "lesson-graphs-degree-metrics.html#degree-range",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.4 Degree Range",
    "text": "8.4 Degree Range\nThe arithmetic difference between \\(max(\\mathbf{k})\\) and \\(min(\\mathbf{k})\\) gives us a sense of the heterogeneity or gap between the connectivity of the best connected and the least well connected nodes in the graph. This is called the graph’s degree range, and it is written \\(k_r\\):\n\\[\nk_r = k_{max} - k_{min}\n\\tag{8.3}\\]\nIn this case, \\(k_r = 5 - 3 = 2\\), which tell us than in this graph, the difference between the best and most poorly connected nodes is not really that large. Most nodes have a fair number of links to others. In real world networks, like the graph in a social medial platform like Meta, the degree range can be pretty big, since the maximum degree can be in the millions of followers, and the minimum degree can be zero."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#sec-degsum",
    "href": "lesson-graphs-degree-metrics.html#sec-degsum",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.5 The Sum of Degrees",
    "text": "8.5 The Sum of Degrees\nNote that if we know the degree set of an undirected graph, we can figure out how many total edges there are in the graph. The reason for this is that the degree of each node can be defined in terms of the total number of edges incident on the node (as we noted earlier). So that means that if we know each node’s degree, then we can also know how many total edges there are in the graph.\nThe first step we need to do is compute the sum of degrees of each node in the graph. This quantity is written \\(\\sum_i k_i\\). For the degree set of the graph shown in Figure 8.1, we can see that the sum of the degrees \\(\\sum_i k_i = 32\\) (go ahead use a calculator to add up all the numbers in Table 8.1).\nIf you go back to the graph shown in Figure 8.1 and count the number of lines, you can see that there are 16. So summing up the degrees of each node ended up totaling twice the number of edges shown in the graph: \\(16 \\times 2 = 32\\). The reason for this is that each edge is incident (touches) two nodes, so it makes sense that when you add up the degrees of each node you end up with twice the amount of lines shown in the visual representation of the graph.\nSo the relationship between the sum of the degrees and the number of edges in an undirected graph is one of simple doubling. In equation form, this is written:\n\\[\n  \\sum_i k_i = 2m\n\\tag{8.4}\\]\nWhere m is the cardinality of the graph’s edge set \\(|E|\\). Some people call this the first theorem of graph theory (Benjamin, Chartrand, and Zhang 2015, 20)!\nAnother way to think about why we end up with twice the number of edges, is that in an undirected graph \\(G\\), if node A is a member of the set of neighbors of node B, then node B is necessarily also a member of the set of neighbors of node A. Thus, when computing the degree of each node, the edge AB does double duty, counting for the computation of both node A and node B’s degree scores, when we sum the degrees the edge thus shows up twice. Note that in the directed case we don’t have that problem, in which case \\(\\sum_i k_i = m\\)."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#graphical-versus-non-graphical-degree-sequences",
    "href": "lesson-graphs-degree-metrics.html#graphical-versus-non-graphical-degree-sequences",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.6 Graphical versus Non-Graphical Degree Sequences",
    "text": "8.6 Graphical versus Non-Graphical Degree Sequences\nEquation 8.4 has an interesting consequence that may not be immediately obvious. Putting our high-school algebra thinking cap on, we can solve for \\(m\\) and this will give us:\n\\[\n  m = \\frac{\\sum_i k_i}{2}\n\\tag{8.5}\\]\nWhich tells us that the number of edges in a graph is the sum of the degrees of each node divided by two. However, this also tell us something else: For any undirected graph, the sum of the degrees of each node will always be an even number. That’s because we know that \\(m\\) is an integer (the number of edges in a graph can’t be a decimal), so that means that the numerator of the Equation 8.5, namely, the sum of degrees \\(\\sum_i k_i\\) is divisible by two, and any number that is divisible by two (like 10, 24, 48, 120, etc.) is an even number. Ergo, the sum of degrees in a graph, \\(\\sum_i k_i\\), is always an even number.\nThis last result, that the sum of degrees of a graph has to be even, has yet another not so obvious consequence. Consider the following two sums:\n\\[\n4 + 3 + 2  + 1 + 2 + 1 + 3 = 16\n\\tag{8.6}\\]\n\\[\n4 + 3 + 3  + 1 + 2 + 1 + 3 = 17\n\\tag{8.7}\\]\nThe numbers in Equation 8.6 add up to sixteen, which is an even number. So we know automatically, that \\(\\{4, 3, 3, 2, 2, 1, 1\\}\\) could be the degree sequence of a possible graph with seven nodes (\\(n = 7\\)) and \\(m = 16 \\div 2 = 8\\) edges. In fact, one such possible graph is shown in Figure 8.2.\nHowever, we also know that since the sum shown in Equation 8.7 is an odd number, it cannot be the sum of degrees of any possible graph whatsoever. No undirected graph with seven nodes can have the degree sequence \\(\\{4, 3, 3, 3, 2, 1, 1\\}\\).\nIn graph theory terms, we say that the degree sequence \\(\\{4, 3, 3, 2, 2, 1, 1\\}\\) is graphical (could be the degree sequence of a possible graph), while the degree sequence \\(\\{4, 3, 3, 3, 2, 1, 1\\}\\) is not graphical.\n\n\n\n\n\nFigure 8.2: An undirected graph with degree sequence k = {4, 3, 3, 2, 2, 1, 1}.\n\n\n\n\nWhat’s the difference between the graphical sequence \\(\\{4, 3, 3, 2, 2, 1, 1\\}\\) and the not graphical sequence \\(\\{4, 3, 3, 3, 2, 1, 1\\}\\)? Time to find out.\nIn a graph, let us call a node with a degree that is an even number an even node. Let’s call a node with a degree that is an odd number (you guessed it) an odd node. You will notice that in the graphical degree sequence, there are four odd nodes \\(\\{3, 3, 1, 1\\}\\) and three even nodes \\(\\{4, 2, 2\\}\\). The not graphical degree sequence has five odd nodes \\(\\{3, 3, 3, 1, 1\\}\\) and two even nodes \\(\\{4, 2\\}\\).\nSo the number of odd nodes in the graphical sequence is even (four) and the number of odd nodes in the not graphical sequence is odd (five). Is this a coincidence? Turns out not!\nIn every graphical degree sequence, the number of odd nodes has to be an even number. That is to say, if a sequence of numbers has an odd number of odd numbers, then it cannot be the degree sequence of any possible graph! We could have checked because any sequence of numbers that has an odd number of odd numbers will sum to an odd number and we already know that a sequence of numbers that sums to an odd number is not graphical.\nThe number of even nodes, on the other hand, as we saw before can be odd and the sequence is still graphical. That’s because any set of even numbers, odd or even, will sum to an even number. So what matters is the number of odd nodes, which has to be even for a degree sequence to describe possible graph (Buckley and Harary 1990, 3). Heady!\nKnowing the link between the sum of degrees and the number of edges, we can begin to calculate some other important graph metrics."
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#the-average-degree",
    "href": "lesson-graphs-degree-metrics.html#the-average-degree",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.7 The Average Degree",
    "text": "8.7 The Average Degree\n\n8.7.1 Average Degree in Undirected Graphs\nOnce we know what sum of the degrees is, we can compute an important graph metric called the average degree. This gives us a sense of whether there are lots of well-connected nodes in the graph (in which case the average degree is high), or whether the typical node in the graph only has a small number of neighbors (in which case the average degree is low).\nTo get the average degree for a graph, written \\(\\bar{k}\\), all we need to do is compute the sum of degrees (like earlier) and divided it by the total number of nodes in the graph:\n\\[\n  \\bar{k} = \\frac{\\sum_i k_i}{n}\n\\tag{8.8}\\]\nIn an undirected graph G like the one shown in Figure 8.1, the graph average degree is given by twice the total number of symmetric ties (the cardinality of the edge set as defined earlier) divided by the total number of nodes (the cardinality of the node node set). Because of the identity identified in Equation 8.4, where \\(\\sum_i k_i = 2m\\), we can also write the formula for the average degree as follows by switching the numerator:\n\\[\n  \\bar{k} = \\frac{2m}{n}\n\\]\nFor Figure 8.1, the average degree of the graph is:\n\\[\n  \\bar{k} = \\frac{2m}{n} = \\frac{2 \\times 16}{9} = \\frac{32}{9} = 3.6\n\\tag{8.9}\\]\n\n\n8.7.2 Average Degree in Directed Graphs\nWe can use a variation of the same procedure to compute the average degree in directed graphs. Take the directed graph shown in Figure 4.2, which has seven nodes and 11 edges. In this case, there is no need to “double” the number of lines as in the case of an undirected graph.\nWe could compute either the average indegree or the average outdegree of each node and it would give us the same answer, because each link unless does single duty in determining either the outdegree or indegree of each node. Accordingly, the average degree in the directed case is:\n\\[\n  \\bar{k} = \\frac{m}{|V|} = \\frac{m}{n} = \\frac{ 11}{7}=1.6\n\\tag{8.10}\\]\nEven easier!\nAlthough straightforward, the graph average degree provides a powerful tool to analyze the social world. For example, if we have two school clubs of the same size and we ask students who they are friends with in the club, we might get very different average degrees. Let us assume that the average degree in the first network is two, while in the second network it is five.\nThis statistic informs us that people in the second network have more friends within the group than in the first network. If we are interested in why the first group failed and the second group kept meeting, we might understand that the underlying social relations of friendship, which might be theorized as contributing to the clubs survival, were weaker in the first group to begin with than they were in the second group. We are thus able to gain insight into the causes and/or underlying conditions that shape the social world.\n\n\n8.7.3 The Connection Between Density and Average Degree\nThere is an intimate mathematical relationship between the graph density and the graph average degree. Let’s see what it is.\nTake another look at Equation 5.5. It says that density is equal to:\n\\[\n  d(G)^u = \\frac{2m}{n(n-1)}\n\\tag{8.11}\\]\nWe can write the same equation as the product of two different fractions. Like this:\n\\[\n  d(G)^u = \\frac{2m}{n} \\times \\frac{1}{n-1}\n\\tag{8.12}\\]\nNow, take a look at the first (left-hand) fraction of this product. Does it look familiar? It should, because it is the formula for the average degree we wrote down in Equation 8.8!\nSo that means that if we know the average degree and we know the number of nodes in a graph, we also know the density of the graph without knowing anything about the number of edges. Mathematical magic.\nHow do we do that? Well, substituting the fraction \\(\\frac{2m}{n}\\) for its equivalent, the average degree \\(\\bar{k}\\), in Equation 8.12 we get:\n\\[\n  d(G)^u = \\bar{k} \\times \\frac{1}{n-1} =  \\frac{\\bar{k}}{n-1}\n\\tag{8.13}\\]\nSo this formula tells us that the density of a graph is equivalent to its average degree divided by the number of nodes minus one. Pretty simple!"
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#advanced-degree-metrics-topics",
    "href": "lesson-graphs-degree-metrics.html#advanced-degree-metrics-topics",
    "title": "8  Degree-Based Graph Metrics",
    "section": "8.8 Advanced Degree Metrics Topics",
    "text": "8.8 Advanced Degree Metrics Topics\n\n8.8.1 Degree Variance\nOnce we know the average degree of a graph, it is possible to compute more complex measures of the heterogeneity in connectivity across nodes (e.g., the extent to which there is a very big spread between well-connected and not so well-connected nodes in the graph) beyond the simpler measures of range such as the difference between \\(k_{max}\\) and \\(k_{min}\\).\nOne such measure was proposed by the sociologist and statistician Tom Snijders in a paper written in 1981 (Snijders 1981). It is called the degree variance of the graph. It is written \\(\\mathcal{v}(G)\\) and it is defined as the average squared deviation between the degree degree of each node and the average degree:\n\\[\n  \\mathcal{v}(G) = \\frac{\\sum_i (k_i - \\bar{k})^2}{n}\n\\tag{8.14}\\]\nThe equation says that to compute the degree variance, first we create a vector of the square of the difference between each node’s degree and the average degree of the graph \\((k_i - \\bar{k})^2\\), then we sum all the values in this vector (note that since we are squaring all these values will be a positive number), and divide the result by the total number of nodes \\(n\\).\nTo compute the degree variance of the graph shown in Figure 8.1 using Equation 8.14, we can thus go through the following steps.\n\nFirst we compute the vector of differences between each node’s degree \\(k_i\\) (shown in Table 8.1) and the average degree \\(\\bar{k}\\) (already computed in Equation 8.9). This yields the following vector:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.44\n-0.56\n0.44\n1.44\n-0.56\n0.44\n-0.56\n-0.56\n-0.56\n\n\n\nTable 8.3: Differences between the degree of each node and the average degree.\n\n\n\nThen we square each of these numbers \\((k_i - \\bar{k})^2\\), which results in the following vector of squared deviations:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.2\n0.31\n0.2\n2.09\n0.31\n0.2\n0.31\n0.31\n0.31\n\n\n\nTable 8.4: Squred differences between the degree of each node and the average degree.\n\n\n\nThen we compute the sum of all these numbers \\(\\sum_i (k_i - \\bar{k})^2\\), which is 4.2.\nFinally, we divide this sum by the number of nodes in the graph, which gives us:\n\n\\[\n  \\mathcal{v}(G) = \\frac{4.2}{9} = 0.47\n\\]\n\n\n\n\n\nFigure 8.3: A 3-regular undirected graph of order 10\n\n\n\n\nThe degree variance is supposed to capture the extent to which there is inequality in the connectivity of nodes in a graph. Inequality exists when a few nodes have a lot of connections and most nodes only have a few. The more inequality, the higher the degree variance. This means that in graphs where there is very little variation in the connectivity of each node (all nodes have roughly the same number of connections), the degree variance should be at a minimum.\n\n\n8.8.2 Regular Graphs\nThe most extreme version of the low inequality case is shown in Figure 8.3. This graph has ten nodes, and every node has the same degree \\(k = 3\\), which means that the average degree is also the same number \\(\\bar{k} = 3\\). This is called a regular graph.\nBecause in a regular graph, every node has the same degree, we refer to regular by their node degree (\\(k\\)) and their order (\\(n\\)) as k-regular graphs of order n. So, the example shown in Figure 8.3, where every node has degree equal to three (\\(k = 3\\)) and there are ten nodes (\\(n = 10\\)) is a three-regular graph of order ten. Because \\(k - \\bar{k} = 3 - 3 = 0\\) for all nodes in the regular graph, The degree variance of a regular graph is always zero (because the numerator of Equation 8.14 will always be zero).\n\n\n8.8.3 Node Average Nearest Neighbor Degree\nJust in the same way we can compute the degree of each node, we may be interested in in the question of whether nodes in the network tend to connect to others of high degree, or whether the connections in the network occur at random, irrespective of degree. The first situation, where people tend to connect to other people of high degree is called preferential attachment in network science (Barabási and Albert 1999). It is also referred to as a popularity tournament structure in sociology (Waller 1937). If you went to a real live high school, you may know how this works.\nTo get a sense of whether a given node prefers to connect to others who also have a large number of connections, we can compute an index called the average nearest neighbor degree, which is conventionally written \\(\\bar{k}_{nn(i)}\\).\nThis is given by the following formula:\n\\[\n  \\bar{k}_{nn(i)} = \\frac{1}{k_i} \\sum_{j \\in \\mathcal{N(i)}} k_j\n\\tag{8.15}\\]\nThe formula says that to compute the \\(k_{nn}\\) of node i we take sum of the degrees of each of their neighbors j (remember that \\(j \\in \\mathcal{N(i)}\\) means “as long as node j belongs to the set \\(N(i)\\)”), and then divide it by the degree of node i.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n3.8\n4.3\n3.8\n3.6\n4.3\n3.5\n3.3\n3.3\n3.3\n\n\n\nTable 8.5: Average nearest neighbor degree of nodes in an undirected graph\n\n\nAs an example, the \\(\\bar{k}_{nn(i)}\\) for each node in the undirected graph shown in Figure 8.1 are shown in Table 8.5. This is the vector of average nearest neighbor degrees for the graph \\(k_{nn}\\). The table tells us that, nodes B and and E seem to have the strongest tendencies to connect to others who are also popular, while G, H, and I display the weakest such tendencies.\nBut can do we know if a \\(\\bar{k}_{nn(i)} = 4.3\\) is a big number or a small number? Well, we can compare each node’s \\(\\bar{k}_{nn(i)}\\) to the average degree (\\(\\bar{k}\\)) as defined previously. If a node’s \\(\\bar{k}_{nn(i)}\\) is bigger than the average degree of the graph, then we can say that their neighbors are more likely to be popular than expected by chance. For instance, we know, from the calculation shown in Equation 8.9, that the average degree for this graph is 3.6, so a \\(\\bar{k}_{nn(i)} = 4.3\\) tells us that a node connects to others that are (on average) more popular than the average person.\n\n\n8.8.4 The Graph’s Average Nearest Neighbor Degree\nIf we take the average of the average nearest neighbor degree vector for each node shown in Table 8.5, this would give us the graph’s average nearest neighbor degree (\\(\\bar{k}_{nn}\\)). But what could this possibly mean? Well, this would tell us the typical number of friends that the typical friend of the typical node in the graph has! Essentially, the expected number of friends of friends of the average person.\nIn a mind-bending paper, the sociologist Scott Feld (1991) showed, that with very few exceptions (e.g., regular graphs), the average number of friends of the typical person (the graph’s average degree) is smaller than the average number of friends of friends of the typical person in the same social network (the graph’s average nearest neighbor degree). That is, your friends have more friends than you do. Don’t get depressed. This is just how social networks work.\nCheck it out for yourself! If we compute the graph’s average nearest neighbor degree from the numbers in the vector shown in Table 8.5, this would give us \\(\\bar{k}_{nn} = 3.7\\) which is larger than the same graph’s average degree we computed earlier (\\(\\bar{k} = 3.6\\)). Weird!"
  },
  {
    "objectID": "lesson-graphs-degree-metrics.html#references",
    "href": "lesson-graphs-degree-metrics.html#references",
    "title": "8  Degree-Based Graph Metrics",
    "section": "References",
    "text": "References\n\n\n\n\nBarabási, Albert-László, and Réka Albert. 1999. “Emergence of Scaling in Random Networks.” Science 286 (5439): 509–12.\n\n\nBenjamin, Arthur, Gary Chartrand, and Ping Zhang. 2015. The Fascinating World of Graph Theory. Princeton University Press.\n\n\nBuckley, Fred, and Frank Harary. 1990. Distance in Graphs. Addison-Wesley.\n\n\nFeld, Scott L. 1991. “Why Your Friends Have More Friends Than You Do.” American Journal of Sociology 96 (6): 1464–77.\n\n\nKim, Hyunju, Zoltán Toroczkai, Péter L Erdős, István Miklós, and László A Székely. 2009. “Degree-Based Graph Construction.” Journal of Physics A: Mathematical and Theoretical 42 (39): 392001.\n\n\nSnijders, Tom AB. 1981. “The Degree Variance: An Index of Graph Heterogeneity.” Social Networks 3 (3): 163–74.\n\n\nWaller, Willard. 1937. “The Rating and Dating Complex.” American Sociological Review 2 (5): 727–34."
  },
  {
    "objectID": "lesson-graphs-paths.html#sec-paths",
    "href": "lesson-graphs-paths.html#sec-paths",
    "title": "9  Indirect Connections",
    "section": "9.1 Paths",
    "text": "9.1 Paths\nWhat is a path? The best way to begin is with an example. Take for instance, the undirected graph shown in Figure 9.1. In the Figure, nodes A and B are not adjacent (they are part of a null dyad). However, A can still reach B using a graph-theoretic path, written \\(A-B\\). For instance, if A wanted to pass along a message to B without it having to go through the same person twice, they could tell D (using the edge \\(A \\leftrightarrow D\\)), who could tell E (using the edge \\(D \\leftrightarrow E\\)) who could the tell B (using the \\(E \\leftrightarrow B\\) edge). As noted above, this sequence of edges, namely \\(A \\leftrightarrow D, D \\leftrightarrow E, E \\leftrightarrow B\\) defines a path between nodes A and B.\nTo define a path, we could also write the names of the edges separated by commas while omitting the arrows as in: \\(A-B = \\{AD, DE, EB\\}\\). In a path, the first node listed in the sequence is called the origin node and the last node listed is called the destination node. The nodes “in between” the origin and destination nodes in the path are called the inner nodes. Together, the origin and destination nodes are referred to as the end nodes of the path. So in the path \\(A-B = \\{AD, DE, EB\\}\\), node A is the origin node, node B is the destination node, nodes D and and E are the inner nodes, and both nodes A and B are the end nodes.\nSo what is a path?\nIn a graph, a path between two nodes A and B is a sequence of nodes and edges, such that the sequence begins with node A and ends with node B and only goes through each of the inner nodes in the path once.\nNote that every path is actually a subgraph of the original graph as defined in Chapter 3! Except that the subgraph that defines a path will always contain a sequence of vertices and edges connected in a line."
  },
  {
    "objectID": "lesson-graphs-paths.html#properties-of-paths",
    "href": "lesson-graphs-paths.html#properties-of-paths",
    "title": "9  Indirect Connections",
    "section": "9.2 Properties of Paths",
    "text": "9.2 Properties of Paths\nIn graph theory, paths between pairs of nodes have some unique properties:\n\nFirst, as mentioned in the definition, paths do not repeat nodes. This means that each of inner nodes in the path (e.g., the nodes that are not the origin and destination ones) only appears exactly twice when we list the edges that make up the path, like nodes D and E in the AB in the path listing \\((AD, DE, EB)\\). The origin and destination nodes, in contrast, appear exactly once.1\nSecond, while it might not seem immediately obvious, because paths do not repeat nodes, they also do not repeat edges. As we will see later, this property helps differentiate paths from other less restricted types of edge sequences we may define in a graph featuring two nodes at the ends of it.\n\nThird, paths are characterized by their length. The length of a path (\\(\\mathcal{l}_{ij}\\), where \\(i\\) is the origin node and \\(j\\) is the destination node) is given by the number of edges included in it. So the length of the path \\((AD, DE, EB)\\) is \\(\\mathcal{l}_{AB} = 3\\) because there are three edges in the path.2\nFinally, there may be multiple paths connecting the same pair of nodes. For instance, In Figure 9.1 node A can also reach node B via the path \\((AC, CE, EB)\\) which is distinct from the one shown in red the Figure, but which also counts as a proper graph theoretic path (intervening nodes only appear twice in the listing, and the end nodes only appear once)."
  },
  {
    "objectID": "lesson-graphs-paths.html#pairwise-connectivity-and-reachability",
    "href": "lesson-graphs-paths.html#pairwise-connectivity-and-reachability",
    "title": "9  Indirect Connections",
    "section": "9.3 Pairwise Connectivity and Reachability",
    "text": "9.3 Pairwise Connectivity and Reachability\nThis leads us to the graph theory definition of connectivity for pairs of nodes:\n\nIn a graph, two nodes A and B are connected if there is at least one path (of any length), featuring node A as the origin node and node B as the destination node. Otherwise, the two nodes A andB are disconnected.\nWhen a node can indirectly connect to another node via a path, we say that that the origin node can reach the destination node, or that the destination node is reachable by the origin node.\n\nNote that once we understand the graph theory notion of connectivity, it becomes clear that the concept of adjacency (see Chapter 2) is a special (limiting) case of connectivity: Two nodes i and j are adjacent when there is a path of \\(\\mathcal{l_{ij}} = 1\\) between them! Another way of saying this is that connectivity is a generalization of the notion of adjacency to allow for paths of length longer than one.\nIn a simple graph with no isolates, like that shown in Figure 9.1, it is easy to see that all node pairs are connected via a path of some length."
  },
  {
    "objectID": "lesson-graphs-paths.html#shortest-paths",
    "href": "lesson-graphs-paths.html#shortest-paths",
    "title": "9  Indirect Connections",
    "section": "9.4 Shortest Paths",
    "text": "9.4 Shortest Paths\nConsider the multiple ways node B can reach node C via a path in Figure 9.1. One possibility is the path defined by the edge sequence \\((BE, ED, DA, AC)\\). Another possibility is the path defined by the sequence \\((BE, ED, DC)\\). Yet another possibility is the path given by the edge sequence \\((BE, EC)\\). What’s the difference between these paths?\nWell, for the first one, \\(\\mathcal{l}_{BC}^{(1)} = 4\\), for the second one \\(\\mathcal{l}_{BC}^{(2)} = 3\\) and for the last one \\(\\mathcal{l}_{BC}^{(3)} = 2\\). The three paths are of different length, even though all three are proper graph-theoretic paths (they do not repeat inner nodes) and even though all three feature the same actors as the end nodes. Note that the length of the smallest possible shortest path between two non-adjacent vertices is always two!\nLet’s say B was a spy who needed to send an urgent message to C even though they don’t know C directly. If B wanted the message to get to C in the fastest way, they would use the shortest path between them, which in this case is \\((BE, EC)\\). For any two nodes in the network, the shortest path is the smallest existing path (in terms of path length) that has those nodes as the end nodes. This means that for every pair of connected actors in the network, we can define a shortest path between them. The shortest paths between pairs of nodes are also called geodesics.\nSometimes, as with actors A and B in Figure 9.1, there will be multiple shortest paths between two pairs of nodes, because we end up with two or more paths that are “tied” in length (and thus all count as “shortest” paths). So for nodes A and B in Figure 9.1, the two shortest paths connecting them are of length three: \\((AC, CE, EB)\\) and \\((AD, DE, EB)\\).\n\n9.4.1 Paths and Intermediation in Networks\nObserve that, in Figure 9.1, if B really wanted to reach C via the shortest path, they will always have to go through node E. When this is the case, we say that E stands in the shortest path between B and C because it is an inner node in that path. So B is highly dependent on E to reach C. This makes E’s position in the network particular important for B, because they play the role of go-between or broker between B and the other actors in the network (Marsden 1983).\nBecause of this connection to communication efficiency and inter-mediation, shortest paths figure prominently in various measures of node position, called centrality measures we will deal with in Chapter 20. One such measure, called betweenness centrality is based on counting the number of shortest paths between every other pair of nodes that a given node stands on.\nThe length of the shortest path between two pair of non-adjacent vertices in the network can be thought of as the “degrees of separation” between them. This is also called the geodesic distance between two nodes. For instance, the shortest path between nodes A and B is of length three, so A is three degrees of separation away from B. As we saw in this example, there does not have to be only one shortest path between two nodes. Actors in the network can be connected via multiple distinct shortest paths as are A and B in Figure 9.1."
  },
  {
    "objectID": "lesson-graphs-paths.html#sec-cycles",
    "href": "lesson-graphs-paths.html#sec-cycles",
    "title": "9  Indirect Connections",
    "section": "9.5 Cycles",
    "text": "9.5 Cycles\nConsider the edge sequence \\((AD, DE, EC, CA)\\) highlighted in red in Figure 9.2. What’s so special about it? Well, it looks like a path, because the inner nodes are only listed twice and so all the edges are unique. However, both the origin and destination nodes are the same!\nSomething like this sometimes happens with gossip. You start a rumor about someone by telling somebody else, and then a third person tells you the rumor that you started as if it was news to you! In this case, the rumor has traveled in the network via what is called a cycle.\n\n\n\n\n\nFigure 9.2: An undirected graph showing a cyle beginning and ending with node A (red edges).\n\n\n\n\nA cycle is path,of length three or larger, featuring the same node in both the origin and destination slots. In an undirected graph, cycles of length three are also called triangles or closed triads (see ?sec-triads).\nSome directed graphs, are distinctive because they don’t have any cycles. It doesn’t matter how hard you try, or how long you stare at them, there is no way you will find a directed path of minimum length three that begins and ends with the same node.\nDirected graphs completely lacking in cycles are called directed acyclic graphs (DAGs). Note that tree graphs are by definition graphs that do not contain cycles (as discussed in ?sec-tree). Therefore every tree graph is also a DAG."
  },
  {
    "objectID": "lesson-graphs-paths.html#graph-properties-based-on-paths-and-cycles",
    "href": "lesson-graphs-paths.html#graph-properties-based-on-paths-and-cycles",
    "title": "9  Indirect Connections",
    "section": "9.6 Graph Properties Based on Paths and Cycles",
    "text": "9.6 Graph Properties Based on Paths and Cycles\nGiven what we have learned so far, we can characterize key properties of the whole graph depending on the range (minimum and maximum lengths) of the shortest paths and cycles between pairs of nodes in it, as well as the existence of given paths and cycles with special features:\n\nThe length of the longest geodesic (shortest path) between two nodes in a graph is called the graph diameter.\nThe length of the shortest geodesic (shortest path) between two nodes in a graph is called the graph radius.\nFor graphs that are not acyclic (e.g., they contain at least one cycle), the length of the longest cycle is called the graph circumference (Buckley and Harary 1990, 10).\nIn the same way, the length of the shortest cycle in a graph with cycles is called the graph girth (Buckley and Harary 1990, 10). For a graph with a single cycle, the circumference is always equal to the girth.\nA path that includes every node in a graph is called a Hamiltonian path (also called a spanning path). Obviously not all graphs will contain such a path, graphs that do contain a Hamiltonian path are called traceable graphs.\nA cycle that visits each node in the graph exactly once is called Hamiltonian cycle (also called a spanning cycle). Obviously not all graphs will contain such a cycle, graphs that do contain a Hamiltonian cycle are called (you guessed it) Hamiltonian graphs.3"
  },
  {
    "objectID": "lesson-graphs-paths.html#walks-and-trails",
    "href": "lesson-graphs-paths.html#walks-and-trails",
    "title": "9  Indirect Connections",
    "section": "9.7 Walks and Trails",
    "text": "9.7 Walks and Trails\nNot every sequence of nodes and edges that begins with one node and ends in another counts as a proper graph-theoretic path between two nodes.\nFor instance, looking at Figure 9.1, we could imagine a sequence of edges that started with one node and ended in another one without repeating edges, but repeating nodes. For instance, the sequence \\((CA, AD, DC, CE, ED)\\) has node C as the origin node and node D as the destination node, and all the edges in the sequence are unique. However, the origin node C and the destination node D also appear in the intervening chain, which means that they are listed three times. This fails the path test for this edge sequence. Sequences like this, featuring all unique edges but repeated nodes, are called trails.\nIn the same way, we could imagine some kind of message traveling across the edge sequence \\((AD, DC, CE, ED, DC, EC, EB)\\). This sequence has A as the origin node and has B as the destination node. However, the sequence goes through nodes C, D, and E twice; moreover, the edge DC appears twice, as does the edge EC. This means nodes C, D, and E appear four times in the listing which also fails the “is this a path?” test.\nArbitrary sequences of nodes and edges that begin with one node and end in another node but that feature both repeated nodes and edges along the way are called walks. If a walk begins and ends in the same node (like a cycle) then it is called a closed walk, otherwise, if a walk has different source and destination nodes, it is called an open walk (Buckley and Harary 1990, 10). The same distinction applies to trails. Note that a closed trail is the same as a cycle.\nWalks, trails, and paths, form a hierarchy of increasingly less restricted “travels” from one node to another via edges in a graph (Borgatti 2005). Walks are the least restricted (they can include both repeated nodes and edges) and paths are the most restricted (they cannot include repeated nodes, which by implication also means they can’t include repeated links). Trails are in between. They are forbidden from repeating edges but can use multiple nodes more than once.\nThis means all paths are trails, and are trails are walks, but not all walks are trails, and not all trails are paths!\n\n9.7.1 Keeping Walks and Trails Apart\nOne way to keep these distinctions straight is by thinking about different types of things that flow through a network and how they may use paths, trails, or walks (Borgatti 2005). Take for instance a virus like our (in)famous friend, Covid 19. A virus doesn’t decide where to go. It just gets transmitted from person to person every time pairs of people (or multiple pairs if it’s a mass gathering) come into contact with one another. The same person can become exposed to the virus multiple times via different links (so the chain repeats nodes). In the same way, the same link (your best friend) can expose you to the virus multiple times (so the chain repeats edges).\nThese considerations lead us to conclude that viruses form transmission chains in social networks that look like walks: they repeat both nodes and edges.\nIn this way, viruses are very much like (good old fashioned paper and coin) money even though you may not have thought of that connection before. If you buy something with a dollar, and the person you gave the dollar to buys something with it, the dollar can come back to you after flowing through the network for a while, so money also travels through networks via walks because it can repeat both nodes and edges.\n\n\n9.7.2 Milgram’s Small World Experiment\nHow about trails? We already mentioned the idea of “six degrees of separation.” In the 1960s, the social psychologist Stanley Milgram (1967) designed an experiment where he sent almost two hundred packages containing a letter to a bunch of random people in Nebraska. The letter had the name of a stockbroker who lived in Boston and instructions that read: “If you know this person and where they live, send them this package; if you don’t know this person, forward it to someone you know who you think might know this person.” This is called the small world experiment.\nThe letters in Milgram’s experiment thus began to flow through indirect connections in social networks. Any one “chain” of letters however would go from one link in the network to another. Theoretically it could come back to a person who had already sent the letter (if one of the people forward in the chain didn’t know that person had sent the letter before), but no one would forward the letter back to the same person they had forwarded the letter before. This means that the packages in the small world experiment traveled via trails. They could repeat nodes (e.g., people) but not edges.\nGossip spreads through the network via trails too. You can hear the same piece of gossip from different friends (meaning that you are a repeated node in the trail), but you won’t hear the same piece of gossip from the same friend (meaning that edges are not repeated), unless they had memory loss!\nSo the differences between the ways viruses and gossip travel via social networks should help you keep walks and trails apart."
  },
  {
    "objectID": "lesson-graphs-paths.html#references",
    "href": "lesson-graphs-paths.html#references",
    "title": "9  Indirect Connections",
    "section": "References",
    "text": "References\n\n\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nBuckley, Fred, and Frank Harary. 1990. Distance in Graphs. Addison-Wesley.\n\n\nMarsden, Peter V. 1983. “Restricted Access in Networks and Models of Power.” American Journal of Sociology 88 (4): 686–717.\n\n\nMilgram, Stanley. 1967. “The Small World Problem.” Psychology Today 2 (1): 60–67."
  },
  {
    "objectID": "lesson-graphs-directed-paths.html#mutual-reachability",
    "href": "lesson-graphs-directed-paths.html#mutual-reachability",
    "title": "10  Directed Indirect Connections",
    "section": "10.1 Mutual Reachability",
    "text": "10.1 Mutual Reachability\nIn directed graphs, some pairs of nodes can be mutually reachable. That is, there can be a directed path going from one node to the other, and vice versa. Figure 10.3 shows an example of this case involving nodes \\(B\\) and \\(C\\). Starting with origin node \\(B\\) we can get to \\(C\\) via the highlighted purple path of length \\(l_{BC} = 3\\) formed by the edges \\((BE, ED, DC)\\). In the same way, starting with node \\(C\\), we can get to node \\(B\\) via the separate, highlighted red path of length \\(l_{CB} = 3\\) formed by the edges \\((CD, DE, EB)\\).\nFor mutual reachability between two nodes to happen in a directed graph, the two paths do not have to be the same length in fact, in the case of nodes \\(B\\) and \\(C\\) node \\(C\\) can reach \\(B\\) via path shorter than length 3. This is shown in Figure 10.4), where we can see that \\(C\\) can reach \\(B\\) via the shortest path possible short of being directly connected \\((l_{CB} = 2)\\), formed by the edges \\((CE, EB)\\) (highlighted in red). On the other hand, \\(B\\) cannot reach \\(C\\) via a path shorter than 3.\n\n\n\n\n\nFigure 10.5: A directed graph showing a directed cycle starting and ending with node C."
  },
  {
    "objectID": "lesson-graphs-directed-paths.html#directed-cycles",
    "href": "lesson-graphs-directed-paths.html#directed-cycles",
    "title": "10  Directed Indirect Connections",
    "section": "10.2 Directed Cycles",
    "text": "10.2 Directed Cycles\nJust like in the undirected case, a directed path that begins and ends with the same node is called a directed cycle. We refer to different cycles by the number of directed edges they include. For instance a 3-directed cycle is a directed cycle with three directed edges, a 4-directed cycle is a directed cycle with four edges and so forth. For instance, in Figure 10.5, the directed 3-cycle features node \\(C\\) as both the origin and destination node involving the directed edges \\((CE, ED, DC)\\) is highlighted in red.\nA directed graph that does not contain any cycles is a special kind of directed graph (composed of anti-symmetric ties) called a directed acyclic graph.\n\n\n\n\n\nFigure 10.6: A directed graph showing two weakly connected nodes A and B."
  },
  {
    "objectID": "lesson-graphs-directed-paths.html#types-of-indirect-connections-in-directed-graphs",
    "href": "lesson-graphs-directed-paths.html#types-of-indirect-connections-in-directed-graphs",
    "title": "10  Directed Indirect Connections",
    "section": "10.3 Types of Indirect Connections in Directed Graphs",
    "text": "10.3 Types of Indirect Connections in Directed Graphs\nIn a directed graph, when pairs of nodes are mutually reachable they are also said to be strongly connected. Otherwise if nodes in a directed graph are connected via at least one directed path that only goes in one direction (like nodes A and B in Figure 10.3)), they are said to be unilaterally connected. Two nodes are said to be recursively connected when they are strongly connected and and at least one of the pairs of directed paths going in both directions use the same nodes (like paths \\((BE, ED, DC)\\) and \\((CD, DE, EB)\\) connecting nodes B and C in Figure 10.3)).\nFinally, in a directed graph, two nodes are weakly connected if we can trace a path from one to the other, but only by ignoring the direction of the arrows! For instance, Figure 10.6 shows a directed graph in which nodes A and B have a weak connection via the \\((BE, CE, AC)\\) path (highlighted in red), although this is not the only weak connection they share.\nCan you trace other weakly connected paths between nodes A and B in Figure 10.6)?"
  },
  {
    "objectID": "lesson-graphs-connectivity.html#connected-and-disconnected-graphs",
    "href": "lesson-graphs-connectivity.html#connected-and-disconnected-graphs",
    "title": "11  Graph Connectivity",
    "section": "11.1 Connected and Disconnected Graphs",
    "text": "11.1 Connected and Disconnected Graphs\nLook at Figure 9.1 again. Are there any disconnected pairs of nodes in the graph? The answer is no. If you pick any pair of nodes, they are either directly connected, or indirectly connected to one another via a path of some length. So how can we get two nodes to be disconnected in a simple undirected graph? The answer is that there has to be some kind of “gap” splitting the graph into two or more pieces, so that some set of nodes can no longer reach some of the other ones.\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) A disconnected graph.\n\n\n\n\nFigure 11.1: A connected graph versus a disconnected graph.\n\n\nFor instance, if you compare the undirected graphs Figure 11.1 (a) and Figure 11.1 (b) you can appreciate the difference between a connected graph and a disconnected graph. The difference between Figure 11.1 (a) and Figure 11.1 (b) is that the Figure 11.1 (b) graph is a subgraph of the Figure 11.1 (a) graph, in which the \\(DF\\) edge has been removed (an edge-deleted subgraph according to what we discussed in Chapter 3): \\(G(b) = G(a) - DF\\).\nRemoving the \\(DF\\) edge has resulted in \\(G(b)\\) being disconnected. There is no way we can trace a path connecting any of the vertices in the \\(\\{F, G, H, I\\}\\) subset with any of the vertices in the \\(\\{A, B, C, D, E\\}\\) subset. This is different from Figure 11.1 (a) where we can trace a finite path between any two vertices, even if it is very long. Thus, we can come up with a formal definition of graph connectivity:\n\nA graph is connected if there exists a (finite) path between each pair of nodes.\nA graph is disconnected if there does not exist a path connecting every pair of nodes\n\n\n\n\n\n\nFigure 11.2: A connected graph with three edges highlighted."
  },
  {
    "objectID": "lesson-graphs-connectivity.html#components-and-the-giant-component",
    "href": "lesson-graphs-connectivity.html#components-and-the-giant-component",
    "title": "11  Graph Connectivity",
    "section": "11.2 Components and the Giant Component",
    "text": "11.2 Components and the Giant Component\nWhen a graph goes from connected (like Figure 11.1 (a)) to disconnected (like Figure 11.1 (b)) it is split into a number of subgraphs that are themselves connected.\nFor instance, in Figure 11.1 (b) the set nodes \\(\\{A, B, C, D, E\\}\\) form a connected subgraph of the larger disconnected graph. A connected subgraph that is part of a larger disconnected graph is called a component.\nIn Figure 11.1 (b) there are two components, one formed by the connected subgraph with node set \\(\\{A, B, C, D, E\\}\\), and the other formed by the connected subgraph with node set \\(\\{F, G, H, I\\}\\).\n\n\n\n\n\n\n\n(a) A disconnected graph with three components.\n\n\n\n\n\n\n\n(b) A disconnected graph with a giant component.\n\n\n\n\n\n\n\n(c) Another disconnected graph with a giant component.\n\n\n\n\n\n\n\n\n\n(d) Yet another disconnected graph with a giant component.\n\n\n\n\n\n\n\n(e) A connected graph.\n\n\n\n\n\n\n\n(f) Yet another connected graph.\n\n\n\n\nFigure 11.3: A series of connected and disconnected graphs displaying components and the giant component.\n\n\nIt is possible to split a graph into multiple components not just two. For instance, take a look at the unlabeled graph in Figure 11.2. If we were to generate a subgraph from Figure 11.2 by deleting the two red edges and the one purple edge, we would end up with the graph shown in Figure 11.3 (a). This graph is disconnected, and it features three separate components (connected subgraphs).\nIf were to delete just the red edges from Figure 11.2 and keep the purple edge, we would end up with the graph shown in Figure 11.3 (b). Like the Figure 11.3 (a), Figure 11.3 (b) is also disconnected, but it is split into two not three components.\nNote that one of the connected components in the graphs shown in Figure 11.3 (b), Figure 11.3 (c), and Figure 11.3 (d) is way larger than the other one. For instance, the bigger connected component of graph Figure 11.3 (b) is of order ten, but the smaller component is only of order four.\nWhen a disconnected graph is split into multiple components, the component containing the largest number of nodes (the connected subgraph of the highest order) is called the graph’s giant component."
  },
  {
    "objectID": "lesson-graphs-connectivity.html#sec-edgecon",
    "href": "lesson-graphs-connectivity.html#sec-edgecon",
    "title": "11  Graph Connectivity",
    "section": "11.3 Edge Connectivity",
    "text": "11.3 Edge Connectivity\nWe went from the connected graph shown in Figure 11.2 to the disconnected graphs in Figure 11.3 by removing particular edges, like the red and purple ones in Figure 11.2.\nThese edges are clearly more important than the other colored blue in the graph, because they are responsible for keeping the whole graph together as a connected structure. Obviously this property of edges has a name in graph theory.\nIn a graph, an edge cutset is any set of edges whose removal results in a graph going from being connected to disconnected.\nFor instance, in Figure 11.2 the set of two red edges is a cutset of the graph \\(G\\) since deleting these edges would disconnect the graph as shown in Figure 11.3 (b). In the same way we can see that a subset that combines any one of the red edges with the purple edge is also a cutset of the graph in Figure 11.2, because removing any combination of these two links results in a disconnected graph, as shown in Figure 11.3 (c) and Figure 11.3 (d).\nGenerally, picking a bigger edge cutset will result in more disconnected components. For instance, selecting both the red edges and the purple edge—a cutset of cardinality three—as the cutset results in a graph with three disconnected components, as in Figure 11.3 (a).\nA minimum edge cutset of a graph is any edge cutset that also happens to be of the smallest size (in terms of cardinality) among all the possible cutsets. The cardinality of the minimum edge cutset in the graph is called the graph’s edge connectivity, and it is written \\(\\lambda(G)\\).\nThe graph in Figure 11.2 has edge-connectivity \\(\\lambda(G) = 2\\) because we need to remove at least two edges to disconnected it. As we saw earlier these could be either the set the two red edges of any combination featuring one of the red ones with the purple one.\nWhen a graph has edge-connectivity larger than one, like Figure 11.2, it means that we could remove any one edge and the graph would still be connected. For instance, looking at Figure 11.2, if we were to remove just the purple edge, the graph would still be connected, and it would look like Figure 11.3 (e), which is a connected graph.\nThe same would happen if we removed just one of the red edges from Figure 11.2. If were to do that, we would end up with the graph shown in Figure 11.3 (f), which is still a connected graph.\n\n11.3.1 Bridges\nAs you might imagine, the smallest edge connectivity a connected graph can have is \\(\\lambda(G) = 1\\). A graph with edge-connectivity equal to one has a special edge called a bridge. This is the single edge whose removal disconnects a graph with edge-connectivity equal to one.\nFor instance, in Figure 11.1 (a) the \\(DF\\) edge is a bridge. When a graph is like the one shown in Figure 11.1 (a) and has a bridge (like \\(DF\\)), this edge has two interesting properties (Buckley and Harary 1990, 17).\n\nFirst, if an edge is a bridge in graph \\(G\\), then that edge does not lie on any cycle of \\(G\\) (as defined in Chapter 9). That means that \\(DF\\) in Figure 11.1 (a) does not lie on any cycle of \\(G\\).\nSecond, if an edge is a bridge, then there is at least one pair of nodes i and j such that the bridge is an edge on every path linking i and j. In Figure 11.1 (a), clearly condition (2) is the case for every node in the subset \\(\\{F, G, H, I\\}\\) relative to nodes in the subset \\(\\{A, B, C, D, E\\}\\). Every path linking nodes in the first subset to nodes in the second subset has to include the bridge \\(DF\\)."
  },
  {
    "objectID": "lesson-graphs-connectivity.html#sec-nodecon",
    "href": "lesson-graphs-connectivity.html#sec-nodecon",
    "title": "11  Graph Connectivity",
    "section": "11.4 Node Connectivity",
    "text": "11.4 Node Connectivity\nAs you recall from Chapter 3, we can create subgraphs by removing either edges or nodes. In the same way, we can take a graph from being connected by removing either edges or nodes. In Section 11.3 we considered the case of removing edges to disconnect a graph. Here, we consider the case of removing nodes.\nIn a graph, a node cutset is any set of nodes whose removal results in a graph going from being connected to disconnected.\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) A disconnected graph after removing two nodes (A, D).\n\n\n\n\n\n\n\n\n\n(c) Anoter disconnected graph after removing two nodes (G, H).\n\n\n\n\n\n\n\n(d) A disconnected graph after removing one node (K).\n\n\n\n\nFigure 11.4: Two connected graphs.\n\n\nFigure 11.4 (a) shows a graph that is the same as Figure 11.3, except that the nodes are labeled. In that graph, the set of nodes \\(\\{A, D\\}\\) is a cutset of the graph \\(G\\) since deleting these nodes disconnects the graph, as shown by the resulting node-deleted subgraph shown in Figure 11.4 (b). This graph is now disconnected, featuring a nine-node giant component and a smaller three-node component containing nodes \\(\\{B, C, E\\}\\)\nIn the same way we can see that the set of nodes \\(\\{G, H\\}\\) is also a cutset of the graph, because removing these nodes would separate \\(\\{F, I, J\\}\\) from the rest of their friends, creating a graph with two separate components, as shown in Figure 11.4 (c).\nInterestingly, the set formed by the single node \\(K\\) is also a cutset of the graph. As shown in Figure 11.4 (d), removing this one node disconnects the graph, separating \\(\\{L, M, N\\}\\) from the rest.\nA minimum node cutset of a graph is any node cutset that also happens to be of the smallest size (in terms of cardinality). The cardinality of the minimum node cutset in the graph is called the graph’s node connectivity, and it is written \\(\\kappa(G)\\).\nThe graph in Figure 11.4 (a) has node-connectivity (\\(\\kappa(G) = 1\\)) because, as we have seen, all we need to do is remove \\(K\\) to disconnect it.\n\n11.4.1 Articulation (Cut)Nodes\nIf a graph has a single node whose removal disconnects the graph then that node is called the articulation node of the graph (also called a cutnode). The articulation node is the node equivalent of what a bridge is for edges. A single actor upon whom the entire connectivity of the network depends.\nIf a graph \\(G\\) has an articulation node, then by definition \\(\\kappa(G) = 1\\). Any graph \\(G\\) with \\(\\kappa(G) > 1\\), therefore, lacks an articulation node. It follows from this that if a graph has node-connectivity larger than one (\\(\\kappa(G) > 1\\)), we could remove any one node and the graph would still be connected.\nArticulation nodes have another interesting property related to indirect connectivity. If a graph has an articulation node u, there is at least one other node in the graph v such that u stands in the middle of every path (as an inner node) connecting v to the rest of the other nodes in the graph.\nFinally, if a node in a graph has degree \\(k = 1\\) (that is the node is an end-node) then it cannot be an articulation node. We can always remove an end-node from a connected graph and the graph will stay connected.\nThe larger the graph connectivity (\\(\\kappa(G)\\)) of a graph, then the larger is the structural cohesion of the underlying social network represented by the graph (White and Harary 2001).\n\n\n11.4.2 Graph Connectivity and Minimum Degree\nAn interesting mathematical relationship obtains between three graph properties: Minimum Degree (covered in Chapter 8) and the edge and node connectivities.\nIt goes as follows: In every graph the node connectivity is always smaller or equal to the edge connectivity which is always smaller or equal to the minimum degree (Harary 1969, 43).\nIn mathese, for any graph \\(G\\):\n\\[\n\\kappa(G) \\leq \\lambda(G) \\leq min(k)\n\\tag{11.1}\\]\nNeat! These can be interpreted as three different criteria that tells us whether how well-connected a whole graph, a subgraph, or a component of a larger is. Having a high minimum degree is the weakest, followed by having a high edge-connectivity, with having a high node connectivity being the strongest criterion."
  },
  {
    "objectID": "lesson-graphs-connectivity.html#advanced-local-bridges",
    "href": "lesson-graphs-connectivity.html#advanced-local-bridges",
    "title": "11  Graph Connectivity",
    "section": "11.5 Advanced: Local Bridges",
    "text": "11.5 Advanced: Local Bridges\nIn a classic paper on “The Strength of Weak Ties,” Mark Granovetter (1973) developed the concept of a local bridge. Recall from section Section 11.3.1, that a bridge is an edge that if removed would completely disconnect the graph.\nAnother way of thinking about this is that a bridge is an edge that if removed would increase the length of the shortest paths between two sets of nodes from a particular number to infinity since, in a disconnected graph, the length of the path between nodes that cannot reach one another is indeed infinity! (\\(\\infty\\)).\nThere is another way of thinking of bridges in the context of shortest paths and this is with respect to what happens to the indirect connectivity between particular pairs of nodes in the graph when a specific edge is removed. Clearly, every time we remove an edge, this has to affect the indirect connectivity between pair of nodes, by for instance, increasing their geodesic distance (the length of the shortest path linking them).\nSo even if a graph remains connected after removing an edge (because its edge-connectivity is larger than one), removing an edge can affect how close two nodes in the network are. That is what the concept of a local bridge is intended to capture.\nFormally, a local bridge is an edge that if removed from the graph, would increase the length of the shortest path between a particular pair of nodes to a number that is higher than their current one but less than infinity. This number is called the degree of the local bridge in question. Because a local bridge is always defined with respect to a particular pair of nodes, it is a triplet, involving two nodes i and j and one edge uv.\n\n\n\n\n\n\n\n(a) A connected graph.\n\n\n\n\n\n\n\n(b) Another connected graph.\n\n\n\n\nFigure 11.5: Two connected graphs.\n\n\nFor instance, in Figure 11.5 (a), with respect to nodes \\(A\\) and \\(H\\), the edge \\(HK\\) (pictured in purple) is a local bridge of degree 4. The reason for that is that, as shown in Figure 11.5 (b), when the edge \\(HK\\) is removed from the graph, the shortest path_ between nodes \\(H\\) and \\(K\\) increases from \\(l_{HK} =1\\) (\\(H\\) and \\(K\\) are adjacent in Figure 11.5 (a)) to \\(l_{HK} = 4\\), as given by the edge sequence \\(\\{GH, AG, AD, DK\\}\\) (pictured in red).\nNote that from the perspective of nodes \\(D\\) and \\(H\\), the purple \\(HK\\) edge is a local bridge of degree three, because \\(l_{DH} = 2\\) in Figure 11.5 (a) and \\(l_{DH} = 3\\) when the edge \\(HK\\) is removed from the graph in Figure 11.5 (b)."
  },
  {
    "objectID": "lesson-graphs-connectivity.html#references",
    "href": "lesson-graphs-connectivity.html#references",
    "title": "11  Graph Connectivity",
    "section": "References",
    "text": "References\n\n\n\n\nBuckley, Fred, and Frank Harary. 1990. Distance in Graphs. Addison-Wesley.\n\n\nGranovetter, Mark S. 1973. “The Strength of Weak Ties.” American Journal of Sociology 78 (6): 1360–80.\n\n\nHarary, Frank. 1969. Graph Theory. Addison-Wesley.\n\n\nWhite, Douglas R, and Frank Harary. 2001. “The Cohesiveness of Blocks in Social Networks: Node Connectivity and Conditional Density.” Sociological Methodology 31 (1): 305–59."
  },
  {
    "objectID": "lesson-matrix-intro.html#matrices",
    "href": "lesson-matrix-intro.html#matrices",
    "title": "12  Introduction to Matrices",
    "section": "12.1 Matrices",
    "text": "12.1 Matrices\nThus, switching to representing social networks as a matrix provides us with more analytic leverage. This was a brilliant idea that first occurred to Elaine Forsyth and Leo Katz in the mid 1940s (Forsyth and Katz 1946). When we represent the network as a matrix, we are able to efficiently calculate features of the network that we would not be able to estimate via “eyeballing.”\nWhat is a matrix?1 A matrix is, quite simply, a set of attributes that represent the values of a particular case. Breaking that explanation down, we can imagine a matrix as in Table 12.1. This common matrix, which we will refer to as an attribute-value matrix, a toy example of which is presented in Table 12.1, seems similar to a spreadsheet. Well, that is because a spreadsheet is a matrix!\n\n\nTable 12.1: Example of a general matrix.\n\n\n\nAttribute 1\nAttribute 2\nAttribute 3\n\n\n\n\nCase 1\nValue 1\nValue 4\nValue 7\n\n\nCase 2\nValue 2\nValue 5\nValue 8\n\n\nCase 3\nValue 3\nValue 6\nValue 9\n\n\nCase 4\nValue 10\nValue 11\nValue 12\n\n\nCase 5\nValue 13\nValue 14\nValue 15\n\n\n\n\nThe most important feature of a matrix is thus its organization into rows and columns. The number of rows and the number of columns define the dimensions of the matrix (like the length and and the width of your apartment define its dimensions in space). So when we say that a matrix is 5 \\(\\times\\) 3 when mean that it has five rows and three columns. When referring to the dimensions of matrix the rows always come first, and the columns always come second. So the more general way of saying is that the dimensions of a matrix are R \\(\\times\\) C, where R is the number of rows and C is the number of columns.\nThe intersection of a particular row (say row 2 in Table 12.1 and a particular column (say column 3 Table 12.1 defines a cell in the matrix. So when referring to a particular value in Table 12.1 we would speak of the \\(ij^{th}\\) cell in the matrix (or \\(c_{ij}\\)), where c is a general stand-in for the value of a given cell, i a general stand-in for a given row, and j is a generic stand-in for a given column. We refer to i as the matrix row index and j as the matrix column index.\nTypically, we give matrices names using boldfaced capital letters, so if we call the matrix shown in Table 12.1, matrix B, then we can refer to a specific cell in the matrix using the notation b\\(_{ij}\\) (note the lowercase), which says “the cell corresponding to row i and column j of the B matrix.”\nThus, in Table 12.1, cell b\\(_{32}\\) refers to the intersection between row 3 (representing case 3) and column 2 (representing attribute 2), where we can find value 6. For instance, let’s say cases are people and attributes are information we collected on each person (e.g., by surveying them) like their age, gender identity, and racial identification and so forth. Thus, if attribute 2 in Table 12.1 was age, and case 3 was a person, then value 6 would record that persons age (e.g., 54 years old).\n\n12.1.1 Relationship Matrices\nWe do not generally use attribute-value matrices to represent networks. Instead, we typically use a particular type of matrix called a relationship matrix. A relationship matrix is when, instead of asking what value of an attribute a case has, we ask about the value of describing how a case relates to other cases. If attribute-value matrices relate cases to attributes, then relationship matrices relate cases to one another (which is precisely the idea behind a “network”).\nTo do that, we put the same list of cases on both the rows and columns the matrix. Thus, we create a matrix with the organizational properties shown in Table 12.2.\n\n\nTable 12.2: Example of a relationship matrix.\n\n\n\nCase 1\nCase 2\nCase 3\n\n\n\n\nCase 1\nValue 1\nValue 2\nValue 3\n\n\nCase 2\nValue 4\nValue 5\nValue 6\n\n\nCase 3\nValue 7\nValue 8\nValue 9\n\n\n\n\nA relationship matrix thus captures exactly that, the relationship between two cases as shown in Table 12.2. So each cell, as the intersection of two cases (the row case and column case) gives us the value of the relationship between the cases. This value could be “friends” (if the two people are friends) or “not friends” (if they are not friends). The value could be the strength of the relationship. For instance each cell could contain the number of times a given case (e.g., a person) messaged another case.\nRelationship matrices are different from attribute value matrices, in that the latter are typically rectangular matrices. In a rectangular matrix, the number of rows (e.g., people) can be different from the number of columns (e.g., attributes). For instance, the typical attribute-value matrix used by social scientists who collect survey data on people are typically rectangular containing many more cases (rows) and columns (attributes). Some networks, like two mode networks represented as bipartite graphs, are best studied using rectangular matrices.\nRelationship matrices have some unique attributes. For instance, all relationship matrices are square matrices. A square matrix is one that has the same number of rows and columns: \\(R = C\\). So the relationship matrix shown in Table 12.2 is \\(3 \\times 3\\). A square matrix with n rows (and thus the same number of columns) is said to be a matrix of order n.\n\n\n12.1.2 Diagonal versus off-diagonal cells\nIn a relationship matrix, we need to distinguish between two types of cells. First, there are the cells that fall along the main diagonal an imaginary line that runs from the uppermost left corner to the lowermost right corner; these are called diagonal cells, the values corresponding to which are shown in italics in Table 12.2. So if we name the matrix in Table 12.2 matrix A, then we can see that any cell a\\(_{ij}\\) in which i = j falls along the main diagonal; these are Values 1, 5, and 9 Table 12.2. Every other cell in which i \\(\\neq\\) j, is an off-diagonal cell.2\nIn reference to the main diagonal, off-diagonal cells are said to be above the main diagonal if the row index for that cell is smaller than the column index (e.g., a\\(_{i < j}\\)). So in Table 12.2, values 2, 3, and 6, corresponding to cells a\\(_{12}\\) a\\(_{13}\\) and a\\(_{23}\\), respectively, are above the main diagonal. In the same way, cells in which the row index is larger than the column index are said to be below the main diagonal (e.g., a\\(_{i > j}\\)). So in Table 12.2, values 4, 7, and 8, corresponding to cells a\\(_{21}\\) a\\(_{31}\\) and a\\(_{32}\\), respectively, are below the main diagonal.\nNote that in a square matrix, the values above and below the main diagonal have a “triangular” arrangement. Accordingly, sometimes we refer to these areas of a square matrix as the upper and lower triangles.\nNote also that if the relationship matrix represents the relationship between the cases, and the cases are people in a social network, then the diagonal cells in a relationship matrix represent the relationship of people with themselves! Now if you have seen M. Night Shyamalan movies about people with split personalities, it is quite possible for people to have a rich set of relationships with themselves. Some of these may even form a social network inside a single head (Martin 2017). But we are not psychiatrists, so we are interested primarily in interpersonal not intrapersonal relations.\n\n\nTable 12.3: Example of a relationship matrix with blocked diagonals.\n\n\n\nCase 1\nCase 2\nCase 3\n\n\n\n\nCase 1\n–\nValue 1\nValue 2\n\n\nCase 2\nValue 3\n–\nValue 4\n\n\nCase 3\nValue 5\nValue 6\n–\n\n\n\n\nThis means that most of the time, we can ignore the diagonal cells in relationship matrices and rewrite them as in Table 12.3, in which values appear only for the off-diagonal cells. So here we can see the relationship between Case 1 and Case 2 is Value 1, and the relationship between Case 2 and Case 1 is Value 3. Wait, would that mean Value 2 and 4 are the same? The answer is maybe. Depends on what type of network tie is being captured, as these were discussed in the lesson on graph theory. If the tie is symmetric (and thus represented in an undirected graph), then the values will have to be the same. But if the asymmetric (and thus represented in a directed graph) then they don’t have to be.\nBy convention, in a relationship matrix, we say that the case located in row i sends (a tie) to the case located in column j, so if the relationship matrix was capturing friendship, we might say that i considers j to be a friend (sends the consideration) and so if i is Case 1 (row 1) and j is Case 2 (column 2), that would be Value 1 (e.g., “Are we friends?” Value 1 = Yes/No). But when i is now Case 2 (row 2) and j is Case 1 (column 1), we are now asking if Case 2 considers Case 1 to be a friend (e.g., “Are we friends?” Value 3 = Yes/No). If friendship is considered an asymmetric tie in this case, then that could be true, or it could not be. For instance, Case 2 can rebuff Case 1’s friendship offer.\nNote that if the tie we recorded in a relationship matrix is symmetric, we can simplify the presentation even further. The reason is that as already noted, if a relationship is symmetric, then the value of the tie that i sends to j is necessarily the same as the value of the tie that j sends to i. This means that, in the relationship matrix, the value of cell a\\(_{ij}\\) has to be the same as the value of the cell a\\(_{ji}\\) for all rows i and columns j in the matrix. This yields a symmetric relationship matrix, like that shown in Table 12.4.\n\n\nTable 12.4: Example of a symmetric relationship matrix with blocked diagonals.\n\n\n\nCase 1\nCase 2\nCase 3\n\n\n\n\nCase 1\n–\nValue 1\nValue 2\n\n\nCase 2\nValue 1\n–\nValue 3\n\n\nCase 3\nValue 2\nValue 3\n–\n\n\n\n\nNote that a symmetric relationship matrix is simpler than its asymmetric counterpart, because now we only have to worry about half of the values. So before, in Table 12.3 we had to worry about six distinct relationship values, but now we only have to worry about three. This means that, in a symmetric matrix, all the network information we need to look at is contained in either the lower triangle or the upper triangle. As we will see, in many applications, we can ignore one of the triangles altogether!\nThere are many types of relationship and attribute-value matrices as the basic principles just stated can be varied to capture different underlying facets of relationships. Subsequent lessons will cover various ways different aspects of networks can be best captured in matrix form and then manipulated to produce sociologically meaningful results."
  },
  {
    "objectID": "lesson-matrix-intro.html#references",
    "href": "lesson-matrix-intro.html#references",
    "title": "12  Introduction to Matrices",
    "section": "References",
    "text": "References\n\n\n\n\nForsyth, Elaine, and Leo Katz. 1946. “A Matrix Approach to the Analysis of Sociometric Data: Preliminary Report.” Sociometry 9 (4): 340–47.\n\n\nMartin, John Levi. 2017. “The Structure of Node and Edge Generation in a Delusional Social Network.” Journal of Social Structure 18 (1): 1–21."
  },
  {
    "objectID": "lesson-matrix-adjacency.html#symmetric-adjadcency-matrices-for-undirected-graphs",
    "href": "lesson-matrix-adjacency.html#symmetric-adjadcency-matrices-for-undirected-graphs",
    "title": "13  The Adjacency Matrix",
    "section": "13.1 Symmetric Adjadcency Matrices for Undirected Graphs",
    "text": "13.1 Symmetric Adjadcency Matrices for Undirected Graphs\nThe first step in building the adjacency matrix that represents the graph is to list all the nodes \\(\\{A, B, C, D, E, F, G, H, I\\}\\) as both a row and a column entry for each node. Next, one goes sequentially across the rows and columns, asking the question “does actor i have the relationship I am examining with actor j?” If the question asked is about the absence or presence of a relationship, 0’s and 1’s are used. If A has a relationship with B, the value 1 is marked. Otherwise, 0.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n–\n1\n1\n1\n1\n0\n0\n0\n0\n\n\nB\n1\n–\n1\n1\n0\n0\n0\n0\n0\n\n\nC\n1\n1\n–\n1\n1\n0\n0\n0\n0\n\n\nD\n1\n1\n1\n–\n1\n1\n0\n0\n0\n\n\nE\n1\n0\n1\n1\n–\n0\n0\n0\n0\n\n\nF\n0\n0\n0\n1\n0\n–\n1\n1\n1\n\n\nG\n0\n0\n0\n0\n0\n1\n–\n1\n1\n\n\nH\n0\n0\n0\n0\n0\n1\n1\n–\n1\n\n\nI\n0\n0\n0\n0\n0\n1\n1\n1\n–\n\n\n\nTable 13.1: Symmetric adjacency matrix corresponding to an undirected graph.\n\n\nAs we can see in Table 13.1, A indeed has a relationship with B, so the corresponding cell for the row corresponding to A and the column corresponding to B is marked 1. In fact, A has a relationship with B, C, D, and E and has 1’s in each of the cells corresponding to these actors, but not with F, G, H, or I and so 0’s are in these cells.\nBut what do we do about the cells where we are theoretically supposed to ask if A has a relationship with A? As we have seen, for most sociological applications, it makes sense to just put a dash there, thus blocking the diagonals. It’s not sociologically meaningful for A to have a relationship with itself.\nFor example, asking “Is A friends with A?” does not make much sense, but there are rare cases when it does, such as when A is a group of people and not an individual, and the relationship we are looking at might occur both within and between groups. As we saw in Chapter 2, these are called reflexive-ties or loops. But if the network is represented as a simple graph it should contain no loops.\nAfter completing the first row, we ask does actor B have a relationship with actor A? Well yes, it does. In fact, we can know without even looking because if you recall, this network is defined ahead of time as reciprocal, meaning if A is friends with B, B is friends with A. We can remember this because the graph we are using is undirected. This means that the resulting matrix is going to be symmetric. Symmetric matrices are those that, when flipped along the diagonal (as shown in Figure Table 13.1), the two sides of the matrix will be mirror images of each other."
  },
  {
    "objectID": "lesson-matrix-adjacency.html#asymmetric-adjacency-matrices-for-directed-graphs",
    "href": "lesson-matrix-adjacency.html#asymmetric-adjacency-matrices-for-directed-graphs",
    "title": "13  The Adjacency Matrix",
    "section": "13.2 Asymmetric Adjacency Matrices for Directed Graphs",
    "text": "13.2 Asymmetric Adjacency Matrices for Directed Graphs\nConversely, a directed graph describing a network of asymmetric or anti-symmetric ties will create an asymmetric matrix. Saying a matrix is asymmetric means that the values contained in the upper and lower triangles of the matrix do not mirror each other. In other words, In an asymmetric matrix the cell values are not necessarily the same (the relationship is not necessarily equivalent) between every pair of cases.\nFigure 4.2 shows an example directed graph. The corresponding asymmetric adjacency matrix is shown in Table 13.2. Note that while some relationships (such as between node A and B) are reciprocated, not all connections in the network are reciprocated. Node G sends ties to D and F, but does not receive any ties back.\nIn the resulting matrix, A to B and B to A each have a 1 listed for the value, while G to D and G to F also have a value of 1. However, the cells corresponding to F to G and D to G each have a value of 0 because the ties are unreciprocated. These unreciprocated ties make the resulting matrix asymmetric. The two halves across the diagonal are no longer mirror images, but contain different entries.\n\n\n\n\n\n \n  \n     \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n  \n \n\n  \n    A \n    -- \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    B \n    1 \n    -- \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    0 \n    -- \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    1 \n    -- \n    0 \n    0 \n  \n  \n    F \n    1 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    -- \n  \n\n\n\nTable 13.2:  Asymmetric adjacency matrix corresponding to an directed graph. \n\n\nWhy are the ties are not reciprocated? You might remember from our lessons on types of ties and types of graphs, but it is because of the type of data that the graph and matrix are representing. For example, the matrix in Table 13.2 and graph shown in Figure 4.2 could represent a intramural basketball club where they ask everyone in the club who they like to have as a teammate. Not everyone could agree that they like to have one another as teammates, and the matrix and graph in Figure 4.2 would represent that.\nIn this case, a node like G and E look really lonely since they have nobody who wants to play with them. However, if the tie were to be about advice, such that actually G gives advice to D and F, but does not take their advice back, G (and E) now look like respected figures in the network."
  },
  {
    "objectID": "lesson-matrix-row-column-sums.html#row-sums",
    "href": "lesson-matrix-row-column-sums.html#row-sums",
    "title": "14  Matrix Operations: Row and Column Sums",
    "section": "14.1 Row Sums",
    "text": "14.1 Row Sums\nOnce we have constructed a matrix, we can do various numerical operations on the matrix to compute all kinds of exciting things. Perhaps the most basic operation that we can do on a matrix is compute its row sums and its column sums.\n\n\n\n\n\n\n  \n    3 \n    3 \n    2 \n    6 \n    5 \n  \n  \n    4 \n    6 \n    9 \n    5 \n    3 \n  \n  \n    9 \n    9 \n    9 \n    3 \n    8 \n  \n  \n    7 \n    9 \n    3 \n    4 \n    1 \n  \n  \n    7 \n    5 \n    7 \n    9 \n    9 \n  \n\n\n\nTable 14.1:  A Matrix. \n\n\nConsider the matrix shown in Table 14.1. This matrix is of dimensions \\(5 \\times 5\\) meaning it has five rows and five columns. Let’s call the matrix \\(\\mathbf{B}\\).\nThe row sums of \\(\\mathbf{B}\\) is written:\n\\[\n\\sum_i b_{ij}\n\\tag{14.1}\\]\nThis is called sigma notation. In this formula i refers to the rows of the matrix and j refers to the columns. So the formula says, “to get the row sums, pick a row i, and sum the cells across the columns j.” So for instance, if \\(i = 2\\), then Equation 14.1 turns into:\n\\[\n\\sum_2 b_{2j} = 4 + 6 + 9 + 5 + 3 = 27\n\\tag{14.2}\\]\nNote that the numbers in the sum are just the number in row two of Table 14.1. The same goes for all the other rows. Note that if we do the row sums of all the rows in the matrix, the result is a row sum vector, containing the total sum for the numbers in each row.\nThe row sum vector for Table 14.1 is:\n\n\n\n\n\n  \n    19 \n    27 \n    38 \n    24 \n    37 \n  \n\n\n\n\n\nThe first number corresponds to the row sums of the first row, the second to the rows sums of the second row, and so forth."
  },
  {
    "objectID": "lesson-matrix-row-column-sums.html#column-sums",
    "href": "lesson-matrix-row-column-sums.html#column-sums",
    "title": "14  Matrix Operations: Row and Column Sums",
    "section": "14.2 Column Sums",
    "text": "14.2 Column Sums\nThe column sums of a matrix work pretty much the same way. In sigma notation the column sum for matrix \\(\\mathbf{B}\\) is written as:\n\\[\n\\sum_j b_{ij}\n\\tag{14.3}\\]\nNote that the main difference between Equation 14.3 and Equation 14.1 is the subscript under the \\(\\sum\\) symbol, which is j instead of i. Equation 14.3 says: “to get the column sums, pick a column, and then sum each number down the rows.”\nSo for instance, if \\(j = 3\\), then Equation 14.3 turns into:\n\\[\n\\sum_3 b_{i3} = 2 + 9 + 9 + 3 + 7 = 30\n\\tag{14.4}\\]\nThe column sum vector for Table 14.1 is:\n\n\n\n\n\n  \n    30 \n    32 \n    30 \n    27 \n    26 \n  \n\n\n\n\n\nThe first number corresponds to the column sums of the first column, the second to the column sums of the second column, and so forth."
  },
  {
    "objectID": "lesson-matrix-row-column-sums.html#row-and-column-sums-of-the-symmetric-adjacency-matrix",
    "href": "lesson-matrix-row-column-sums.html#row-and-column-sums-of-the-symmetric-adjacency-matrix",
    "title": "14  Matrix Operations: Row and Column Sums",
    "section": "14.3 Row and Column Sums of the Symmetric Adjacency Matrix",
    "text": "14.3 Row and Column Sums of the Symmetric Adjacency Matrix\nWhat happens if we calculate the row and column sum vectors of a symmetric adjacency matrix corresponding to an undirected graph?\nLet us go back to the example corresponding to Figure 4.1, whose adjacency matrix is show in Table 13.1. The row sum vector of the adjacency matrix is shown in Table 14.2 (a), and the column sum vector is shown in Table 14.2 (b).\n\n\nTable 14.2: Row and column sum vector of the symmetric adjacency matrix for an undirected graph.\n\n\n\n\n(a) Row sums of the symmetric adjacency matrix. \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    4 \n    3 \n    4 \n    5 \n    3 \n    4 \n    3 \n    3 \n    3 \n  \n\n\n\n\n\n\n(b) Column sums of the symmetric adjacency matrix. \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n  \n \n\n  \n    4 \n    3 \n    4 \n    5 \n    3 \n    4 \n    3 \n    3 \n    3 \n  \n\n\n\n\n\n\nNote two things. For the symmetric adjacency matrix, the row and column sums vectors are identical. So we get the same answer whether we sum the numbers for each row across columns, or for each column down the rows.\nBut what are these numbers? If you stare at them long enough, you will see that they are familiar, for they are nothing but the undirected graph’s degree set. So, for an undirected graph, the row or column sums of the symmetric adjacency matrix gives us the degrees of each node!"
  },
  {
    "objectID": "lesson-matrix-row-column-sums.html#row-and-column-sums-of-the-asymmetric-adjacency-matrix",
    "href": "lesson-matrix-row-column-sums.html#row-and-column-sums-of-the-asymmetric-adjacency-matrix",
    "title": "14  Matrix Operations: Row and Column Sums",
    "section": "14.4 Row and Column Sums of the Asymmetric Adjacency Matrix",
    "text": "14.4 Row and Column Sums of the Asymmetric Adjacency Matrix\nNow what happens if we compute the row and column sums for an asymmetric adjacency matrix? We already saw one such matrix in Table 13.2 corresponding to Figure 4.2. The row and column sums of that adjcency matrix is shown in Table 14.3 (a) and Table 14.3 (b).\n\n\nTable 14.3: Row and column sum vector of the asymmetric adjacency matrix for a directed graph.\n\n\n\n\n(a) Row sums of the asymmetric adjacency matrix. \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n  \n \n\n  \n    2 \n    2 \n    1 \n    1 \n    2 \n    1 \n    2 \n  \n\n\n\n\n\n\n(b) Column sums of the asymmetric adjacency matrix. \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n  \n \n\n  \n    2 \n    3 \n    1 \n    3 \n    0 \n    2 \n    0 \n  \n\n\n\n\n\n\nNote that in contrast to the symmetric case, the row and column sum vectors of the asymmetric adjacency matrix corresponding to a directed graph are different. What’s the difference?\nWell if you stare at them long enough, it might dawn on you that the row sum vector corresponds to the directed graph’s outdegree set, and the column sum vector corresponds to the directed graph’s indegree set.\nSo in the directed case, each of the sum operations that we can perform on the asymmetric adjacency matrix captures a separate graph property, in this case the difference between each node’s indegree (given by the row sums) and each node’s outdegree (given by the column sums). Neat!"
  },
  {
    "objectID": "lesson-matrix-operations.html#matrix-addition-and-subtraction",
    "href": "lesson-matrix-operations.html#matrix-addition-and-subtraction",
    "title": "15  Basic Matrix Operations",
    "section": "15.1 Matrix Addition and Subtraction",
    "text": "15.1 Matrix Addition and Subtraction\nPerhaps the simplest operation we can do with two matrices is add them up. To add two matrices, we simply add up the corresponding entries in each cell. In matrix notation:\n\\[\n\\mathbf{H} + \\mathbf{C} = h_{ij} + c_{ij}\n\\tag{15.1}\\]\nWhere \\(h_{ij}\\) is the corresponding entry for nodes i and j in the hanging out adjacency matrix \\(\\mathbf{H}\\), and \\(c_{ij}\\) is the same entry in the co-working adjacency matrix \\(\\mathbf{C}\\).\nWhy would we want to do this? Well, if we were studying the network shown in Figure 15.1, we might be interested in which dyads have uniplex (or single-stranded) relations, and which ones have multiplex (or multi-stranded) relations. That is, while some actors in the network either hang out together or work together, some of the do both. Adding up the adjacency matrices shown in Table 15.1, will tell us who these are. The result is shown in Table 15.2.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    1 \n    2 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    -- \n    2 \n    2 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    2 \n    2 \n    -- \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    2 \n    1 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    -- \n    2 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    2 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    2 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    2 \n    -- \n    1 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    -- \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    -- \n    1 \n    2 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    1 \n  \n  \n    L \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    2 \n    1 \n    -- \n  \n\n\n\nTable 15.2:  Uniplex and Multiplex adjacency matrix. \n\n\nTable 15.2 shows that the \\(BC\\) dyad has a multiplex relation (there is a “2” in the corresponding cell entry) and so does the \\(AC\\), \\(FH\\), \\(GH\\), \\(EF\\), and \\(JL\\) dyads. A graph shown the nodes linked only by multiplex relations (hanging out and co-working) is shown in Figure 15.1 (c).\nNote that matrix subtraction works the same way:\n\\[\n\\mathbf{H} - \\mathbf{C} = h_{ij} - c_{ij}\n\\tag{15.2}\\]\nTo subtract two matrices, we simply subtract the corresponding entries in each cell. Why would we ever want to do this? Perhaps we could be interested in those dyads in the network that are connected by a single, special purpose uniplex ties while disregarding both the disconnected and multiplex connected dyads.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    0 \n    1 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    1 \n    -- \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    0 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    -- \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    -- \n    1 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    1 \n  \n  \n    L \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    -- \n  \n\n\n\nTable 15.3:  Uniplex only adjacency matrix. \n\n\nIn which case, subtracting the two matrices, and then taking the absolute value of each of the cell entries (e.g., turning negative entries into positive ones), written \\(|h_{ij} - c_{ij}|\\) will result in a binary matrix that will only contain ones for people who are either hangout buddies or workmates but not both. Such a marix is shown in Table 15.3 and corresponding to the graph shown in Figure 15.1 (d)."
  },
  {
    "objectID": "lesson-matrix-operations.html#sec-dotproduct",
    "href": "lesson-matrix-operations.html#sec-dotproduct",
    "title": "15  Basic Matrix Operations",
    "section": "15.2 The Matrix Dot Product",
    "text": "15.2 The Matrix Dot Product\nAnother way of figuring out which pairs of people in a network have multiplex ties is to compute the matrix dot product (symbol: \\(\\cdot\\)), sometimes this is also called the Hadamard product named after French mathematician Jacques Hadarmard, (symbol: \\(\\circ\\)). Just like matrix addition, we find the matrix dot product by multiplying the corresponding entries in each of the matrices. In matrix format:\n\\[\n\\mathbf{H} \\circ \\mathbf{C} = h_{ij} \\times c_{ij}\n\\tag{15.3}\\]\nIf we take the dot product of two adjacency matrices like \\(\\mathbf{H}\\) and \\(\\mathbf{C}\\), then the resulting matrix will have a one in a given cell only if \\(h_{ij} = 1\\) and \\(c_{ij} = 1\\). Otherwise, it will have a zero. This means that the dot product of two adjacency matrices will retain only the multiplex ties and erase all the other ones. The result of the dot products of the adjacency matrices shown in Table 15.1 is shown in Table 15.4.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    1 \n    0 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    -- \n  \n\n\n\nTable 15.4:  Multiplex relationship matrix. \n\n\nAs we can see, the only dyads that have non-zero entries in Table 15.4 are the multiplex dyads in Table 15.2. The resulting network, composed of the combined “hanging + co-working” relation is shown in Figure 15.1 (c). Note that this network is much more sparse than either of the other two, since there’s an edge between nodes only when they are adjacent in both the Figure 15.1 (a) and Figure 15.1 (b) networks."
  },
  {
    "objectID": "lesson-matrix-operations.html#sec-transpose",
    "href": "lesson-matrix-operations.html#sec-transpose",
    "title": "15  Basic Matrix Operations",
    "section": "15.3 The Matrix Transpose",
    "text": "15.3 The Matrix Transpose\nOne thing we can do with a matrix is “turn it 90 degrees” so that the rows of the new matrix are equal to the columns of the resulting matrix and the columns of the first matrix equal the rows of the resulting matrix. This is called the matrix transpose (symbol: \\(^T\\)).\nFor instance, if we have a matrix \\(\\mathbf{A}_{4 \\times 5}\\) of dimensions \\(4 \\times 5\\) (four rows and five columns), then the transpose \\(A^T_{5 \\times 4}\\) will have five rows and four columns, with the respective entries in each matrix given by the formula:\n\\[\na_{ij} = a^T_{ji}\n\\] That is the number that in the first matrix appears in the \\(i^{th}\\) row and \\(j^{th}\\) column now appears in the transposed version of the matrix in the \\(j^{th}\\) row and \\(i^{th}\\) column.\nAn example of a matrix and its tranpose is shown in Table 16.1.\n\n\nTable 15.5: A matrix and its transpose\n\n\n\n\n(a) Original Matrix. \n\n  \n    3 \n    4 \n    5 \n  \n  \n    7 \n    9 \n    3 \n  \n  \n    4 \n    6 \n    2 \n  \n  \n    5 \n    3 \n    4 \n  \n  \n    2 \n    5 \n    4 \n  \n\n\n\n\n\n\n(b) Transposed Matrix. \n\n  \n    3 \n    7 \n    4 \n    5 \n    2 \n  \n  \n    4 \n    9 \n    6 \n    3 \n    5 \n  \n  \n    5 \n    3 \n    2 \n    4 \n    4 \n  \n\n\n\n\n\n\nSo let’s check out how the transpose works. The original matrix in Table 16.1 (a) has five rows and three columns. The transposed matrix has three rows and five columns. We can find the same numbers in the original and transposed matrices by switching the rows and columns. Thus, in the original matrix, the number in third row and second column is a six (\\(a_{32} = 6\\)). In the transposed version of the matrix, that same six is in second row and third column (\\(a^T_{23} = 6\\)). If you check, you’ll see that’s the case for each number! Thus, the transposed version of a matrix has the same information as the original, it is just that the rows and columns are switched. While this might seem like a totally useless thing to do (or learn) at the moment, we will see in Chapter 22 that the matrix transpose comes in very handy in the analysis of social networks, and particular in the analysis of two mode networks and cliques."
  },
  {
    "objectID": "lesson-matrix-operations.html#references",
    "href": "lesson-matrix-operations.html#references",
    "title": "15  Basic Matrix Operations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-multrules",
    "href": "lesson-matrix-multiplication.html#sec-multrules",
    "title": "16  Matrix Multiplication",
    "section": "16.1 Matrix Multiplication Rules",
    "text": "16.1 Matrix Multiplication Rules\nFirst, we will let out the basic rules of matrix multiplication:\n\nYou can always multiply two matrices as long as the number of columns of the first matrix equal the rows of the second matrix. To check whether this is the case, all you have to do is put the two matrices side by side and list their dimensions.\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6}\n\\] - The two little “fives” in bold are called the inner dimensions of the two matrices. The little “three” on the left and the little “six” on the right are called the outer dimensions. So another way of stating the first rule of matrix multiplication is that the product of two matrices is defined as long as their inner dimensions equal to one another when you line them up from left to right.\n\nWhen the number of columns of a matrix equal the number of rows of another matrix so that their inner dimensions match we say that the the two matrices are conformable. When this is not the case, we say the matrices are non-conformable.\nThus, another way of stating the first rule is that only the product of conformable matrices is defined. If the matrices are not conformable then their product is not defined (e.g., there is no answer to the question of what we get if we multiply them!).\nThis means that unlike numbers or the matrix dot product, where the order of the two things you are multiplying doesn’t matter (\\(4 \\times 3 = 3 \\times 4\\) or \\(\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{B} \\cdot \\mathbf{A}\\)), in matrix multiplication it does matter. Alas, for any two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\),\n\n\\[\n\\mathbf{A} \\times \\mathbf{B} \\neq \\mathbf{B} \\times \\mathbf{A}\n\\]\n\nWhen you multiply a matrix times another matrix, the resulting matrix will have number of rows equal to the number of rows of the first matrix and number of columns equal to the number of columns of the second matrix. Thus:\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6} = \\mathbf{C}_{3 \\times 6}\n\\tag{16.1}\\]\n\nEquation 16.1 says that the product of a three by five matrix \\(\\mathbf{A}\\) (three rows and five columns) times a five by six matrix \\(\\mathbf{B}\\) (five rows and six columns) is a third matrix \\(\\mathbf{C}\\) with three rows and six columns. Another way of saying this last rule is that the product of two conformable matrices will have dimensions equal to their outer dimensions."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-matmultex",
    "href": "lesson-matrix-multiplication.html#sec-matmultex",
    "title": "16  Matrix Multiplication",
    "section": "16.4 Matrix Multiplication Examples",
    "text": "16.4 Matrix Multiplication Examples\nNow let’s see some examples of how matrix multiplication works. Table 16.2 shows the result of multiplying the matrix shown in Table 16.1 (a) times its transpose, shown in Table 16.1 (b).\n\n\n\n\n\n\n  \n    50 \n    72 \n    46 \n    47 \n    46 \n  \n  \n    72 \n    139 \n    88 \n    74 \n    71 \n  \n  \n    46 \n    88 \n    56 \n    46 \n    46 \n  \n  \n    47 \n    74 \n    46 \n    50 \n    41 \n  \n  \n    46 \n    71 \n    46 \n    41 \n    45 \n  \n\n\n\nTable 16.2:  Matrix resulting from multiplying a matrix times its transpose \n\n\nNow where the heck did these numbers come from? Don’t panic. We’ll break it down. First, let’s begin with the number \\(50\\) in cell corresponding to the first row and first column of Table 16.2. To find out where this number came from, let’s look at the first row of Table 16.1 (a), composed of the vector \\(\\{3, 4, 5\\}\\), and the first-column of Table 16.1 (b), composed of the same vector \\(\\{3, 4, 5\\}\\). Now, the number \\(50\\) comes from the fact that we multiply each of the corresponding entries of the two vectors, and then add them up, as follows:\n\\[\n(3 \\times 3) + (4 \\times 4) + (5 \\times 5) = 9 + 16 + 25 = 50\n\\]\nNeat! Now let’s see where the number \\(74\\) in the fourth row and second column of Table 16.2 came from. For that we look at the entries in the fourth row of Table 16.1 (a), composed of the vector \\(\\{5, 3, 4\\}\\) and the second column of Table 16.1 (b) composed of the vector \\(\\{7, 9, 3\\}\\). Like before, we take the first number of the first vector and multiply it by the first number of the second vector, the second number of the first vector and multiply it by the second number of the second vector, and the third number of the first vector and multiply it by the third number of the second vector and add up the results:\n\\[\n(5 \\times 7) + (3 \\times 9) + (4 \\times 3) = 35 + 27 + 12 = 74\n\\] And we keep on going like this to get each of the twenty five numbers in Table 16.2 (there are twenty five numbers because Table 16.2 has five rows and five columns and five times five equal twenty five). In general terms, the number in the \\(i^{th}\\) row and \\(j^{th}\\) column of Table 16.2 is equal to the sum of the products of the numbers in the \\(i^{th}\\) row of the Table 16.1 (a) and the \\(j^{th}\\) column of Table 16.1 (b).\nNote that the resulting product matrix shown in Table 16.2 is symmetric. The same numbers that appear in the upper-triangle also appear in the lower triangle, such that \\(b_{ij} = b_{ji}\\). So once you know the numbers in one of the triangles, you can fill up the numbers in the other one without having to do all the multiplying and adding up!\nNow, let’s multiply the matrix in Table 16.1 (b) times the matrix in Table 16.1 (a). As the rules of matrix multiplication show, this will result in a matrix of dimensions \\(3 \\times 3\\) because Table 16.1 (b) has three rows and Table 16.1 (a) has three columns. This is shown in Table 16.3.\n\n\n\n\n\n\n  \n    103 \n    124 \n    72 \n  \n  \n    124 \n    167 \n    91 \n  \n  \n    72 \n    91 \n    70 \n  \n\n\n\nTable 16.3:  Matrix resulting from multiplying a matrix times its transpose \n\n\nLike before, if we want to figure out where the number \\(72\\) in the third row and first column of Table 16.3 came from, we go to the first row of Table 16.1 (b) composed of the vector \\(\\{5, 3, 2, 4, 4\\}\\) and the first column of Table 16.1 (a), composed of the vector \\(\\{3, 7, 4, 5, 2\\}\\) match up each number in terms of order, multiplying them and add up the result:\n\\[\n(5 \\times 3) + (3 \\times 7) + (2 \\times 4) + (4 \\times 5) + (4 \\times 2) =\n\\]\n\\[\n15 + 21 + 8 + 20 + 8 = 72\n\\]\n\n\nTable 16.4: Powers of a matrix.\n\n\n\n\n(a) A matrix. \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) Matrix squared. \n\n  \n    1 \n    1 \n    2 \n    0 \n  \n  \n    1 \n    1 \n    2 \n    1 \n  \n  \n    2 \n    1 \n    2 \n    2 \n  \n  \n    1 \n    1 \n    1 \n    2 \n  \n\n\n\n\n\n\n(c) Matrix cubed. \n\n  \n    2 \n    2 \n    3 \n    3 \n  \n  \n    3 \n    2 \n    4 \n    3 \n  \n  \n    4 \n    3 \n    5 \n    4 \n  \n  \n    3 \n    2 \n    4 \n    2 \n  \n\n\n\n\n\n\nMatrix powers work the same as regular matrix multiplication, except that we are working on just one matrix not two. So for instance, the number \\(2\\) in the first row and third column of Table 16.4 (b) comes from the numbers in the first row of Table 16.4 (a) (\\(\\{0, 1, 0, 1\\}\\)) and the numbers in the third column of Table 16.4 (a) (\\(\\{0, 1, 1, 1\\}\\)). We line them up, multiplying them, and add them:\n\\[\n(0 \\times 1) + (1 \\times 1) + (0 \\times 1) + (1 \\times 1) = 0 + 1 + 0 + 1 = 2\n\\] Since we are working with a binary matrix, the product of each of the cell entries will be either a zero (when at least one of the entries is zero) or a one (when both entries are one).\nTo get the cubed entries in Table 16.4 (c), we just take Table 16.4 (b) as the first matrix and Table 16.4 (a) as the second matrix, and do matrix multiplication magic. Thus, to get the number \\(4\\) in the third row and fourth column of Table 16.4 (c), we take the numbers in the third row of Table 16.4 (b) \\(\\{2, 1, 2, 2\\}\\) and the numbers in the fourth column of Table 16.4 (a) \\(\\{1, 0, 1, 0\\}\\), line them up, multiply them, and add them:\n\\[\n(2 \\times 1) + (1 \\times 0) + (2 \\times 1) + (1 \\times 0) = 2 + 0 + 2 + 0 = 4\n\\]\nPretty easy!"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#matrix-multiplication-of-vectors",
    "href": "lesson-matrix-multiplication.html#matrix-multiplication-of-vectors",
    "title": "16  Matrix Multiplication",
    "section": "16.3 Matrix Multiplication of Vectors",
    "text": "16.3 Matrix Multiplication of Vectors\nRecall from Section 8.1 that a vector is a sequence of numbers of a given length. So for instance, the vector \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\) is a vector of length five.\nWell, and here comes the big reveal, it turns out that another way to think of a vector, is as a special case of matrix. That is, a matrix with one row, and as many columns as the length of the vector! This is a called a row vector. So the row vector \\(\\mathbf{a}\\) vector can be thought of as a matrix of dimensions \\(1 \\times 5\\) (one row and five columns) or \\(\\mathbf{a}_{1 \\times 5}\\).\nIn matrix form:\n\n\n\n\n\n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\nTable 16.5:  Matrix resulting from multiplying a matrix times its transpose \n\n\nSince vectors are matrices, we can perform the same type of matrix operations on them as we did with matrices. For instance, we can compute the transpose of a vector. In the case of \\(\\mathbf{a}\\), the transpose \\(\\mathbf{a}^T\\) is:\n\n\n\n\n\n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\nTable 16.6:  Matrix resulting from multiplying a matrix times its transpose \n\n\nThe transpose of a row vector is called (you may have guessed) a column vector. The column vector in Table 16.6 is a matrix with five rows and one column.\nThis also means that the same rules of matrix multiplication apply. For instance, we can always multiply a row vector times a column vector, because it is the equivalent of multypling a matrix times its transpose, and we have already seen in Section 16.1.1, that this can always be done:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{a}^T_{5 \\times 1} = b_{1 \\times 1}\n\\tag{16.4}\\]\nEquation 16.5 says that the product of the \\(1 \\times 5\\) row vector \\(\\mathbf{a}\\) times a \\(5 \\times 1\\) column vector \\(\\mathbf{a}^T\\) is a \\(1 \\times 1\\) “matrix” otherwise known as a scalar (that is, a regular old number). We’ve already seen examples of this, because in regular matrix multiplication, each cell of the product matrix is a scalar obtained from multiplying the corresponding terms taken from a row of the first matrix (which is a row vector) times those of the column of the second matrix (which is a column vector).\nSo in this case this would be:\n\\[\n(2 \\times 2) + (4 \\times 4) + (7 \\times 7) + (2 \\times 2) + (4 \\times 4) =\n\\]\n\\[\n4 + 16 + 49 + 4 + 16 = 89\n\\]\n\nThe first rule of vector matrix multiplication is that you can always multiply a row vector times a column vector (even when their entries are not the same) as long as they are the same length (e.g., the number of columns of the row vector equal the number of rows of the column vector).\nThe second rule of vector matrix multiplication is that when you multiply a row vector times another a column vector the result is always scalar (a single number).\n\nNow notice that if we change the order, and multiply the transpose of a vector times the original? This should be allowed because it conforms to the rules that we have already discussed:\n\\[\n\\mathbf{a}^T_{5 \\times 1} \\times \\mathbf{a}_{1 \\times 5} = B_{5 \\times 5}\n\\tag{16.5}\\]\nThis matrix multiplication is defined because the inner dimensions of the two matrices (the column and row vectors) are the same (one). But note that, according to the rules of matrix multiplication, when you multiply the transpose of a vector times the original, the result is a square matrix, with dimensions \\(n \\times n\\) where \\(n\\) is the length of the original row vector (the number of columns). In our example if the original vector is \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\), then \\(\\mathbf{a}^T \\times \\mathbf{a}\\) is equal to the matrix shown in Table 16.7.\n\n\n\n\n\n\n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n  \n    14 \n    28 \n    49 \n    14 \n    28 \n  \n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n\n\n\nTable 16.7:  Matrix resulting from multiplying a matrix times its transpose \n\n\n\nSo, the third and final rule of vector matrix multiplication is that when you multiply a column vector times a row vector of the same length, the result is a square matrix of row and column dimensions equal to the length of the original vectors."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "href": "lesson-matrix-multiplication.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "title": "16  Matrix Multiplication",
    "section": "16.6 Multiplying a Vector Times A Matrix (and Vice Versa)",
    "text": "16.6 Multiplying a Vector Times A Matrix (and Vice Versa)\nSince vectors are just matrices, it means that we can always multiply a vector times a matrix (and a matrix times a vector), as long as we follow the matrix multiplication rules laid out in Section 16.1.\n\n16.6.1 Row Vector Times Matrix\nFor instance, take the row vector \\(\\mathbf{b} = \\{4, 9, 3, 5\\}\\) and the binary matrix \\(\\mathbf{A}\\) shown in Table 16.4 (a). Because the row vector \\(\\mathbf{b}\\) is of dimensions \\(1 \\times 4\\) and matrix \\(\\mathbf{A}\\) is of dimensions \\(4 \\times 4\\), it is possible to multiply the vector times the matrix as follows:\n\\[\n\\mathbf{b}_{1 \\times 4} \\times \\mathbf{A}_{4 \\times 4} = \\mathbf{c}_{1 \\times 4}\n\\tag{16.6}\\]\nEquation 16.6 says that the product of a \\(1 \\times 4\\) row vector times a \\(4 \\times 4\\) square matrix is another row vector of dimensions equal to the original row vector. The result for this example is shown in Table 16.8.\n\n\nTable 16.8: Row vector resulting from multiplying a row vector times a square matrix\n\n\n\n\n(a) 1 X 4 row vector \n\n  \n    4 \n    9 \n    3 \n    5 \n  \n\n\n\n\n\n\n\n\n(b) 4 X 4 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 4 product vector \n\n  \n    8 \n    13 \n    17 \n    7 \n  \n\n\n\n\n\n\nOf course, it is also possible to multiply a row vector times a rectangular matrix (where the number of rows is not necessarily equal to the number of columns), as long as the number of rows of the rectangular matrix equals the length of the original row vector. For instance, take a row vector \\(\\mathbf{a}_{1 \\times 5}\\) shown in Table 16.9 (a) and a matrix \\(B_{5 \\times 3}\\). Its product would be given by:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{B}_{5 \\times 3} = \\mathbf{c}_{1 \\times 3}\n\\tag{16.7}\\]\nEquation 16.7 says that the product of a row vector of dimensions \\(1 \\times 5\\) and a matrix of dimensions \\(5 \\times 3\\) is another row vector \\(\\mathbf{c}\\) of dimensions (\\(1 \\times 3\\)). A numerical example corresponding to this situation is shown in Table 16.9.\n\n\nTable 16.9: Row vector resulting from multiplying row vector times a rectangular matrix\n\n\n\n\n(a) 1 X 5 row vector \n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\n\n\n\n\n\n(b) 5 x 3 matrix \n\n  \n    2 \n    2 \n    7 \n  \n  \n    0 \n    1 \n    13 \n  \n  \n    12 \n    16 \n    13 \n  \n  \n    12 \n    17 \n    12 \n  \n  \n    13 \n    14 \n    6 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 3 product vector \n\n  \n    164 \n    210 \n    205 \n  \n\n\n\n\n\n\nTo get the “179” entry in row one and column one of Table 16.9, we take the entries of the row vector shown in Table 16.9 (a) and multiply them by the corresponding entries in the first column of the matrix shown in Table 16.9 (b) and add up the results:\n\\[\n(2 \\times 2) + (4 \\times 5) + (7 \\times 11) + (2 \\times 15) + (4 \\times 12) =\n\\]\n\\[\n4 + 20 + 77 + 30 + 48 = 179\n\\]\nAnd so on for the other two entries in Table 16.9 (c). So the main rule of multiplying a row vector times a matrix with number of rows equal to the length of the row vector is that the result will always be another row vector of length equal to the number of columns of the matrix.\n\n\n16.6.2 Matrix Times Column Vector\nIn the same way, we can always multiply a matrix times a column vector, as long as the the number of columns of the matrix is equal to the length of the column vector. For instance, take the binary square matrix \\(A_{5 \\times 5}\\) shown in Table 16.10 (a) and the column vector \\(\\mathbf{b}_{5 \\times 1}\\) shown in Table 16.9 (b). Their product \\(\\mathbf{c}\\) would be given by:\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{b}_{5 \\times 1} = \\mathbf{c}_{5 \\times 1}\n\\tag{16.8}\\]\nEquation 16.8 says that the product of a matrix of dimensions \\(5 \\times 5\\) and a column vector of dimensions \\(5 \\times 1\\) is another column vector \\(\\mathbf{c}\\) of dimensions equal to the original column vector (\\(5 \\times 1\\)). A numerical example of this situation is shown in Table 16.10.\n\n\nTable 16.10: Column vector resulting from multiplying a square matrix times a column vector\n\n\n\n\n(a) 5 x 5 square matrix \n\n  \n    16 \n    1 \n    18 \n    6 \n    8 \n  \n  \n    2 \n    18 \n    11 \n    2 \n    5 \n  \n  \n    16 \n    12 \n    17 \n    1 \n    7 \n  \n  \n    18 \n    16 \n    8 \n    1 \n    10 \n  \n  \n    3 \n    15 \n    12 \n    6 \n    20 \n  \n\n\n\n\n\n\n(b) 5 x 1 column vector \n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\n\n\n\n(c) 5 X 1 product vector \n\n  \n    206 \n  \n  \n    177 \n  \n  \n    229 \n  \n  \n    198 \n  \n  \n    242 \n  \n\n\n\n\n\n\n\n\n16.6.3 Matrix Times the All Ones Vector\nIn matrix multiplication, there is a special row and column vector called the all ones vector. As you may have guessed this is a vector of all ones, of some length \\(n\\). For instance and all ones row vector of length five is \\(\\mathbf{1}_{1 \\times 5} = \\{1, 1, 1, 1, 1\\}\\) (the symbol for the all ones vector is a boldface “1”). We can also get the transpose of this all ones row vector to get the all ones column vector \\(\\mathbf{1}^T\\).\nWhy do we care about vectors full of ones? Well, it turns out that the all one row and column vectors have a neat property when we multiplied by matrices. We already know, from the rules of vector matrix multiplication reviewed earlier, that the product of a row vector times a square matrix is always a row vector of the same length as the original, and the product of a square matrix times a column vector is always a column vector of the same length as the original.\nLet’s say we a matrix \\(\\mathbf{A}\\) of dimensions \\(5 \\times 5\\), and we multiplied the all ones row vector of length five times this matrix, which would result in the row vector \\(\\mathbf{b}\\). This would be given by the formula:\n\\[\n\\mathbf{1}_{1 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = b_{1 \\times 5}\n\\tag{16.9}\\]\nA numerical example of the situation depicted in Equation 16.9 is shown in Table 16.11.\n\n\nTable 16.11: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 1 X 5 all ones row vector \n\n  \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 5 product row vector \n\n  \n    3 \n    5 \n    2 \n    3 \n    1 \n  \n\n\n\n\n\n\nIf you look at the resulting row vector in Table 16.11 (c), we can see that the result of multiplying the all ones row vector times a matrix is a vector that contains the column sums of the matrix entries! So the “2” in position 1 of Table 16.11 (c) comes from adding up the numbers in the first column of the matrix, the “1” in position 2 of Table 16.11 (c) comes from adding the numbers in the second column and so forth.\n\n\nTable 16.12: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) 5 X 1 all ones column vector \n\n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n\n\n\n\n\n\n(c) 5 X 1 product column vector \n\n  \n    2 \n  \n  \n    3 \n  \n  \n    4 \n  \n  \n    2 \n  \n  \n    3 \n  \n\n\n\n\n\n\nIn the same way, if we multiply the same matrix times the all ones column vector, we get the results shown in Table 16.12. We can see that the result of multiplying a matrix times the all ones column vector, is another column vector contains the row sums of the original matrix! So, the “2” in the first position of the column vector comes from adding the numbers in the first row of the matrix, the “3” comes from adding the numbers in the second row, and so forth."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#multiplying-a-matrix-times-column-vector",
    "href": "lesson-matrix-multiplication.html#multiplying-a-matrix-times-column-vector",
    "title": "16  Matrix Multiplication",
    "section": "16.5 Multiplying a Matrix Times Column Vector",
    "text": "16.5 Multiplying a Matrix Times Column Vector\nIn the same way, we can always multiply a matrix times a column vector, as long as the the number of columns of the matrix is equal to the length of the column vector. For instance, take the binary square matrix \\(A_{5 \\times 5}\\) shown in Table 16.10 (a) and the column vector \\(\\mathbf{b}_{5 \\times 1}\\) shown in Table 16.9 (b). Their product \\(\\mathbf{c}\\) would be given by:\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{b}_{5 \\times 1} = \\mathbf{c}_{5 \\times 1}\n\\tag{16.8}\\]\nEquation 16.8 says that the product of a matrix of dimensions \\(5 \\times 5\\) and a column vector of dimensions \\(5 \\times 1\\) is another column vector \\(\\mathbf{c}\\) of dimensions equal to the original column vector (\\(5 \\times 1\\)). A numerical example of this situation is shown in Table 16.10.\n\n\nTable 16.10: Column vector resulting from multiplying a square matrix times a column vector\n\n\n\n\n(a) 5 x 5 square matrix \n\n  \n    18 \n    10 \n    5 \n    14 \n    6 \n  \n  \n    6 \n    17 \n    8 \n    13 \n    13 \n  \n  \n    8 \n    6 \n    5 \n    16 \n    2 \n  \n  \n    5 \n    12 \n    8 \n    10 \n    2 \n  \n  \n    17 \n    5 \n    9 \n    1 \n    3 \n  \n\n\n\n\n\n\n(b) 5 x 1 column vector \n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\n\n\n\n(c) 5 X 1 product vector \n\n  \n    163 \n  \n  \n    214 \n  \n  \n    115 \n  \n  \n    142 \n  \n  \n    131"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#multiplying-matrices-times-the-all-ones-vector",
    "href": "lesson-matrix-multiplication.html#multiplying-matrices-times-the-all-ones-vector",
    "title": "16  Matrix Multiplication",
    "section": "16.6 Multiplying Matrices Times the All Ones Vector",
    "text": "16.6 Multiplying Matrices Times the All Ones Vector\nIn matrix multiplication, there is a special row and column vector called the all ones vector. As you may have guessed this is a vector of all ones, of some length \\(n\\). For instance and all ones row vector of length five is \\(\\mathbf{1}_{1 \\times 5} = \\{1, 1, 1, 1, 1\\}\\) (the symbol for the all ones vector is a boldface “1”). We can also get the transpose of this all ones row vector to get the all ones column vector \\(\\mathbf{1}^T\\).\nWhy do we care about vectors full of ones? Well, it turns out that the all one row and column vectors have a neat property when we multiplied by matrices. We already know, from the rules of vector matrix multiplication reviewed earlier, that the product of a row vector times a square matrix is always a row vector of the same length as the original, and the product of a square matrix times a column vector is always a column vector of the same length as the original.\nLet’s say we a matrix \\(\\mathbf{A}\\) of dimensions \\(5 \\times 5\\), and we multiplied the all ones row vector of length five times this matrix, which would result in the row vector \\(\\mathbf{b}\\). This would be given by the formula:\n\\[\n\\mathbf{1}_{1 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = b_{1 \\times 5}\n\\tag{16.9}\\]\nA numerical example of the situation depicted in Equation 16.9 is shown in Table 16.11.\n\n\nTable 16.11: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 1 X 5 all ones row vector \n\n  \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 5 product row vector \n\n  \n    3 \n    5 \n    2 \n    3 \n    1 \n  \n\n\n\n\n\n\nIf you look at the resulting row vector in Table 16.11 (c), we can see that the result of multiplying the all ones row vector times a matrix is a vector that contains the column sums of the matrix entries! So the “2” in position 1 of Table 16.11 (c) comes from adding up the numbers in the first column of the matrix, the “1” in position 2 of Table 16.11 (c) comes from adding the numbers in the second column and so forth.\n\n\nTable 16.12: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) 5 X 1 all ones column vector \n\n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n\n\n\n\n\n\n(c) 5 X 1 product column vector \n\n  \n    2 \n  \n  \n    3 \n  \n  \n    4 \n  \n  \n    2 \n  \n  \n    3 \n  \n\n\n\n\n\n\nIn the same way, if we multiply the same matrix times the all ones column vector, we get the results shown in Table 16.12. We can see that the result of multiplying a matrix times the all ones column vector, is another column vector contains the row sums of the original matrix! So, the “2” in the first position of the column vector comes from adding the numbers in the first row of the matrix, the “3” comes from adding the numbers in the second row, and so forth."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#the-identity-matrix",
    "href": "lesson-matrix-multiplication.html#the-identity-matrix",
    "title": "16  Matrix Multiplication",
    "section": "16.7 The Identity Matrix",
    "text": "16.7 The Identity Matrix\nThe last “interesting” matrix we will cover is called the identity matrix. This is a square matrix, usually written using the symbol \\(\\mathbf{I}\\) of dimensions \\(n \\times n\\). This matrix will have “1” in every diagonal cell, and “0” in every off-diagonal cell. For instance, an identity matrix of dimensions \\(5 \\times 5\\) is shown Table 16.13.\n\n\n\n\n\n\n  \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\nTable 16.13:  A 5 X 5 Identity Matrix. \n\n\nThe interesting thing about this matrix is that when you multiply it times another square matrix of the same dimensions, the result is always the original matrix! So it plays the role that the number “1” plays in regular number multiplication, in matrix algebra. This means that, for any square matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} \\times \\mathbf{I} = \\mathbf{A}\n\\tag{16.10}\\]\nAnd also,\n\\[\n\\mathbf{I} \\times \\mathbf{A} = \\mathbf{A}\n\\tag{16.11}\\] Neat!"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#references",
    "href": "lesson-matrix-multiplication.html#references",
    "title": "16  Matrix Multiplication",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "lesson-theory-equivalence.html#the-position-approach",
    "href": "lesson-theory-equivalence.html#the-position-approach",
    "title": "17  Equivalence and Similarity",
    "section": "17.1 The Position Approach",
    "text": "17.1 The Position Approach\nThe basic idea behind the position approach to dividing up the nodes in a graph is to come up with a measure of how similar two nodes are in terms of their patterns of connectivity with others. This measure then can be used to partition the nodes into what are called equivalence or similarity classes. Nodes in the same equivalence class are said to occupy the same position in the social structure described by the network.\nThere are two main ways to partition nodes into equivalence classes. The first is based on the idea that two nodes occupy the same position is they have similar patterns of connectivity to the same other nodes in the graph. This is called structural equivalence (Lorrain and White 1971).\nThe second is based on the idea that two nodes are equivalent if they are connected to people who are themselves equivalent, even if these are not literally the same people. This is called regular equivalence (White and Reitz 1983).\nThis and the following lessons will deal mainly with various ways of partitioning the nodes in a network based on structural equivalence and its more relaxed cousin, structural similarity."
  },
  {
    "objectID": "lesson-theory-equivalence.html#sec-equiv",
    "href": "lesson-theory-equivalence.html#sec-equiv",
    "title": "17  Equivalence and Similarity",
    "section": "17.2 Structural Equivalence",
    "text": "17.2 Structural Equivalence\nTwo nodes are structurally equivalent if they are connected to the same others. Thus, their patterns of connectivity (e.g., their row in the adjacency matrix) is exactly the same.\n\n\n\n\n\nFigure 17.1: An undirected graph with nodes colored by membership in the same structural equivalence class.\n\n\n\n\nFor instance in Figure 17.1, nodes C and D are structurally equivalent because they are connected to the same neighbors \\(\\{A, B, E\\}\\). In the same way, nodes A and B are structurally equivalent because they are connected to the same neighbors \\(\\{C, D\\}\\). Finally, nodes E and F occupy unique positions in the network because their neighborhoods are not equivalent to that of any other nodes. Node E is the only node that has a neighborhood composed of nodes \\(\\{C, D, F\\}\\), and node F is the only node that has a neighborhood composed of node \\(\\{E\\}\\) only. Perhaps F is the main boss, and E is the second in command.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    D \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 17.1:  Adjancency Matrix of an Undirected Graph \n\n\nWe can also see by looking at Table 17.1) that, indeed, the rows corresponding to the structurally equivalent nodes \\(\\{A, B\\}\\) and \\(\\{C, D\\}\\) in the corresponding adjacency matrix are indistinguishable from one another. The nodes that have unique positions in the network \\(\\{E, F\\}\\), also have a unique pattern of 0s and 1s across the rows of the adjacency matrix."
  },
  {
    "objectID": "lesson-theory-equivalence.html#references",
    "href": "lesson-theory-equivalence.html#references",
    "title": "17  Equivalence and Similarity",
    "section": "References",
    "text": "References\n\n\n\n\nLorrain, Francois, and Harrison C White. 1971. “Structural Equivalence of Individuals in Social Networks.” The Journal of Mathematical Sociology 1 (1): 49–80.\n\n\nWhite, Douglas R, and Karl P Reitz. 1983. “Graph and Semigroup Homomorphisms on Networks of Relations.” Social Networks 5 (2): 193–234."
  },
  {
    "objectID": "lesson-sna-local-similarity.html#structural-equivalence-and-local-similarity",
    "href": "lesson-sna-local-similarity.html#structural-equivalence-and-local-similarity",
    "title": "18  Local Node Similarities",
    "section": "18.1 Structural Equivalence and Local Similarity",
    "text": "18.1 Structural Equivalence and Local Similarity\nIf two nodes A and B are structurally equivalent then their neighborhood overlap \\(O(A, B)\\) is equivalent to their total set of neighbors, meaning that the cardinality of their neighborhood sets is the same as the cardinality of the intersection of those sets. In mathese, for structurally equivalent nodes:\n\\[\nN(A) = N(B)\n\\] \\[\nO(A,B) = |N(A) \\cap N(B)| = |N(A)| = |N(B)|\n\\tag{18.1}\\]\nEquation 18.1 says that when two nodes are structurally equivalent, their overlap is the same as the cardinality as the intersection between their neighborhoods, which happens to be the same as the cardinality of their original neighborhoods!\nFor instance, imagine you have a friend and that friend knows all your friends and you know all their friends. In which case we would say that the overlap between your node neighborhoods is pretty high; in fact the two neighborhoods overlap completely, which makes you structurally equivalent!\nBut even if your friend knows 90% of the people in your network (and you know 90% of the people in their network) that would make you very structurally similar to one another.\nNow imagine you just met a new person online who lives in a far away country, and as far as you know, they know none of your friends and you know none of their friends. In which case, we would say that the overlap if the two neighborhoods is nil or as close to zero as it can get. You occupy completely different positions in the network.\n\n\n\n\n\nFigure 18.1: An undirected graph."
  },
  {
    "objectID": "lesson-sna-local-similarity.html#the-neighborhood-overlap-matrix",
    "href": "lesson-sna-local-similarity.html#the-neighborhood-overlap-matrix",
    "title": "18  Local Node Similarities",
    "section": "18.2 The Neighborhood Overlap Matrix",
    "text": "18.2 The Neighborhood Overlap Matrix\nTo measure structural similarity between nodes based on their neighborhood overlap, we need to construct a new matrix, called the neighborhood overlap matrix that stores the neighborhood overlap information for each pair of nodes.\nConsider the graph shown in Figure 18.1. Its corresponding neighborhood overlap matrix is show in Table 18.1. In the matrix, each cell gives us the overlap between the row node and the column node. Because \\(O(i, j) = O(j, i)\\) for all nodes i and j in a graph, the neighborhood overlap matrix is symmetric (has the same information in the upper and lower triangles).\nTable 18.1 shows that some nodes like A and I have a very strong overlaps in their neighborhoods: \\(O(A, I) = 4\\), for other nodes, like \\(G\\) and \\(F\\), the overlap is much lower \\(O(G, F) = 1\\). For nodes that don’t have any neighbors in common, like B and H the overlap is the lowest it can be, namely, zero \\(O(B, H) = 0\\).\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    4 \n    0 \n    1 \n    3 \n  \n  \n    B \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    2 \n    0 \n    2 \n    0 \n    1 \n    1 \n  \n  \n    C \n    1 \n    0 \n    0 \n    2 \n    1 \n    2 \n    0 \n    3 \n    2 \n    2 \n    2 \n    2 \n  \n  \n    D \n    0 \n    1 \n    2 \n    0 \n    2 \n    1 \n    1 \n    3 \n    2 \n    3 \n    3 \n    1 \n  \n  \n    E \n    0 \n    0 \n    1 \n    2 \n    0 \n    0 \n    0 \n    2 \n    0 \n    2 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    2 \n    1 \n    0 \n    0 \n    1 \n    0 \n    3 \n    1 \n    1 \n    1 \n  \n  \n    G \n    1 \n    2 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    2 \n    0 \n    1 \n    1 \n  \n  \n    H \n    0 \n    0 \n    3 \n    3 \n    2 \n    0 \n    0 \n    0 \n    1 \n    3 \n    1 \n    1 \n  \n  \n    I \n    4 \n    2 \n    2 \n    2 \n    0 \n    3 \n    2 \n    1 \n    0 \n    2 \n    2 \n    3 \n  \n  \n    J \n    0 \n    0 \n    2 \n    3 \n    2 \n    1 \n    0 \n    3 \n    2 \n    0 \n    1 \n    1 \n  \n  \n    K \n    1 \n    1 \n    2 \n    3 \n    1 \n    1 \n    1 \n    1 \n    2 \n    1 \n    0 \n    2 \n  \n  \n    L \n    3 \n    1 \n    2 \n    1 \n    1 \n    1 \n    1 \n    1 \n    3 \n    1 \n    2 \n    0 \n  \n\n\n\nTable 18.1:  Neighborhood overlap matrix of an undirected graph. \n\n\n\n18.2.1 Jaccard Similarity\nOnce we have recorded the neighborhood overlap information we can construct a measure of structural similarity between two nodes called Jaccard’s Similarity Coefficient (\\(J_{ij}\\)) (Jaccard 1901). Actually we will need one more bit of information before computing this measure (and the other ones that follow), which is the graph’s degree set. This is shown in Table 18.2.\n\n\n\n\n\n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    4 \n    2 \n    6 \n    5 \n    2 \n    3 \n    2 \n    5 \n    7 \n    4 \n    4 \n    4 \n  \n\n\n\nTable 18.2:  Degree set of an undirected graph. \n\n\nThe Jaccard similarity between two nodes i and j goes like this. Let’s say \\(n_{ij}\\) is the number of friends that nodes i and j have in common, and the total number of i’s friends if \\(k_i\\) (i’s degree) and the total number of j’s friends if \\(k_j\\) (j’s degree). Then the structural similarity of i and j is given by:\n\\[\n  J_{ij} = \\frac{n_{ij}}{k_i + k_j - n_{ij}}\n\\tag{18.2}\\]\nEquation 18.2 says that the structural similarity of two nodes is equivalent to the number of friends that the two persons know in common, divided by the sum of their degrees minus the number of people they know in common. Jaccard’s coefficient ranges from zer (when \\(n_{ij}=0\\) and the two nodes have no neighbors in common) to 1.0 (when \\(n_{ij} = k_i\\) and \\(n_{ij} = k_j\\) and the two nodes are structurally equivalent).\nFor instance, Table 18.1 tells us that nodes D and H have three neighbors in common: \\(n_{DH} = 3\\), and Table 18.2 tells us that the degree of D is \\(k_D = 5\\) and that the degree of H is also \\(k_H = 5\\). This means that the Jaccard similarity \\(J_{DH}\\) is:\n\\[\nJ_{DH} = \\frac{3}{5 + 5 - 3} = \\frac{3}{10-3}=\\frac{3}{7} = 0.43\n\\]\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    0.2 \n    0.11 \n    0 \n    0 \n    0.17 \n    0.2 \n    0 \n    0.57 \n    0 \n    0.14 \n    0.6 \n  \n  \n    B \n    -- \n    -- \n    0 \n    0.17 \n    0 \n    0.25 \n    1 \n    0 \n    0.29 \n    0 \n    0.2 \n    0.2 \n  \n  \n    C \n    -- \n    -- \n    -- \n    0.22 \n    0.14 \n    0.29 \n    0 \n    0.38 \n    0.18 \n    0.25 \n    0.25 \n    0.25 \n  \n  \n    D \n    -- \n    -- \n    -- \n    -- \n    0.4 \n    0.14 \n    0.17 \n    0.43 \n    0.2 \n    0.5 \n    0.5 \n    0.12 \n  \n  \n    E \n    -- \n    -- \n    -- \n    -- \n    -- \n    0 \n    0 \n    0.4 \n    0 \n    0.5 \n    0.2 \n    0.2 \n  \n  \n    F \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.25 \n    0 \n    0.43 \n    0.17 \n    0.17 \n    0.17 \n  \n  \n    G \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0 \n    0.29 \n    0 \n    0.2 \n    0.2 \n  \n  \n    H \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.09 \n    0.5 \n    0.12 \n    0.12 \n  \n  \n    I \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.22 \n    0.22 \n    0.38 \n  \n  \n    J \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.14 \n    0.14 \n  \n  \n    K \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.33 \n  \n  \n    L \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n  \n\n\n\nTable 18.3:  Structural similarity matrix based on Jaccard’s index. \n\n\nThe structural similarities for all nodes in Figure 18.1 based on Jaccard’s index are shown in Table 18.3.\n\n\n18.2.2 Dice Similarity\nA second measure of structural similarity between nodes based on the neighborhood overlap is the Dice Similarity Index. It goes like this (Dice 1945):\n\\[\n  D_{ij} = \\frac{2n_{ij}}{k_i + k_j}\n\\tag{18.3}\\]\nWhich says that the structural similarity between two nodes is equivalent to the twice the number of people the know in common, divided by the sum of their degrees.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    0.33 \n    0.2 \n    0 \n    0 \n    0.29 \n    0.33 \n    0 \n    0.73 \n    0 \n    0.25 \n    0.75 \n  \n  \n    B \n    -- \n    -- \n    0 \n    0.29 \n    0 \n    0.4 \n    1 \n    0 \n    0.44 \n    0 \n    0.33 \n    0.33 \n  \n  \n    C \n    -- \n    -- \n    -- \n    0.36 \n    0.25 \n    0.44 \n    0 \n    0.55 \n    0.31 \n    0.4 \n    0.4 \n    0.4 \n  \n  \n    D \n    -- \n    -- \n    -- \n    -- \n    0.57 \n    0.25 \n    0.29 \n    0.6 \n    0.33 \n    0.67 \n    0.67 \n    0.22 \n  \n  \n    E \n    -- \n    -- \n    -- \n    -- \n    -- \n    0 \n    0 \n    0.57 \n    0 \n    0.67 \n    0.33 \n    0.33 \n  \n  \n    F \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.4 \n    0 \n    0.6 \n    0.29 \n    0.29 \n    0.29 \n  \n  \n    G \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0 \n    0.44 \n    0 \n    0.33 \n    0.33 \n  \n  \n    H \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.17 \n    0.67 \n    0.22 \n    0.22 \n  \n  \n    I \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.36 \n    0.36 \n    0.55 \n  \n  \n    J \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.25 \n    0.25 \n  \n  \n    K \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.5 \n  \n  \n    L \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n  \n\n\n\nTable 18.4:  Structural similarity matrix based on Dice’s index. \n\n\nFor nodes D and H in Figure 18.1, this would be equal to:\n\\[\nD_{DH} = \\frac{2 \\times 3}{5 + 5} = \\frac{6}{10} = 0.60\n\\]\nThe structural similarities for all nodes in Figure 18.1 based on Dice’s index are shown in Table 18.4.\n\n\n18.2.3 Cosine Similarity\nA third and final measure of structural similarity between two nodes based on the neighborhood overlap is the cosine similarity between their respective neighborhoods (\\(C_{ij}\\)). This is given by:\n\\[\n  C_{ij} = \\frac{n_{ij}}{\\sqrt{k_ik_j}}\n\\tag{18.4}\\]\nWhich says that the structural similarity between two nodes is equivalent to the number of people they know in common divided by the square root of the product of their degrees (which is also referred to as the geometric mean of their degrees).\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    0.35 \n    0.2 \n    0 \n    0 \n    0.29 \n    0.35 \n    0 \n    0.76 \n    0 \n    0.25 \n    0.75 \n  \n  \n    B \n    -- \n    -- \n    0 \n    0.32 \n    0 \n    0.41 \n    1 \n    0 \n    0.53 \n    0 \n    0.35 \n    0.35 \n  \n  \n    C \n    -- \n    -- \n    -- \n    0.37 \n    0.29 \n    0.47 \n    0 \n    0.55 \n    0.31 \n    0.41 \n    0.41 \n    0.41 \n  \n  \n    D \n    -- \n    -- \n    -- \n    -- \n    0.63 \n    0.26 \n    0.32 \n    0.6 \n    0.34 \n    0.67 \n    0.67 \n    0.22 \n  \n  \n    E \n    -- \n    -- \n    -- \n    -- \n    -- \n    0 \n    0 \n    0.63 \n    0 \n    0.71 \n    0.35 \n    0.35 \n  \n  \n    F \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.41 \n    0 \n    0.65 \n    0.29 \n    0.29 \n    0.29 \n  \n  \n    G \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0 \n    0.53 \n    0 \n    0.35 \n    0.35 \n  \n  \n    H \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.17 \n    0.67 \n    0.22 \n    0.22 \n  \n  \n    I \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.38 \n    0.38 \n    0.57 \n  \n  \n    J \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.25 \n    0.25 \n  \n  \n    K \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    0.5 \n  \n  \n    L \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n    -- \n  \n\n\n\nTable 18.5:  Structural similarity matrix based on the cosine distance. \n\n\nFor nodes D and H in Figure 18.1, this would be equal to:\n\\[\nD_{DH} = \\frac{3}{\\sqrt{5 \\times 5}} = \\frac{3}{\\sqrt{25}} = \\frac{3}{5} = 0.60\n\\]\nThe structural similarities for all nodes in Figure 18.1 based on the cosine distance are shown in Table 18.5.\nA lot of the times, these three measures of structural similarity will tend to agree closely.\n\n\n18.2.4 Neighborhood Overlap in Directed Graphs\nSimilarity works in similar (pun intended) ways when studying asymmetric ties in directed graph. The main difference, as usual, is that in a directed graph pairs of nodes can structurally similar in two different ways.\nFist, pairs of nodes can be similar with respect to their out-neighborhoods, in which case we say that nodes are structural similar if they point to the same set of neighbors. This is called the out-similarity.\nSecond, pairs of nodes can be similar with respect to their in-neighborhoods, in which case we say that nodes are structural similar if they receive ties or nominations from the same set of neighbors. This is called the in-similarity.\nSpecial cases of the out and in-similarities between nodes show up in particular types of networks. For instance, consider an information network composed of scientific papers. Here a directed tie emerges when paper A cites or refers to paper B. This is called a citation network.\nIn a citation network out-similar papers are papers that cite the same other papers. Out-similar papers are said to exhibit bibliographic coupling (essentially the overlap or set intersection between their reference lists). A weighted network of similarities between papers, where the weight of the edge is the number of other papers that that they both cite in common is called a bibliographic coupling network. A bibliographic coupling network is essentially a network of out-similarities between papers in a scientific information network.\nIn a citation network, in-similar papers are papers that get cited by the same set of others. In this case, we say that the two papers are co-cited a third paper. A weighted network of similarities between papers, where the weight of the edge is the number of other papers that cite both of them is called co-citation network. A co-citation network is essentially a network of in-similarities between papers in a scientific information network.\nThe two measures of out and in-similarities can be defined in the same way as before. If \\(n^{out}_{ij}\\) is the number of common out-neighbors of nodes i and j and \\(n^{in}_{ij}\\) is the number of their common out-neighbors, \\(k_{out}\\) is the total number of out-neighbors of a particular node, and \\(k_{in}\\) is the total number of in-neighbors, then the structural out and in-similarities between pairs of nodes i and j are given by (using the cosine distance measure) by:\n\\[\n  C_{ij}^{out} = \\frac{n_{ij}^{out}}{\\sqrt{k_i^{out}k_j^{out}}}\n\\tag{18.5}\\]\n\\[\n  C_{ij}^{in} = \\frac{n_{ij}^{in}}{\\sqrt{k_i^{in}k_j^{in}}}\n\\tag{18.6}\\]"
  },
  {
    "objectID": "lesson-sna-local-similarity.html#references",
    "href": "lesson-sna-local-similarity.html#references",
    "title": "18  Local Node Similarities",
    "section": "References",
    "text": "References\n\n\n\n\nDice, Lee R. 1945. “Measures of the Amount of Ecologic Association Between Species.” Ecology 26 (3): 297–302.\n\n\nJaccard, Paul. 1901. “Distribution of the Alpine Flora in the Dranse’s Basin and Some Neighbouring Regions.” Bulletin de La Societe Vaudoise Des Sciences Naturelles 37 (1): 241–72."
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#the-correlation-distance",
    "href": "lesson-sna-blockmodeling.html#the-correlation-distance",
    "title": "19  Blockmodeling",
    "section": "19.1 The Correlation Distance",
    "text": "19.1 The Correlation Distance\nIt turns out there is an even fancier way to find out whether two nodes in a graph are structurally similar. It relies on a more complicated measure of distance called the correlation distance. This measure compares the row (or column) vectors of nodes in a graph and returns a number between \\(-1\\) and \\(+1\\). When it comes to structural equivalence, the correlation distance works like this:\n\nPairs of structurally equivalent nodes get a \\(+1\\). Nodes that are almost structurally equivalent but not quite (they are structurally similar) get a positive number larger than zero. The closer that number is to \\(+1\\) the more structurally similar the two nodes are.\nNodes that are completely different from one another (that is connect to completely disjoint sets of neighbors) get a \\(-1\\). In this case, nodes are opposites: Every time one node i has a \\(1\\) in their row vector in the adjacency matrix, the other has a \\(0\\) and vice versa. Nodes that are structurally dissimilar thus get a negative number between zero and \\(-1\\). The closer that number is to \\(-1\\), the more structurally dissimilar the two nodes are.\nNodes that have an even combination of similarities and differences in their pattern of connectivity to others get a number close to zero, with \\(0\\) indicating that two nodes have an even number of commonalities and differences.\n\nThe correlation distance between two nodes k and l is computed using the following formula:\n\\[\n    d^{corr}_{k, l} =\n    \\frac{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j}) \\times\n    (a_{(l)j} - \\overline{a}_{(l)j})\n    }\n    {\n    \\sqrt{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j})^2 \\times\n    \\sum_j\n    (a_{(l)j} - \\overline{a}_{(l)j})^2\n        }\n    }\n\\tag{19.1}\\]\nEquation 19.1 looks like a monstrously complicated one, but it is actually not that involved.\nLet’s go through each components that we have already encountered in the lesson structural equivalence and structural similarity:\n\n\\(a_{(k)j}\\) is the row (or column) vector for node k in the adjacency matrix.\n\\(a_{(l)j}\\) is the row (or column) vector for node l in the adjacency matrix.\n\nNow let’s introduce ourselves to some new friends. For instance, what the heck is \\(\\overline{a}_{(k)j}\\)? The little “bar” on top the \\(a\\) indicates that we are taking the mean or the average of the elements of the row vector.\nIn equation form:\n\\[\n\\overline{a}_{(k)j} = \\frac{\\sum_j a_{kj}}{N}\n\\tag{19.2}\\]\nIn Equation 19.2, \\(\\sum_i a_{kj}\\) is the sum of all the elements in the vector, and \\(N\\) is the length of the vector, which is equivalent to the order of the graph from which adjacency matrix came from (the number of nodes in the network).\nSo for instance, in Table 17.1, the row vector for node A is:\n\n\n\n\nTable 19.1: Row vector for node A.\n\n\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\nWhich implies:\n\\[\n\\sum_i a_{(A)j} = 0 + 0 + 1 + 1 + 0 + 0 = 2\n\\]\nAnd we know that \\(N = 6\\), so that implies:\n\\[\n\\overline{a}_{(A)j} = \\frac{\\sum_i a_{Aj}}{N} = \\frac{2}{6}=0.33\n\\] The term \\(\\overline{a}_{(k)j}\\) is called the row mean for node k in the adjacency matrix. Just like we can compute row means, we can also compute column means by using the elements of the column vector \\(\\overline{a}_{(k)i}\\)\nNow that we know what the row means are, we can make sense of the term \\((a_{(k)j} - \\overline{a}_{(k)j})\\) in Equation 19.1. This is a vector composed of the differences between the row vector entries in the adjacency matrix and the row mean for that node. So in the case of node A and the row vector in Table 19.1 that would imply:\n\n\nTable 19.2: Row vector of mean differences for node A.\n\n\n\n\n(a) Vector of row entries minus the row mean.\n\n\n\n\n\n\n\n\n\n\n(0 - 0.33)\n(0 - 0.33)\n(1 - 0.33)\n(1 - 0.33)\n(0 - 0.33)\n(0 - 0.33)\n\n\n\n\n\n\n\n\n(b) Result of subtracting row mean from row entries.\n\n\n-0.33\n-0.33\n0.67\n0.67\n-0.33\n-0.33\n\n\n\n\n\n\nWhich implies: \\[\n\\sum_j (a_{(k)j} - \\overline{a}_{(k)j}) = -0.33 -0.33 + 0.67 + 0.67 -0.33 -0.33 = 0.02\n\\]\nThe numerator of Equation 19.1, just says: “Take the entries in the row vector for the first node, and create a new vector composed of the those entries minus the row means and sum the vector. Then do the same for the other node and multiply the two numbers” And in the denominator of the equation we just square the same vectors sum them, multiply each of the two numbers and take the square root of the result product. Once we have the numerator and denominator we can evaluate the fraction and compute the correlation distance between those two nodes.\nWhen we do that for each pair of nodes in Table 17.1, we end up with the structural similarity matrix shown in Table 19.3, based on the correlation distance.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.71\n-0.71\n0.71\n-0.32\n\n\nB\n1\n–\n-0.71\n-0.71\n0.71\n-0.32\n\n\nC\n-0.71\n-0.71\n–\n1\n-1\n0.45\n\n\nD\n-0.71\n-0.71\n1\n–\n-1\n0.45\n\n\nE\n0.71\n0.71\n-1\n-1\n–\n-0.45\n\n\nF\n-0.32\n-0.32\n0.45\n0.45\n-0.45\n–\n\n\n\nTable 19.3: Correlation distance matrix for an undirected graph.\n\n\nIn Table 19.3, the structurally equivalent pairs of nodes, A and B and C and D have \\(d^{corr} = 1.0\\). Nodes that are completely non-equivalent like C and E and D and E have \\(d^{corr} = -1.0\\). Nodes that are structurally similar, but not equivalent, like nodes C and F (\\(d^{corr} = 0.45\\)) get a positive number that is less than \\(1.0\\)."
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#iterated-correlational-distances-concor",
    "href": "lesson-sna-blockmodeling.html#iterated-correlational-distances-concor",
    "title": "19  Blockmodeling",
    "section": "19.2 Iterated Correlational Distances: CONCOR",
    "text": "19.2 Iterated Correlational Distances: CONCOR\nWhat happens if we were to try to use Equation 19.1 to find the correlation distance of a correlation distance matrix? If we were to do this and use Table 19.3 as our input matrix, we end up with Table 19.4 (a).\n\n19.2.1 We Need to go Deepah\nNow, as Leo (when stuck in a dream, about a dream, about a dream…) always says: “We need to go deeper.”1 And, indeed, we can. We can take the correlation distance of the nodes based on Table 19.4 (a). If we do that, we end up with the entries in Table 19.4 (b). If we keep on going, we end up with the entries in Table 19.4 (c). Note that in Table 19.4 (c), there are only two values for all the entries: \\(+1\\) and \\(-1\\)!\n\n\nTable 19.4: Iterated correlation distances.\n\n\n\n\n(a) Second Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.97\n-0.97\n0.97\n-0.84\n\n\nB\n1\n–\n-0.97\n-0.97\n0.97\n-0.84\n\n\nC\n-0.97\n-0.97\n–\n1\n-1\n0.84\n\n\nD\n-0.97\n-0.97\n1\n–\n-1\n0.84\n\n\nE\n0.97\n0.97\n-1\n-1\n–\n-0.84\n\n\nF\n-0.84\n-0.84\n0.84\n0.84\n-0.84\n–\n\n\n\n\n\n\n(b) Third Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-0.99\n\n\nB\n1\n–\n-1\n-1\n1\n-0.99\n\n\nC\n-1\n-1\n–\n1\n-1\n0.99\n\n\nD\n-1\n-1\n1\n–\n-1\n0.99\n\n\nE\n1\n1\n-1\n-1\n–\n-0.99\n\n\nF\n-0.99\n-0.99\n0.99\n0.99\n-0.99\n–\n\n\n\n\n\n\n\n\n(c) Fourth Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-1\n\n\nB\n1\n–\n-1\n-1\n1\n-1\n\n\nC\n-1\n-1\n–\n1\n-1\n1\n\n\nD\n-1\n-1\n1\n–\n-1\n1\n\n\nE\n1\n1\n-1\n-1\n–\n-1\n\n\nF\n-1\n-1\n1\n1\n-1\n–\n\n\n\n\n\n\nMore importantly, as shown in Table 19.5, the structurally equivalent nodes have exactly the same pattern of \\(+1\\)s and \\(-1\\)s across the rows. This procedure of iterated correlations, invented by a team of sociologists and psychologists at Harvard University in the mid-1970s (Breiger, Boorman, and Arabie 1975), is called CONCOR—and acronym for the hard to remember title of “convergence of iterate correlations’’—and is designed to extract structurally equivalent positions from networks even when the input matrix is just based on structural similarities.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    -- \n    1 \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    B \n    1 \n    -- \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    C \n    -1 \n    -1 \n    -- \n    1 \n    -1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    1 \n    -- \n    -1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    -1 \n    -1 \n    -- \n    -1 \n  \n  \n    F \n    -1 \n    -1 \n    1 \n    1 \n    -1 \n    -- \n  \n\n\n\nTable 19.5:  Structurally Equivalent Positions in an undirected graph."
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#blockmodeling",
    "href": "lesson-sna-blockmodeling.html#blockmodeling",
    "title": "19  Blockmodeling",
    "section": "19.3 Blockmodeling",
    "text": "19.3 Blockmodeling\nThe example we considered previously concerns the relatively small network shown in Figure 17.1. What happens when we apply the method of iterated correlations (CONCOR) to a bigger network, something like the one shown in Figure 19.1?\n\n\n\n\n\nFigure 19.1: An undirected graph.\n\n\n\n\nWell, we can begin by computing the correlation distance across all ,the \\(V=22\\) nodes in that network using Equation 19.1. The result of that looks like Table 19.6 (a). Note that even before we do any iterated correlations of correlation matrices we can see that the peripheral, single-connection nodes \\(E, F, G, H\\), \\(I, J, K, L, M\\) and \\(N, O, P, Q, R\\) are perfectly structurally equivalent. This makes sense, because all the nodes in each of these three groups have identical neighborhoods, since they happen to be connected to the same central node \\(A\\) for the first group, \\(B\\) for the second group and \\(C\\) for the third group. Note also that \\(U\\) and \\(V\\) are structurally equivalent, since their neighborhoods are the same: Their single connection is to node \\(S\\).\n\n\nTable 19.6: Correlation Distance Matrices Corresponding to an Undirected Graph.\n\n\n\n\n(a) Original Correlation Distance Matrix. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1.00 \n    -0.15 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    0.05 \n    -0.13 \n    -0.13 \n  \n  \n    B \n    -0.15 \n    1.00 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    C \n    -0.15 \n    -0.15 \n    1.00 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    D \n    -0.24 \n    -0.24 \n    -0.24 \n    1.00 \n    0.34 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    -0.16 \n    -0.09 \n    -0.09 \n  \n  \n    T \n    -0.19 \n    -0.19 \n    -0.19 \n    0.34 \n    1.00 \n    0.69 \n    0.69 \n    0.69 \n    0.69 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.13 \n    0.69 \n    0.69 \n  \n  \n    E \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    F \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    G \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    H \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    I \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    J \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    K \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    L \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    M \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    N \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    O \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    P \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    Q \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    R \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    S \n    0.05 \n    -0.24 \n    -0.24 \n    -0.16 \n    -0.13 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    1.00 \n    -0.09 \n    -0.09 \n  \n  \n    U \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n  \n    V \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(b) Original Correlation Distance Matrix After Ten Iterations. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(c) Correlation Distance Matrix in (a) with Rows and Columns Reshuffled to Show Hidden Pattern. \n \n  \n      \n    V \n    U \n    S \n    H \n    G \n    F \n    E \n    T \n    C \n    A \n    B \n    R \n    Q \n    P \n    O \n    N \n    M \n    L \n    K \n    J \n    D \n    I \n  \n \n\n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWhat happens when we take the correlation distance of the correlation distance matrix shown in Table 19.6 (a), and the correlation distance of the resulting matrix, and keep going until we only have zeros and ones? The results is Table 19.6 (b). This matrix seems to reveal a much deeper pattern of commonalities in structural positions across the nodes in Figure 19.1. In fact, if we were a conspiracy theorist like Charlie from Always Sunny in Philadelphia, we might even surmise that there is a secret pattern that can be revealed if we reshuffled the rows and the columns of the matrix (without changing any of the numbers of course!).2\nIf we do that, we end up with Table 19.6 (c). So it turns out that there is indeed a secret pattern! The reshuffling shows that the nodes in the network can be divided into two blocks such within blocks all nodes are structurally similar (and some structurally equivalent) and across blocks, all nodes are structurally dissimilar. Thus \\(V, U, S, H, G, F, E, T, C, A, B\\) are members of one structurally similar block (let’s called them “Block 1”), and nodes \\(R, Q, P, O, N, M, L, K, J, D, I\\) are members of another structurally similar block (let’s called them “Block 2”). Nodes in “Block 1” are structurally dissimilar from nodes in “Block 2,” but structurally similar to one another and vice versa. To illustrate, Figure 19.2 is the same as Figure 19.1, but this time nodes are colored by their memberships in two separate blocks.\n\n\n\n\n\nFigure 19.2: An undirected graph with block membership indicated by node color.\n\n\n\n\nNote that we haven’t changed any of the information in Table 19.6 (b) to get Table 19.6 (c). If you check, the row and column entries for each node in both figures are identical. It’s just that we changed the way the rows ordered vertically and the way the columns are ordered horizontally. For instance, node \\(A\\)’s pattern of connections is negatively correlated with node \\(I\\)’s in Table 19.6 (b), and has the same negative correlation entry in Table 19.6 (c). The same goes for each one of node \\(A\\)’s other correlations, and the same for each node in the table. Table 19.6 (b) and Table 19.6 (c) contain the same information it’s just that Table 19.6 (c) makes it easier to see a hidden pattern.\nThis property of the method of iterated correlations is the basis of a strategy for uncovering blocks of structurally similar actors in a network developed by a team of sociologists, physicists, and mathematicians working at Harvard in the 1970s. The technique is called blockmodeling (White, Boorman, and Breiger 1976). Let’s see how it works.\n\n19.3.1 We Need to go Deepah\nOf course, as Leo always says: “We need to go deeper.” And indeed we can. What happens if we do the same analysis as above, but this time in the two node-induced subgraphs defined by the set of structurally similar nodes in each of the two blocks we uncovered in the original graph? Then we get Table 19.7 (a) and Table 19.7 (b).\n\n\nTable 19.7: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    B \n    A \n    C \n    S \n    V \n    U \n    T \n    E \n    F \n    H \n    G \n  \n \n\n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    I \n    J \n    K \n    M \n    L \n    D \n    N \n    O \n    P \n    R \n    Q \n  \n \n\n  \n    I \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWe can see that Table 19.6 (a) separates our original Block 1 into two further sub-blocks. Let’s call them “Block 1a” and “Block 1b.” Block 1a is composed of nodes \\(A, B, C, S, U, V\\) and Block 1b is composed of nodes \\(E, F, G, H, T\\). Table 19.6 (b) separates our original Block 2 into three further sub-blocks. There’s the block composed of nodes \\(I, J, K, L, M\\). Let’s call this “Block 2a”, the block composed of nodes \\(N, O, P, Q, R\\). Let’s call this “Block 2b.” Then, there’s node \\(D\\). Note that this node is only structurally similar to itself and is neither similar nor dissimilar to the other nodes in the subgraph \\(d^{corr} = 0\\), so it occupies a position all by itself! Let’s call it “Block 2c.”\n\n\nTable 19.8: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    S \n    C \n    B \n    A \n    V \n    U \n  \n \n\n  \n    S \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    V \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    U \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    B \n    C \n    A \n    S \n  \n \n\n  \n    B \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    S \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\nNow let’s do a couple of final splits of the subgraph composed of nodes \\(A, B, C, S, U, V\\). This is shown in Table 19.8. The first split separates nodes in block \\(A, B, C, S\\) from those in block \\(U, V\\) (Table 19.8 (a)). The second splits the nodes in subgraph \\(A, B, C, S\\) into two blocks composed of \\(A, S\\) and \\(B, C\\), respectively (Table 19.8 (b)).\n\n\n\n\n\nFigure 19.3: An undirected graph with block membership indicated by node color.\n\n\n\n\nFigure 19.3 shows the nodes in Figure 19.1 colored according to our final block partition. It is clear that the blockmodeling approach captures patterns of structural similarity. For instance, all the single-connection nodes connected to more central nodes get assigned to their own position: Block 1b: \\(E, F, G, H, T\\), Block 2a: \\(I, J, K, L, M\\), and Block 2b: \\(N, O, P, Q, R\\). The most central node \\(D\\) (in terms of Eigenvector centrality) occupies a unique position in the graph. Two of the three central nodes (in terms of degree centrality) \\(B, C\\) get assigned to their own position. Meanwhile \\(A, S\\) form their own structurally similar block. Finally, \\(U, V\\) also form their own structurally similar block as both are structurally equivalent in the orignal graph.\n\n\n19.3.2 The Blocked Adjancency Matrix\nWhat happens if we were to go back fo the adjacency matrix corresponding to Figure 19.1, and then reshuffle the rows and columns to correspond to all these wonderful blocks we have uncovered? Well, we would en up with something like Table 19.9. This is called the blocked adjacency matrix. In the blocked adjacency matrix, the division between the nodes corresponding to each block of structurally similar nodes in Table 19.7 and Table 19.8 is marked by thick black lines going across the rows and columns.\nEach diagonal rectangle in Table 19.9 corresponds to within-block connections. Each off-diagonal rectangle corresponds to between block connections. There are two kinds of rectangles in the blocked adjacency matrix. First, there are rectangles that only contains zero entries. These are called zero blocks. For instance the top-left rectangle in Table 19.9 is a zero block. Then there rectangles that have some non-zero entries in them (ones, since this is a binary adjacency matrix). These are called one blocks. For instance, the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) is a one block.\n\n\n\n\n\n \n  \n      \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    T \n    E \n    F \n    G \n    H \n    B \n    C \n    A \n    S \n    U \n    V \n    D \n  \n \n\n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    M \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    O \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    P \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    Q \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    R \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    T \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    S \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n  \n  \n    U \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    V \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\nTable 19.9:  Blocked adjancency matrix. \n\n\nZero-blocks indicate that the members of the row block don’t have any connections with the members the column block (which can include themselves!). For instance, the zero-block in the top-left corner of the blocked adjacency matrix in Table 19.9 indicates that the members of this block are not connected to one another in the network (and we can verify from Figure 19.3 that this is indeed the case).\nOne blocks indicate that the members of the column block share some connections with the members of the column block (which can also include themselves!). For instance, the one-block in the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) tells us that members of this block are connected to at least one member of the \\(I, J, K, L, M\\) block (and we can verify from Figure 19.3 that this is indeed the case, since \\(B\\) is connected to all of them).\n\n\n19.3.3 The Image Matrix\nFrom this reshuffled adjacency matrix, we can get to a reduced image matrix containing the relations not between the nodes in the graph, but between the blocks in the graph. The way we proceed to construct the image matrix is as follows:\n\nFirst we create an empty matrix \\(\\mathbf{B}\\) of dimensions \\(b \\times b\\) where \\(B\\) is the number of blocks in the blockmodel. In our example, \\(b = 7\\) so the image matrix has seven rows and seven columns. The \\(ij^{th}\\) cell in the image matrix \\(\\mathbf{B}\\) records the relationship between row block i and column block j in the blockmodel.\nSecond, we put a zero in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 19.9 is a zero-block.\nThird, we put a one in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 19.9 is a one-block.\n\nThe result is Table 19.10.\n\n\n\n\n\n \n  \n      \n    I, J, K, L \n    N, O, P, Q, R \n    T, E, F, G, H \n    B, C \n    A, S \n    U, V \n    D \n  \n \n\n  \n    I, J, K, L \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    N, O, P, Q, R \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    T, E, F, G, H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B, C \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A, S \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    U, V \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\nTable 19.10:  Image matrix Corresponding to the blockmodel of an Undirected Graph. \n\n\nSo the big blocked adjacency matrix in Table 19.6 (c) can be reduced to the image matrix shown Table 19.10, summarizing the relations between the blocks in the graph. This matrix, can then even be represented as a graph, so that we can see the pattern of relations between blocks! This is shown in Figure 19.4\n\n\n\n\n\nFigure 19.4: Graph representation of reduced image matrix from a blockmodel.\n\n\n\n\nThis is how blockmodeling works!"
  },
  {
    "objectID": "lesson-sna-blockmodeling.html#references",
    "href": "lesson-sna-blockmodeling.html#references",
    "title": "19  Blockmodeling",
    "section": "References",
    "text": "References\n\n\n\n\nBreiger, Ronald L, Scott A Boorman, and Phipps Arabie. 1975. “An Algorithm for Clustering Relational Data with Applications to Social Network Analysis and Comparison with Multidimensional Scaling.” Journal of Mathematical Psychology 12 (3): 328–83.\n\n\nWhite, Harrison C, Scott A Boorman, and Ronald L Breiger. 1976. “Social Structure from Multiple Networks. I. Blockmodels of Roles and Positions.” American Journal of Sociology 81 (4): 730–80."
  },
  {
    "objectID": "lesson-sna-centrality.html#the-big-three-centrality-metrics",
    "href": "lesson-sna-centrality.html#the-big-three-centrality-metrics",
    "title": "20  Centrality",
    "section": "20.1 The “big three” centrality metrics",
    "text": "20.1 The “big three” centrality metrics\nLinton Freeman (1979), in the aforementioned paper, defined the “big three” classic centrality metrics, roughly corresponding to the extent that a node accumulates one of the three network goods mentioned above. - So the degree centrality metric deal with nodes that have more edges directly incident upon them (Nieminen 1974). - The closeness centrality metric has to do with nodes that can reach more nodes via smallest shortest paths and thus accumulate as many of these paths in which they figure as the origin node as possible (Sabidussi 1966). - Finally, the betweenness centrality metric has to do with a node’s accumulation of the largest share of shortest paths in which they intermediate between two other nodes, and thus featuring them as one of the inner nodes in the paths between others (Freeman 1977).\nOther centrality metrics can be seen as generalizations or special cases of any of these three basic notions (Borgatti 2005).\nThe rest of the lesson goes over the basic interpretation and calculation (using the graph theory and matrix algebra tools discussed in previous lessons) of “big three” centrality metrics."
  },
  {
    "objectID": "lesson-sna-centrality.html#the-star-graph",
    "href": "lesson-sna-centrality.html#the-star-graph",
    "title": "20  Centrality",
    "section": "20.2 The Star Graph",
    "text": "20.2 The Star Graph\nFreeman showed that the three basic measures reach their theoretical maximum for the central node in a star graph, such as the one shown in Figure 20.1).\n\n\n\n\n\nFigure 20.1: A star graph\n\n\n\n\nA star graph is a graph containing a central or inner node (in Figure 20.1, node A), who is connected to all the other nodes in the graph, called the satellite or outer nodes (in Figure 20.1, nodes B through F). These nodes in contrast have only one connection and that is to the central node, none among themselves.\nBecause of these restrictions, it is easy to see that if \\(G = (V, E)\\) is a star graph of order \\(n\\), then we know that that graph size \\(m = |E|\\) (the size of the edge set), has to be \\(n-1\\). So in the example shown in Figure 20.1, \\(n =7\\) and \\(m = n-1 = 7-1=6\\). Neat!"
  },
  {
    "objectID": "lesson-sna-centrality.html#degree-centrality",
    "href": "lesson-sna-centrality.html#degree-centrality",
    "title": "20  Centrality",
    "section": "20.3 Degree Centrality",
    "text": "20.3 Degree Centrality\nThe most basic way of defining centrality is simply as a measure of how many alters an ego is connected to. This simply takes a node’s degree as introduced in the lesson on graph theory, and begins to consider this measure as a reflection of importance of the node in the network. The logic is that those with more direct connections to others, compared to those with fewer, hold a more prominent place in the network.\nOnce we have constructed the adjacency matrix for the network (A), then degree centrality is easy to calculate. As Equation 20.1 for a given node i the degree centrality is given by summing the entries of its corresponding row.\n\\[\n  C_i^{DEG} = \\sum_{j= 1}^{n}a_{ij}\n\\tag{20.1}\\]\nEquation 20.1 thus ranks each node in the graph based on the number of other nodes that it is adjacent to. Just like real life, some nodes will be popular (they will be adjacent to lots of other nodes), while others will be unpopular.\nAlthough it might seem a simple task to just add up the number of connections of each node, that is essentially what the below mathematical equation is doing! Mathematical notation plays an important role in expressing network measures in succinct formats.\nFor instance, if we were to use Equation 20.1 to calculate the degree centrality of each node from the symmetric adjacency matrix corresponding to the graph shown in Figure 4.1 then we would end up with the following degree centralities for each node:\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 20.1: Degree centralities of nodes in an undirected graph."
  },
  {
    "objectID": "lesson-sna-centrality.html#indegree-and-outdegree-centrality",
    "href": "lesson-sna-centrality.html#indegree-and-outdegree-centrality",
    "title": "20  Centrality",
    "section": "20.4 Indegree and Outdegree Centrality",
    "text": "20.4 Indegree and Outdegree Centrality\nIf we are talking about a directed graph, then there are two types of degree centralities that can be calculated. On the one hand, we may be interested in how central a node is in terms of sociability or expansiveness that is how many other nodes in the graph a given node sends links to. This is called the outdegree centrality of that node, written as \\(C_i^{OUT}\\). As with the undirected case, this is computed by summing across the rows of the asymmetric adjacency matrix corresponding to the directed graph in question, using Equation 20.1:\n\\[\n  C_i^{OUT} = \\sum_ja_{ij}\n\\tag{20.2}\\]\nHowever, in a directed graph, we may also be interested in how popular or sought after by others a given node is. That is, how many other actors send ties to that node. In which case we need to sum across the columns of the asymmetric adjacency matrix, and modify the formula as follows:\n\\[\n  C_j^{IN} = \\sum_ia_{ij}\n\\tag{20.3}\\]\nNote that in this version of the equation, we are summing over j (the columns) not over i (the rows) as given by subscript under the \\(\\sum\\) symbol.\nFor instance, if we were to use equations Equation 20.2 and Equation 20.2 to calculate the outdegree and indegree centrality of each node from the asymmetric adjacency matrix corresponding to the graph shown in Figure 4.2), then we would up with the following centralities for each node:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nOutdegre\n2\n2\n1\n1\n2\n1\n2\n\n\nIndegree\n2\n3\n1\n3\n0\n2\n0\n\n\n\nTable 20.2: Out and Indegree centralities of nodes in a directed graph.\n\n\nJust like the degree centrality for undirected graphs, the outdegree and indegree centralities rank each node in a directed graph. The first, outdegree centrality, ranks each node based on the number of other nodes that they are connected to. This is a kind of popularity based on sociability, or the tendency to seek out the company of others. The second, indegree centrality, ranks each node in the graph based on the number of other nodes that connect to that node. This is a kind of popularity based on on being sought after a kind of status.\n\n20.4.1 Normalized Degree Centrality\nWhen we compute the degree centrality of a node, are counting the number of other nodes that they are connected to. Obviously, the more nodes there are to connect to, the more opportunities there will be to reach a larger number. But what happens if we wanted to compare the degree centrality of nodes in two very different networks?\nFor instance, if your high-school has one thousand people and you have twenty friends, that’s very different from having twenty friends in a high-school of only one hundred people. It seems like the second person, with twenty friends (covering 20% of the population) in a high-school of one-hundred people is definitely more popular than the second person with twenty friends (covering 2% of the population), in a high school with one thousand people.\nThat’s why Freeman Freeman (1979) proposed normalizing the degree centrality of each node by the maximum possible it can take in a given network. As you may have guessed, the maximum degree in a network is \\(N-1\\) the order of the graph minus one. Essentialy, everyone but you!\nWe can compute the normalized degree centrality using the following equation:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{C_{i}^{DEG}}{N-1}\n\\tag{20.4}\\]\nWhere we just divide the regular degree centrality computed using Equation 20.1 by the order of the graph minus one. This will be equal to \\(1.0\\) if a person knows everyone and \\(0\\) is a person knows no one. For all the other nodes it will be a number between zero and one.\nMoreover, this measure is sensitive to the order of the graph. Thus, for a person with twenty friends in a high-school of a thousand people, the normalized degree centrality is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{1000-1}= 0.02\n\\tag{20.5}\\]\nBut for the person with the same twenty friends in a high-school of one-hundred people, it is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{100-1}= 0.20\n\\tag{20.6}\\]\nIndicating that a person with the same number of friends in the smaller place is indeed more central!"
  },
  {
    "objectID": "lesson-sna-centrality.html#closeness-centrality",
    "href": "lesson-sna-centrality.html#closeness-centrality",
    "title": "20  Centrality",
    "section": "20.5 Closeness Centrality",
    "text": "20.5 Closeness Centrality\nSometimes it not important how many people you directly connected to. Instead, what is important is that you are indirectly connected to a lot of others. As we saw in the lesson on indirect connectivity, the best way to conceptualize indirect connectivity in social networks is via the idea of shortest paths. So if you can reach the most other people in the network via shortest paths with only a few hops, then you are better connected that someone who has to use longer paths to reach the same other people.\n\n\n\n\n\nFigure 20.2: An undirected graph showing the node with the maximum closeness centrality (in red).\n\n\n\n\nThis insight serves as an inspiration for a measure of centrality based on closeness. The closeness between two nodes is the inverse of the geodesic distance them (Bavelas 1950). Recall that the geodesic distance is given by the length of the shortest path linking two nodes in the graph. The smallest the length of the shortest path separating two nodes in the graph, the closer the two nodes and vice versa.\nRemember that for any number \\(n\\), the mathematical operation of taking the inverse simply means dividing one by that number. So, the inverse of \\(n\\) is \\(\\frac{1}{n}\\). This means that if \\(d_{ij}\\) is the geodesic distance between nodes i and j in graph \\(G\\), then the closeness between two nodes is \\(\\frac{1}{d_{ij}}\\).\nThe information on the pairwise geodesic distances between every pair of nodes in a given graph is captured in the geodesic distance matrix. For instance, take the graph shown in Figure 20.2. The distance matrix for this graph is shown in Table 20.3.\n\n\n\n\n\n\n\n\nE\nA\nM\nL\nB\nJ\nG\nN\nF\nH\nD\nC\nK\nI\n\n\n\n\nE\n0\n1\n2\n1\n2\n1\n2\n1\n2\n1\n3\n2\n2\n2\n\n\nA\n1\n0\n3\n1\n3\n2\n2\n2\n3\n2\n2\n1\n1\n2\n\n\nM\n2\n3\n0\n3\n1\n2\n2\n1\n2\n2\n1\n3\n2\n4\n\n\nL\n1\n1\n3\n0\n3\n2\n1\n2\n2\n2\n2\n1\n2\n1\n\n\nB\n2\n3\n1\n3\n0\n1\n2\n2\n1\n2\n2\n2\n3\n4\n\n\nJ\n1\n2\n2\n2\n1\n0\n2\n1\n1\n2\n2\n1\n3\n3\n\n\nG\n2\n2\n2\n1\n2\n2\n0\n3\n1\n2\n1\n2\n2\n2\n\n\nN\n1\n2\n1\n2\n2\n1\n3\n0\n2\n1\n2\n2\n2\n3\n\n\nF\n2\n3\n2\n2\n1\n1\n1\n2\n0\n1\n1\n2\n2\n3\n\n\nH\n1\n2\n2\n2\n2\n2\n2\n1\n1\n0\n2\n3\n1\n3\n\n\nD\n3\n2\n1\n2\n2\n2\n1\n2\n1\n2\n0\n3\n1\n3\n\n\nC\n2\n1\n3\n1\n2\n1\n2\n2\n2\n3\n3\n0\n2\n2\n\n\nK\n2\n1\n2\n2\n3\n3\n2\n2\n2\n1\n1\n2\n0\n3\n\n\nI\n2\n2\n4\n1\n4\n3\n2\n3\n3\n3\n3\n2\n3\n0\n\n\n\nTable 20.3: Geodesic distance matrix for an undirected graph.\n\n\nAs shown in Table 20.3, a node like I, who seems to be at the outskirts of the network, also shows up as having the largest geodesic distances from other nodes in the graph. Other nodes, like E, G, and L seem to be “closer” to others, in terms of having to traverse smaller geodesic distances to reach them.\nThat means that we can use the distance table to come up with a measure of centrality called closeness centrality for each node. We can do that by adding up the entries corresponding to each row in the distance matrix (\\(\\sum_j d_{ij}\\)), to get a summary the total pairwise distances separating the node corresponding to row i in the matrix from the other nodes listed in each column j.\nNote that because closeness is better than “farness,” we would want the node with highest closeness centrality to be the one with the smallest sum of pairwise distances. This can be calculated using the following equation:\n\\[\n  C_i^{CLOS} = \\frac{1}{\\sum_jd_{ij}}\n\\tag{20.7}\\]\nIn Equation 20.7, the denominator is the sum across each column j, for each row i in Table 20.3 which corresponds to the distance between node i and each of the other nodes in the graph j (skipping the diagonal cell when \\(i=j\\), because the geodesic distance of node to itself is always zero!).\nAs noted, we take the mathematical inverse of this quantity, dividing one by the sum of the distances, so that way, the smallest number comes out on top and the bigger number comes out on the bottom (since, as we said, we want to measure closeness not “farness.”)\nLet’s see how this work for the graph in Figure 20.2. First, we get the row sums of geodesic distances from Table 20.3. These are shown in the first column of Table 20.4, under the heading “Sum of Distances.” This seems to work; node \\(E\\) has the smallest number here (\\(\\sum_j d_{Ej} = 22\\)) suggesting it can reach the most nodes via the shortest paths. Node \\(I\\) has the largest number (\\(\\sum_j d_{Ij} = 35\\)) indicating it is the most isolated from the other nodes.\n\n\n\n\n\n \n  \n      \n    Sum of Distances (d) \n    Inverse (1/d) \n    Normalized (N-1/d) \n  \n \n\n  \n    E \n    22 \n    0.045 \n    0.59 \n  \n  \n    A \n    25 \n    0.040 \n    0.52 \n  \n  \n    M \n    28 \n    0.036 \n    0.46 \n  \n  \n    L \n    23 \n    0.043 \n    0.57 \n  \n  \n    B \n    28 \n    0.036 \n    0.46 \n  \n  \n    J \n    23 \n    0.043 \n    0.57 \n  \n  \n    G \n    24 \n    0.042 \n    0.54 \n  \n  \n    N \n    24 \n    0.042 \n    0.54 \n  \n  \n    F \n    23 \n    0.043 \n    0.57 \n  \n  \n    H \n    24 \n    0.042 \n    0.54 \n  \n  \n    D \n    25 \n    0.040 \n    0.52 \n  \n  \n    C \n    26 \n    0.038 \n    0.50 \n  \n  \n    K \n    26 \n    0.038 \n    0.50 \n  \n  \n    I \n    35 \n    0.029 \n    0.37 \n  \n\n\n\nTable 20.4:  Sum of geodesic distances for each node in an undirected graph and its inverse. \n\n\nBut we want closeness, not farness, so the second column of Table 20.4 shows what happens when we divide one by the number in the second column. Now, node \\(E\\) has the largest score \\(CC^{CLOS}_E = 0.045\\) which is what we want.\nHowever, because we are dividing one by a relatively large number, we end up with a bunch of small decimal numbers as centrality scores, and like it happened with degree, this number is sensitive to how big the network is (the larger the network, the more likely there is to be really long short paths). So Freeman (1979) proposes a normalized version of closeness that takes into account network size. It is a variation of Equation 20.7:\n\\[\n  C_i^{CLOS} = \\frac{N-1}{\\sum_jd_{ij}}\n\\tag{20.8}\\]\nEquation 20.8 is the same as Equation 20.7, except that instead of dividing one by the sum of distances, we divide \\(N-1\\) by the sum of distances, where \\(N\\) is the order of the graph (the number of nodes). In this case, \\(N=14\\).\nNormalizing the sum of distances shown in the second column of Table 20.4 according to Equation 20.8, gives us the centrality scores shown in the fourth column of the table, under the heading “Normalized.” These scores range from zero to one, with one being the maximum possible closeness centrality score for that graph.\nThe normalized closeness centrality scores listed in the fourth column of Table 20.4 agree with our informal impressions. Node I comes out at the bottom (\\(CC_I^{CLOS} = 0.37\\)), showing it to be the one with the least closeness centrality, given the relatively large geodesic distances separating it from the other nodes in the graph. Node E (marked red in Figure 20.2) comes out on top (\\(CC_E^{CLOS} = 0.59\\)), given its relative geodesic proximity to other nodes in the graph.\nAs we will see later, having closeness centrality information for nodes in a graph can be useful. For instance, if Figure 20.2 was a social network, and we wanted to spread an innovation or a new product among the actors in the fastest amount of time, we would want to give it to node E first. Note however that if something bad (like a disease) was spreading across the network, then it would also be very bad if actor E got it first!4"
  },
  {
    "objectID": "lesson-sna-centrality.html#houston-we-have-a-problem",
    "href": "lesson-sna-centrality.html#houston-we-have-a-problem",
    "title": "20  Centrality",
    "section": "20.6 Houston, We Have a Problem",
    "text": "20.6 Houston, We Have a Problem\nSo far, so good. Closeness seems to be a great measure of node importance, giving us a sense of who can reach most others in a network in the most efficient way. However, what would happen if we tried to compute closeness centrality for a disconnected graph like the one shown in Figure Figure 11.1 (b)? Well, the shortest paths distance matrix for that graph looks like the one in Table 20.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\nInf\nInf\nInf\nInf\n\n\nB\n1\n0\n1\n1\n2\nInf\nInf\nInf\nInf\n\n\nC\n1\n1\n0\n1\n1\nInf\nInf\nInf\nInf\n\n\nD\n1\n1\n1\n0\n1\nInf\nInf\nInf\nInf\n\n\nE\n1\n2\n1\n1\n0\nInf\nInf\nInf\nInf\n\n\nF\nInf\nInf\nInf\nInf\nInf\n0\n1\n1\n1\n\n\nG\nInf\nInf\nInf\nInf\nInf\n1\n0\n1\n1\n\n\nH\nInf\nInf\nInf\nInf\nInf\n1\n1\n0\n1\n\n\nI\nInf\nInf\nInf\nInf\nInf\n1\n1\n1\n0\n\n\n\nTable 20.5: Geodesic distance matrix for an undirected, disconnected graph.\n\n\nNote that in Table 20.5, pairs of nodes that cannot reach one another in the disconnected graph, get a geodesic distance of “Inf” (infinity) in the respective cell of the geodesic distance matrix. This is a problem because when we compute the row sums of the geodesic distance matrix to try to calculate centrality according to Equation 20.7, we get the “numbers” shown in Table 20.6.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n\nTable 20.6: Row sums of a geodesic distance matrix from a disconnected graph.\n\n\nSo that’s a bummer since all the “numbers” in Table 20.6, are just infinity. Not to get too philosophical, but the problem is that when you add any number to “infinity,” the answer is, well, infinity.5 This means that closeness centrality is only defined for connected graphs. When it comes to disconnected graphs, we are out of luck.\nThankfully, there is a solution develoed by Beauchamp (1965). It consists of a modification of Equation 20.7 called harmonic closeness centrality. The formula goes as follows:\n\\[\n  C_i^{HARM} = \\frac{1}{N-1}\\sum_j\\frac{1}{d_{ij}}\n\\tag{20.9}\\]\nNow, this might seem like we just re-arranged the stuff in Equation 20.8, and indeed that’s what we did! But the re-arrangement matters a lot, because it changes the order in which we do the various arithmetic operations (Boldi and Vigna 2014).\nSo, in English, while Equation 20.8 says “first sum the geodesic distances for each node (to get the denominator), and then divide \\(N-1\\) by this sum,” Equation 20.9 says “first divide one by the geodesic distance, and then sum the result of all these divisions, and then multiply this sum by one over \\(N-1\\).\nOnce again, the philosophy of mathematical infinity kicks in here, since the main difference is that one divided by infinity is actually a real number: zero.6\nSo let’s check by taking every entry in Table 20.5 and dividing one by the number in each cell (except for the diagonals, which we don’t care about). The results are shown in Table 20.7.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1.0\n1\n1\n1.0\n0\n0\n0\n0\n\n\nB\n1\n0.0\n1\n1\n0.5\n0\n0\n0\n0\n\n\nC\n1\n1.0\n0\n1\n1.0\n0\n0\n0\n0\n\n\nD\n1\n1.0\n1\n0\n1.0\n0\n0\n0\n0\n\n\nE\n1\n0.5\n1\n1\n0.0\n0\n0\n0\n0\n\n\nF\n0\n0.0\n0\n0\n0.0\n0\n1\n1\n1\n\n\nG\n0\n0.0\n0\n0\n0.0\n1\n0\n1\n1\n\n\nH\n0\n0.0\n0\n0\n0.0\n1\n1\n0\n1\n\n\nI\n0\n0.0\n0\n0\n0.0\n1\n1\n1\n0\n\n\n\nTable 20.7: Reciprocal of the geodesic distance matrix for an undirected, disconnected graph.\n\n\nBeautiful! Now, instead of weird “Inf”s we have zeroes, which is great because we can add stuff to zero and get a real number back. We can then apply Equation 20.9 to the numbers in Table 20.7 (e.g., computing the sum of each row and then multiplying that by \\(\\frac{1}{N-1}\\)) to get the harmonic closeness centrality for each node in Figure 11.1 (b). These are shown in Table 20.8.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.5\n0.44\n0.5\n0.5\n0.44\n0.38\n0.38\n0.38\n0.38\n\n\n\nTable 20.8: Harmonic Closeness Centrality scores for nodes in a disconnected, undirected graph.\n\n\nGreat! Now we have a measure of closeness centrality we can apply to all kinds of graphs, whether they are connected or disconnected."
  },
  {
    "objectID": "lesson-sna-centrality.html#betweenness-centrality",
    "href": "lesson-sna-centrality.html#betweenness-centrality",
    "title": "20  Centrality",
    "section": "20.7 Betweenness Centrality",
    "text": "20.7 Betweenness Centrality\nRecall that in our discussion of shortest paths between pair of nodes in the lesson on indirect connections, we noted the importance of the inner nodes that intervene or mediate between a node that wants to reach another one. Nodes that stand in these brokerage or gatekeeper slots in the network, occupy an important position (Marsden 1983), and this is different from having a lot of contacts (like degree centrality), or being able to reach lots of other nodes by traversing relatively small distances (like closeness centrality). Instead, this is about being in-between the indirect communications of other nodes in graph. We can compute a centrality metric for each node called betweenness centrality that captures this idea Freeman (1980).\n\n\n\n\n\nFigure 20.3: An undirected graph showing the node with the maximum betweenness centrality (in red)\n\n\n\n\nFor instance, let’s say you were actor K in the network shown in Figure 20.2, and you wanted to know who is the person that you depend on the most to communicate with actor J. Here dependence means that you are forced to “go through them” if I wanted to reach N via a shortest path. One way K could figure this out is by listing every shortest path having them as the origin node and having N as the destination node. After you have this list, you can see which of other other nodes shows up as an inner node—an intermediary or gatekeeper—in those paths the most times.\nThis shortest path list would look like this:\n\n\\(\\{KH, HF, FJ\\}\\)\n\\(\\{KD, DF, FJ\\}\\)\n\\(\\{KH, HN, NJ\\}\\)\n\\(\\{KA, AC, CJ\\}\\)\n\\(\\{KA, AE, EJ\\}\\)\n\\(\\{KH, HE, EJ\\}\\)\n\nThere are six shortest paths of length three indirectly connecting actors K and J in Figure 20.2), with nodes \\(\\{A, C, D, E, F, H, N\\}\\) showing up as an inner node in at least one of those paths. To see which other actor in the network is the most frequent intermediary between J and K, we can create a list with the number of times each of these nodes shows up as an intermediary in this shortest path list. This would look like this:\n\n\n\n\n\n\n\nNode\nFreq.\nProp.\n\n\n\n\nA\n2\n0.33\n\n\nC\n1\n0.17\n\n\nD\n1\n0.17\n\n\nE\n2\n0.33\n\n\nF\n2\n0.33\n\n\nH\n3\n0.50\n\n\nN\n1\n0.17\n\n\n\nTable 20.9: Intermediaries between nodes J and K\n\n\nSo it looks like, looking at the second column of Table 20.9, that H is the other actor that J depends on the most to reach K. A better way to quantify this, is to actually look at the proportion of paths linking J and K that a particular other node (like H) shows up in. Let’s call this \\(p_{K(H)J}\\) which can be read as “the proportion of paths between K and J featuring H as an inner node.” This is shown in the third column of Table 20.9 We can write this in equation form like this:\n\\[\n  p_{K(H)J} = \\frac{g_{K(H)J}}{g_{KJ}} = \\frac{3}{6} = 0.5\n\\tag{20.10}\\]\nIn Equation 20.10, \\(g_{K(H)J}\\) is the number of shortest paths linking K and J featuring H as an inner node, and \\(g_{KJ}\\) is the total number of paths linking K and J. Freeman (1980) calls this measure the pair-dependency of actor K on actor H to reach a given node J. In this case, \\(g_{K(H)J} = 3\\) and \\(g_{KJ} = 6\\), which means that actor K depends on actor H for fifty percent of their shortest path access to J. Making H the actor in the network J depends on the most to be able to reach J.\nGeneralizing this approach, we can do the same for each triplet of actors i, j, and k in the network. This is the basis for calculating betweenness centrality. That is, we can count the number of times k stands on the shortest path between two other actors i and j. We can all this number \\(g_{i(k)j}\\). We can then divide it by the total number of shortest paths linking actors i and j in the network, which we refer by \\(g_{ij}\\). Remember that two actors can be indirectly linked by multiple shortest paths of the same length, and that we can figure out how many short paths links pairs of actors in the network using the shortest paths matrix.\nThis ratio, written \\(\\frac{g_{i(k)j}}{g_{ij}}\\) then gives us the proportion of shortest paths in the network that have i and j as the end nodes and that feature k as an intermediary inner node. This can range from zero (no shortest paths between i and j feature node k as an intermediary) to one (all the shortest paths between i and j feature node k as an intermediary).\nWe can then use the following equation to compute the average of this proportion for each node k across each pair of actors in the network i and j:\n\\[\n  C_k^{BET} = \\sum_i \\sum_j \\frac{g_{ikj}}{g_{ij}}\n\\tag{20.11}\\]\nComputing this quantity for the graph shown in Figure 20.3, yields the betweenness centrality scores shown in Table 20.10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nJ\nK\nL\nM\nN\nI\n\n\n\n\n5\n1.5\n3\n6.8\n11.4\n9.3\n6.3\n4.8\n10.8\n3.7\n16.4\n2.8\n5.3\n0\n\n\n\nTable 20.10: Betweenness centrality scores.\n\n\nThe numbers in the Table can be readily interpreted as percentages. Thus, the fact that node J has a a betweenness centrality score of 10.8 tells us that they stand in about 11% of the shortest paths between pairs of nodes in the graph. Interestingly, as shown in Figure 20.3, the node that ends up with the highest betweenness score is L (\\(C_L^{BET} = 16.4\\)), mostly due to the fact that node I, who has the lowest possible betweenness score of zero, depends on this node for access to every other actor in the network.\nNote also that two different nodes end up being ranked first on closeness and betweenness centrality in the same network (compare the red nodes in Figure 20.2 and Figure 20.3). This tells us that closeness and betweenness are analytically distinct measures of node position. One (closeness) gets at reachability, and the other (betweenness) gets at intermediation potential."
  },
  {
    "objectID": "lesson-sna-centrality.html#the-big-three-centralities-in-the-star-graph",
    "href": "lesson-sna-centrality.html#the-big-three-centralities-in-the-star-graph",
    "title": "20  Centrality",
    "section": "20.8 The Big Three Centralities in the Star Graph",
    "text": "20.8 The Big Three Centralities in the Star Graph\nDegree, Closeness, and Betweenness centralities have an interesting property that provides a conceptual connection between them (Freeman 1979). Consider the star graph shown in Figure 20.1 with central node A. The degree, closeness, and betweenness centralities of the different nodes are shown in Table 20.11).\nOf course, by definition, we know beforehand that the central node in a star graph has to have the highest degree, since the degree of peripheral nodes is fixed to one and the degree of the central node is always \\(n-1\\), where \\(n\\) is the graph order.\nHowever, note also that the central node has to have the highest closeness, since it is directed by a path of length one (and edge) to every peripheral node, but each peripheral node can only reach other peripheral nodes in the graph by a path of length two. They are farther away from other nodes than the central node.\nFinally, note that the central node in the star will also always have the highest betweenness because each of the paths of length two connecting every pair of peripheral nodes to one another has to include the central node. So it serves as the intermediary between any communication between peripheral nodes.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nDegree\n6.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nCloseness\n8.2\n4.5\n4.5\n4.5\n4.5\n4.5\n4.5\n\n\nBetwenness\n15.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\nTable 20.11: Centralities in a star graph of order 7.\n\n\nThe mathematical sociologist Linton Freeman (1979) thus thinks that the “big three” centrality measures are the big three precisely because they are maximized for the central node in a star graph."
  },
  {
    "objectID": "lesson-sna-centrality.html#references",
    "href": "lesson-sna-centrality.html#references",
    "title": "20  Centrality",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBavelas, Alex. 1950. “Communication Patterns in Task-Oriented Groups.” The Journal of the Acoustical Society of America 22 (6): 725–30.\n\n\nBeauchamp, Murray A. 1965. “An Improved Index of Centrality.” Behavioral Science 10 (2): 161–63.\n\n\nBoldi, Paolo, and Sebastiano Vigna. 2014. “Axioms for Centrality.” Internet Mathematics 10 (3-4): 222–62.\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nBorgatti, Stephen P, and Martin G Everett. 2006. “A Graph-Theoretic Perspective on Centrality.” Social Networks 28 (4): 466–84.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41.\n\n\n———. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39.\n\n\n———. 1980. “The Gatekeeper, Pair-Dependency and Structural Centrality.” Quality and Quantity 14 (4): 585–92.\n\n\nMarsden, Peter V. 1983. “Restricted Access in Networks and Models of Power.” American Journal of Sociology 88 (4): 686–717.\n\n\nNieminen, Juhani. 1974. “On the Centrality in a Graph.” Scandinavian Journal of Psychology 15 (1): 332–36.\n\n\nSabidussi, Gert. 1966. “The Centrality Index of a Graph.” Psychometrika 31 (4): 581–603."
  },
  {
    "objectID": "lesson-sna-status.html#status-as-indegree-centrality",
    "href": "lesson-sna-status.html#status-as-indegree-centrality",
    "title": "23  Status",
    "section": "23.1 Status as Indegree Centrality",
    "text": "23.1 Status as Indegree Centrality\nConsider a network which could be composed of asymmetric ties indicating some kind of positive regard or esteem that node i has for node j, represented by the directed graph in Figure 23.1, with result adjacency matrix shown in Table 23.1.\nThe directed edges could be “thinks the other person is great,’’ or”respects the other person” or “would take advice from that person.” Note that all these relations are asymmetric you can think that person A is great, but that does not mean they think the same thing about you.\n\n\n\n\n\nFigure 23.1: A directed graph.\n\n\n\n\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 23.1:  Adjacency matrix corresponding to a directed graph. \n\n\nAn easy approach is to measure the status of each node is by counting the number of direct nominations they get from others. This would be trying to measure the status of each node by using their indegree centrality. Recall, from Chapter 20, that this is given by computing the column sums of the adjacency matrix (summing down each column across rows). In matrix form:\n\\[\nC^{IN}_i = \\sum_j a_{ij}\n\\tag{23.1}\\]\nThe results of the indegree calculation for all the nodes in the graph shown in Figure 23.1—with corresponding adjacency matrix shown in Table 23.1—are shown in Table 23.2 (a). According to the table, nodes \\(F\\) and \\(I\\) are the highest status nodes in Figure 23.1 because they each receive five and four nominations respectively They are followed by nodes \\(\\{A, B, E, J\\}\\) who receive three nominations each. Node \\(C\\) is the lowest status, as no one thinks they are important.\nHowever, the problem with using the number of incoming nominations as a measure of status is that the indegree centrality only measures the number of ties that are incoming to each node, but it does not differentiate between who sends each tie. Every nomination counts as the same. But as we noted, the whole point of the idea of status is that you gain status when you receive ties from high-status others, and their status is established by their receiving ties from high status others, and so forth.\nThis brings up another problem with the indegree centrality, which is that it is a purely local measure, counting only paths of length one (direct connections). But it is possible that you get status not only from the people you are connected to, but the people they are connected to (paths of length two) and the people those people are connected to (paths of length three) and so forth. A good measure of status would incorporate information from indirect ties (see Chapter 9).\nSo indegree centrality won’t do as a measure of status in social networks, if we aim to capture the full idea behind the concept."
  },
  {
    "objectID": "lesson-sna-status.html#using-exogeneous-status-information",
    "href": "lesson-sna-status.html#using-exogeneous-status-information",
    "title": "23  Status",
    "section": "23.2 Using Exogeneous Status Information",
    "text": "23.2 Using Exogeneous Status Information\nLet us deal with the problem of treating every incoming tie the same first. One possibility is that we check some measure of status that comes from outside the network (the fancy word for this is exogenous). This could be for instance, the position of each node in the organizational chart, with ten indicating a top position and zero indicating an entry-level position. We could record this information using a \\(1 \\times 12\\) row vector where the exogenous status of each node i is given by each entry \\(\\mathbf{b}_i\\). Such an exogenous status score vector is shown in Table 23.2. In the table, larger numbers indicate higher exogenous status.\n\n\n\n\n\nTable 23.2: Example of estimating status using exogeneous scores for nodes.\n\n\n\n\n(a) Indegree Centrality \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    3 \n    3 \n    0 \n    1 \n    3 \n    5 \n    1 \n    2 \n    4 \n    3 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Exogenous Status Scores \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    2 \n    5 \n    4 \n    6 \n    8 \n    8 \n    2 \n    4 \n    9 \n    4 \n    9 \n    6 \n  \n\n\n\n\n\n\n\n\n(c) Status Scores Based on Exogeneous Information \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    17 \n    14 \n    0 \n    8 \n    22 \n    24 \n    5 \n    10 \n    17 \n    14 \n    6 \n    6 \n  \n\n\n\n\n\n\n\n\n(d) Status Scores Based on Endogeous Information (In-degree) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    6 \n    11 \n    0 \n    5 \n    12 \n    7 \n    3 \n    1 \n    5 \n    8 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(e) Status Scores Based on All Indirect Paths (Katz) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    13.5 \n    27.3 \n    0 \n    8.7 \n    29.3 \n    19.1 \n    13 \n    2.9 \n    15.3 \n    21 \n    2.6 \n    4.3 \n  \n\n\n\n\n\n\n\n\n(f) Status Scores Based on All Indirect Paths and Exogeneous Status Information (Hubbell) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    80.9 \n    154.7 \n    4 \n    60.6 \n    174 \n    120.4 \n    72.1 \n    21 \n    91.1 \n    129 \n    24.2 \n    33.4 \n  \n\n\n\n\n\n\nNow the status score for each person \\(\\mathbf{s}\\) can be determined by taking each of their incoming—recorded in the columns of the adjacency matrix in Table 23.1—nominations and weighting them by the exogenous status score of each of the other people, so that nominations from higher status nodes (like node \\(K\\) in Table 23.2 (b)) count for more than those coming from lower status nodes (like node \\(A\\) in Table 23.2 (b)). To calculate the status of each node we add up the status of each of the nodes that point to it.\nHow do we do this? Recall from Chapter 15, that it is always possible to multiply a row vector times a square matrix as long as the row vector is of the same length as the matrix’s row and column dimensions. The result is always another row vector of the same length as the original.\nAccordingly, we can get each person’s weighted status score by taking the exogenous status scores vector \\(\\mathbf{b}\\) and multiplying by the network’s adjacency matrix \\(\\mathbf{A}\\). In this case, \\(\\mathbf{b}\\) is \\(1 \\times 12\\) and \\(\\mathbf{A}\\) \\(\\mathbf{b}\\) is \\(12 \\times 12\\). Because the number of rows of the matrix are the same as the lengtth (number of columns) of the vector, the multiplication is allowed (the terms are conformable) and the result will be a \\(1 \\times 12\\) row vector of status scores that we will call \\(\\mathbf{s}^{ex}\\):\n\\[\n  \\mathbf{s}^{ex} = \\mathbf{b}\\mathbf{A}\n\\tag{23.2}\\]\nWhen we do that, we end up with the status scores shown in Table 23.2 (c).\nAs we can see, the status order is a bit different once we take into account the exogenous status of the other people who nominate each node. Yes, node \\(F\\) is still the highest ranked node, and node \\(C\\) is the lowest ranked. However, node \\(I\\) is no longer the second highest status node, that honor now goes to node \\(E\\). The reason is that while \\(I\\) has a larger indegree than \\(E\\), node \\(I\\)’s in-neighbors, as shown in Figure Figure 23.1 and Table 23.1, \\(N_{in}(I) = \\{A, C, G, K\\}\\) are relatively low status (except for \\(K\\)). \\(E\\)’s in-neighbors, by way of contrast, \\(N_{in}(E) = \\{B, F, I\\}\\) are all high to mid-status.\nNote that for each node, we can reconstruct their status score simply by adding up the status scores of their in-neighbors in Table 23.1. So for instance, node \\(A\\) gets at status score of 17. Where does this number come from? Well if we go to Table 23.1, we can see that \\(A\\)’s in-neighbors are \\(\\{H, J, K\\}\\) (looking down the column corresponding to \\(A\\)), and we can see that the corresponding status scores of each of these nodes in Table 23.2 (b) are 4, 4, and 9 (respectively), which means that \\(4 + 4 + 9 = 17\\), the status score for node \\(A\\)!"
  },
  {
    "objectID": "lesson-sna-status.html#using-endogeneous-network-information",
    "href": "lesson-sna-status.html#using-endogeneous-network-information",
    "title": "23  Status",
    "section": "23.3 Using Endogeneous Network Information",
    "text": "23.3 Using Endogeneous Network Information\nIt turns out, that in many cases, we don’t have exogenous status information on each node in the network to rely on. In that case, we must rely on endogenous network information to determine the status of each of the other nodes.\nOne approach is just to use original indegree centrality scores shown in Table 23.2 (a) as the status of each other the nodes. We can then say that a node is high status if it is pointed to by other nodes who are also pointed to by many other nodes. Conversely, a node is low status if it is pointed to by other nodes that are not pointed to by many other nodes.\n\\[\n  s^{en} = C^{IN}\\mathbf{A}\n\\tag{23.3}\\]\nWhere \\(C^{IN}\\) is the \\(1 \\times 12\\) row vector of indegree centralities shown in Table 23.2 (a) and \\(\\mathbf{A}\\) is the the network’s adjacency matrix shown in Table 23.1. The results are shown in Table 23.2 (d). As we can see, considering only endogenous network information gives us a completely different picture of the status order than using exogenous information. Now \\(E\\) is definitely the highest status node, and \\(F\\) which was the highest status node based on exogenous considerations drops to fourth place, behind \\(B\\) and \\(J\\).\nLooking at Figure 23.1, we can see why this happened. Take the set of \\(F\\)’s in-neighbors \\(\\{C, D, H, J, L\\}\\). It is easy to see from Table 23.2 (a), that most of these nodes also have low indegree centrality (except for \\(J\\)). So even though \\(F\\) has five nodes pointing toward them (\\(C^{IN}_F = 5\\)), all of them are not very high-status people. By comparison \\(E\\) only has three in-neighbors \\(\\{B, F, I\\}\\), and all three are towards the top in terms of in-degree centrality. \\(E\\) has higher status than \\(F\\) according to \\(s^{en}\\) because the people that choose \\(E\\) are also chosen by many others, which is exactly what we want in a status measure.\nWhile \\(s^{en}\\) seems like a good measure of status, it does have one big drawback. It still only counts direct connections. As we noted earlier, it is possible that you get status not just from the nodes that point directly toward you, but from the nodes that point to those other people even if they don’t point toward you (e.g., two step connections), and perhaps from the nodes that point to those two-step alters, and the ones that point to those three-steps away, and so forth. A good status measure should be able to take into account the status of your indirect connections in computing your own status score. How do we do this?"
  },
  {
    "objectID": "lesson-sna-status.html#a-mathy-interlude",
    "href": "lesson-sna-status.html#a-mathy-interlude",
    "title": "23  Status",
    "section": "23.4 A Mathy Interlude",
    "text": "23.4 A Mathy Interlude\nBefore we get to that, we will explore some recreational math. dConsider any number \\(x\\), where \\(x < |1|\\) (remember that \\(|a|\\) means “the absolute value of \\(a\\)), and thus \\(-1 > x < 1\\) (this reads”\\(x\\) is between -1 and +1”). Thus, \\(x\\) can be 0.43, or -0.62, or whatever in that interval. Recall that when we take a number in this interval and we raise it to a power, we end up with a smaller number than we begin with. The bigger the power, the smaller the result. For instance, take \\(x = 0.75\\). For instance:\n\\[\n  x^2 = 0.75^2 = 0.562\n\\] \\[\n  x^5 = 0.75^5 = 0.237\n\\] \\[\n  x^{10} = 0.75^{10} = 0.056\n\\] Because mathematicians are strange people, they like to say things like, “since the result gets closer to zero the bigger the power, then that means that when I raise the number to an infinite power, then the result should approach zero.” In equation terms:\n\\[\nx^{\\infty} = 0.75^{\\infty} \\approx 0\n\\] Since raising a number between -1 and 1 to a big power gets you closer to zero the bigger the power, mathematicians then go on to wonder whether adding up the powers, gets to the point where the sum does not grow anymore. For instance, what is the end point of:\n\\[\n  1 + x + x^2 + x^3 + x^4 + x^5 \\ldots + x^{\\infty}\n\\tag{23.4}\\]\nThe idea is that as we move to the right and add a number between -1 and 1 raised to a bigger and bigger power, we add a smaller and smaller number, such that as we approach infinity, we end up adding such an infinitesimally small number that it might as well be zero. For instance, table Table 23.3 shows the result of raising \\(x\\) to the powers between 2 and 20 for \\(x = 0.75\\).\n\n\n\n\n\n \n  \n    Power \n    Result \n  \n \n\n  \n    2 \n    0.562 \n  \n  \n    3 \n    0.422 \n  \n  \n    4 \n    0.316 \n  \n  \n    5 \n    0.237 \n  \n  \n    6 \n    0.178 \n  \n  \n    7 \n    0.133 \n  \n  \n    8 \n    0.100 \n  \n  \n    9 \n    0.075 \n  \n  \n    10 \n    0.056 \n  \n  \n    11 \n    0.042 \n  \n  \n    12 \n    0.032 \n  \n  \n    13 \n    0.024 \n  \n  \n    14 \n    0.018 \n  \n  \n    15 \n    0.013 \n  \n  \n    16 \n    0.010 \n  \n  \n    17 \n    0.008 \n  \n  \n    18 \n    0.006 \n  \n  \n    19 \n    0.004 \n  \n  \n    20 \n    0.003 \n  \n\n\n\nTable 23.3:  Incresing powers of the number 0.75. \n\n\nNow let us sum \\(1 + 0.75\\) and add the result to the sum of all the numbers in the third column of Table 23.3, to get an approximation to the sum shown in Equation 23.4. The result is 3.99.\nIn turns out, by some bit of mathematical magic, that this number is pretty close to: \\[\n  (1-0.75)^{-1} = \\frac{1}{1 - 0.75} = 4\n\\]\nAs we noted, as the power that we raise the number to approaches \\(\\infty\\), we will be adding a number that is closer to zero, so the result of the infinity sum when \\(x = 0.75\\) will converge towards:\n\\[  \n  1 + x + x^2 + x^3 + x^4 + x^5 \\ldots + x^{\\infty} =\n\\] \\[\n  1 + 0.75 + 0.75^2 + 0.75^3 + 0.75^4 + 0.75^5 \\ldots + 0.75^{\\infty} \\approx 4\n\\] In general terms, for any number \\(x\\) the sum of the following infinite series converges to:\n\\[\n  1 + x + x^2 + x^3 + x^4 + x^5 +...x^\\infty =\n\\] \\[\n\\frac{1}{1 - x} = (1-x)^{-1}  \n\\tag{23.5}\\]\nWhy is this bit of math important? We will see next!"
  },
  {
    "objectID": "lesson-sna-status.html#katz-status-score",
    "href": "lesson-sna-status.html#katz-status-score",
    "title": "21  Status",
    "section": "21.5 Katz Status Score",
    "text": "21.5 Katz Status Score\nIn the mid-twentieth century, the great statistician Leo Katz set out to develop a measure of status in social networks that took into account indirect connections. He first observed that the new matrix that results from taking the original adjacency matrix and raising it to a power (using matrix multiplication) has a clear interpretation. For instance, if we raise the adjacency matrix to the third power (\\(\\mathbf{A}^3\\)) the resulting matrix will contain, as cell entries, all the paths of length \\(l = 3\\) that have node \\(i\\) as the starting node and node \\(j\\) as the destination node, the same goes for any number \\(\\mathbf{A}^k\\).\nKatz saw this as a way to incorporate indirect connections to construct a measure of status using only endogenous information. The idea would be to say that your total status is the sum of number of other people who choose you (or think you are great, or a great source or advice or whatever). However, among the people that choose you the ones that are chosen by many others should count for more. Those are people who are two-steps away from you. But the same should apply to the people who choose those others (people three-steps away from you). Overall, you should get more status from people who choose the people who choose the people, who choose the people…who choose you and you should get more status from the more people who are most likely to be chosen by those others, across any number of steps.\nTo accomplish this, we need to construct a new matrix \\(A^*\\) that incorporates all this information about people’s one step, two-step, three-step, connections to others, then sum rows of that matrix. The resulting vector (\\(s^{Katz}_i\\)) would contain the desired score for each node \\(i\\).\nOne way to proceed would be:\n\\[\nI + A + A^2 + A^3 + A^4 + \\ldots A^{\\infty} = \\mathbf{1} \\sum_{k = 0}^{k = \\infty} A^k\n\\tag{21.6}\\]\nWith the understanding that \\(A^0 = I\\) (raising a matrix to the zero power returns the identity matrix), and \\(\\mathbf{1}\\) is a row vector full of ones \\(\\mathbf{1} = \\{1, 1, 1, ... 1\\}\\) of the same length as the number of rows in \\(A\\), which when multiplied by \\(\\sum A^k\\) would return the column sums of that matrix (see Section 16.6.3) which would be our vector of Katz centrality scores for each node.\nThere are a couple of problems with Equation 21.6. First this sum keeps getting bigger and bigger and it does not have a natural end point (keeps going forever). This is because it is counting the direct connections (\\(A\\)) as much as the very indirect connections, like \\(A^5\\), or the number of indirect links connecting you to others five steps away.\nWhat we want is a way to count the first-step links the most, and then discount the longer-step links, with the discount getting larger the longer the chain. So that three-steps links count for less than two-steps links but count for more than four step links to others and so forth.\nKatz’s great idea is to multiply the original adjacency matrix and its powers by a number \\(\\alpha\\) that was larger than zero, but less than one. This leads to:\n\\[\nI + \\alpha A + \\alpha^2 A^2 + \\alpha^3 A^3 + \\alpha^4 A^4 + \\ldots \\alpha^\\infty A^{\\infty} = \\mathbf{1} \\sum_{k = 0}^{k = \\infty} \\alpha^k A^k\n\\tag{21.7}\\]\nWith the understanding that \\(\\alpha^0 A^0 = 1 \\times I = I\\), the first term in the sequence is once again the identity matrix. Now the difference between Equation 21.6 and Equation 21.7, is that as we saw before (see Equation 21.5), while the sum in Equation 21.6 keeps getting bigger and bigger (the technical term is “diverges”), the one in Equation 21.7, will stop growing, because raising a number less than one and more than zero (like \\(\\alpha\\)) to a bigger and bigger power will result in a tinier and tinier number, until we get close to zero. The sum will converge rather than diverge.\nMoreover, Katz knew his math, and noted that there is a version of {Equation 21.5} that applies to matrices. This is:\n\\[\n\\mathbf{1} \\sum_{k = 0}^{k = \\infty} \\alpha^k A^k = \\mathbf{1} (I - \\alpha A)^{-1}\n\\tag{21.8}\\]\nWhere \\(I\\) refers to the identity matrix (see Section 16.7). What Katz (1953) proposed is that we can turn the infinite sum part of Equation 21.8 (\\(I + \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A5 + \\ldots \\alpha A^{\\infty})\\) into just \\((I- \\alpha A)^{-1}\\) following the principle outlined earlier in Equation 21.5. Just like the endless sum of squares of a number \\(x\\) between \\(-1\\) and and \\(1\\) just turns into just \\((1-x)^-1\\), the endless sums of a matrix containing numbers between \\(-1\\) and \\(1\\) as its entries \\(\\alpha A\\) turns into \\((I-\\alpha A)^{-1}\\), with \\(I\\) playing the role of \\(1\\) and raising \\(I-\\alpha A\\) to the power of \\(-1\\) playing the role of taking the reciprocal.1\nKatz showed that the new matrix, \\(A^* = \\alpha A(I-\\alpha A)^{-1}\\) contains all the information we need, as it condenses the sums of all the status that a persons gets from all their connections both direct and indirect, regardless of how indirect, and it weighs each person’s contribution to each other person’s status by the status of those people (which is calculated in the same way). Math magic to the rescue!\nLet’s see how it works, step by step:\n\nFirst we create the \\(12 \\times 12\\) identity matrix \\(I_{12 \\times 12}\\), show in Table 21.4 (a). As noted, this matrix has twelve ones across the diagonals and zeroes everywhere else. Then we choose a value for alpha. There are obscure mathematical reasons for why this value cannot be too big (depending on \\(A\\)), but for this example \\(\\alpha = 0.45\\) will work.\nSecond, we multiply \\(\\alpha\\) times the original adjacency matrix (shown in Figure 21.1) to get \\(\\alpha A\\). This new matrix is shown in Table 21.4 (b). In the new \\(\\alpha A\\) matrix, for every cell in which there is one in \\(A\\), the value 0.45 now appears in \\(\\alpha A\\).\nThird, we subtract \\(I\\) from \\(\\alpha A\\) , to get \\(I-\\alpha A\\). This new matrix is shown in Table 21.4 (c). Note that what this does is to add ones to the diagonals of \\(\\alpha A\\) and change all the other non-zero entries from positive to negative.\nFourth, we find the matrix that equals the reciprocal of \\(I-\\alpha A\\) (also called the matrix inverse of \\(I-\\alpha A\\)) to get \\((I-\\alpha A)^{-1}\\). The matrix inverse is somewhat involved to calculate for larger matrices like \\(I-\\alpha A\\), so, for now, chalk the numbers in Table 21.4 (d) up to math magic. Essentially you are trying to find a new matrix \\(W\\) such that when you multiply it by \\((I-\\alpha A)\\) you get \\(I\\) as the result; that is a matrix \\(W\\) is the inverse of another matrix \\((I-\\alpha A)\\) if and only if \\(W(I-\\alpha A) = I\\). This is just like the the reciprocal of any number \\(x\\), namely, \\(\\frac{1}{x}\\) always equals to one when multiplied by that number: \\(\\frac{1}{x} \\times x = \\frac{x}{x} = 1\\).\nFifth we multiply \\(\\alpha A\\) (shown in Table 21.4 (b)) times the new matrix \\(W = (I-\\alpha A)^{-1}\\) (shown in Table 21.4 (d)) to get the answer to \\(\\alpha A(I-\\alpha A)^{-1}\\). This new matrix, called the Katz proximity matrix is shown in Table 21.4 (e).2 In this matrix, the larger the number in the cell, the more node \\(i\\) is connected to node \\(j\\) via indirect connections.\nFinally, we compute the column sums of the Katz proximity matrix. In equation form:\n\n\\[\ns^{katz}_i = \\sum_jA^*_{ij}\n\\tag{21.9}\\]\nWith \\(A*\\) computed using Equation 21.8. The resulting scores are shown in Table 21.2 (e) for each node of Figure 21.1.\nAs we can see, according to the Katz’s status score, node \\(E\\) is still the highest status node in the network. They are followed by nodes \\(B\\) and \\(J\\), closely agreeing with the endogenous status scores obtained using the in-degree (Table 21.2 (d)). This makes sense, since the Katz scores can be seen as a generalization of the endogenous degree measure, with the latter taken into account only the first step links, and Katz’s taking into account all the indirect links regardless of lengths (but counting the really long ones very little, and counting the first step ones the most). In this way, the Katz approach is the most comprehensive way to compute the status of nodes using only endogenous network information.\n\n\nTable 21.4: Example of estimating status in social networks using exogeneous and endogeneous information for nodes.\n\n\n\n\n(a) Twelve by twelve identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Adjacency matrix multiplied by alpha \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0.0 \n    0.5 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.5 \n    0.5 \n    0.0 \n    0.0 \n  \n  \n    B \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.5 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    C \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.5 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    D \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.5 \n  \n  \n    E \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n  \n  \n    F \n    0.0 \n    0.5 \n    0 \n    0.5 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    G \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    H \n    0.5 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n  \n  \n    I \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    J \n    0.5 \n    0.5 \n    0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    K \n    0.5 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    L \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n    0.5 \n    0.0 \n    0.0 \n    0.5 \n    0.0 \n  \n\n\n\n\n\n\n\n\n(c) Adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1.0 \n    -0.5 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    -0.5 \n    -0.5 \n    0.0 \n    0.0 \n  \n  \n    B \n    0.0 \n    1.0 \n    0 \n    0.0 \n    -0.5 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    C \n    0.0 \n    0.0 \n    1 \n    0.0 \n    0.0 \n    -0.5 \n    0.0 \n    -0.5 \n    -0.5 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    D \n    0.0 \n    0.0 \n    0 \n    1.0 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    -0.5 \n  \n  \n    E \n    0.0 \n    0.0 \n    0 \n    0.0 \n    1.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n  \n  \n    F \n    0.0 \n    -0.5 \n    0 \n    -0.5 \n    -0.5 \n    1.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    G \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    1.0 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    H \n    -0.5 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    -0.5 \n    0.0 \n    1.0 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n  \n  \n    I \n    0.0 \n    0.0 \n    0 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n    0.0 \n    1.0 \n    0.0 \n    0.0 \n    0.0 \n  \n  \n    J \n    -0.5 \n    -0.5 \n    0 \n    0.0 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n    0.0 \n    1.0 \n    0.0 \n    0.0 \n  \n  \n    K \n    -0.5 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    0.0 \n    -0.5 \n    0.0 \n    1.0 \n    0.0 \n  \n  \n    L \n    0.0 \n    0.0 \n    0 \n    0.0 \n    0.0 \n    -0.5 \n    0.0 \n    -0.5 \n    0.0 \n    0.0 \n    -0.5 \n    1.0 \n  \n\n\n\n\n\n\n\n\n(d) Inverse of adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2.1 \n    2.7 \n    0 \n    0.7 \n    2.6 \n    1.5 \n    1.2 \n    0.1 \n    1.6 \n    2.2 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    1.8 \n    0 \n    0.3 \n    1.4 \n    0.6 \n    0.8 \n    0.1 \n    0.6 \n    0.8 \n    0.1 \n    0.1 \n  \n  \n    C \n    1.8 \n    3.5 \n    1 \n    1.3 \n    3.9 \n    3.0 \n    1.6 \n    0.7 \n    2.1 \n    2.9 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.5 \n    3.0 \n    0 \n    2.3 \n    3.3 \n    2.8 \n    1.4 \n    0.5 \n    1.5 \n    2.4 \n    0.5 \n    1.0 \n  \n  \n    E \n    0.8 \n    1.5 \n    0 \n    0.5 \n    2.5 \n    1.0 \n    0.7 \n    0.1 \n    0.7 \n    1.5 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.2 \n    2.9 \n    0 \n    1.4 \n    3.3 \n    3.0 \n    1.3 \n    0.3 \n    1.3 \n    2.2 \n    0.3 \n    0.6 \n  \n  \n    G \n    0.2 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    1.1 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.3 \n    4.0 \n    0 \n    1.4 \n    4.1 \n    3.1 \n    1.8 \n    1.3 \n    2.0 \n    3.5 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.4 \n    0.7 \n    0 \n    0.2 \n    1.1 \n    0.5 \n    0.3 \n    0.0 \n    1.3 \n    0.7 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.7 \n    3.3 \n    0 \n    1.0 \n    3.3 \n    2.3 \n    1.5 \n    0.2 \n    1.6 \n    3.4 \n    0.2 \n    0.5 \n  \n  \n    K \n    1.1 \n    1.5 \n    0 \n    0.4 \n    1.7 \n    0.9 \n    0.7 \n    0.1 \n    1.3 \n    1.3 \n    1.1 \n    0.2 \n  \n  \n    L \n    2.1 \n    3.8 \n    0 \n    1.4 \n    4.1 \n    3.2 \n    1.7 \n    0.7 \n    2.1 \n    3.2 \n    0.7 \n    1.7 \n  \n\n\n\n\n\n\n\n\n(e) Katz proximity matrix (original adjacency matrix multiplied by alpha times the inverse of the adjacency matrix multiplied by alpha subtracted from the identity matrix) \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1.1 \n    2.7 \n    0 \n    0.7 \n    2.6 \n    1.5 \n    1.2 \n    0.1 \n    1.6 \n    2.2 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    0.8 \n    0 \n    0.3 \n    1.4 \n    0.6 \n    0.8 \n    0.1 \n    0.6 \n    0.8 \n    0.1 \n    0.1 \n  \n  \n    C \n    1.8 \n    3.5 \n    0 \n    1.3 \n    3.9 \n    3.0 \n    1.6 \n    0.7 \n    2.1 \n    2.9 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.5 \n    3.0 \n    0 \n    1.3 \n    3.3 \n    2.8 \n    1.4 \n    0.5 \n    1.5 \n    2.4 \n    0.5 \n    1.0 \n  \n  \n    E \n    0.8 \n    1.5 \n    0 \n    0.5 \n    1.5 \n    1.0 \n    0.7 \n    0.1 \n    0.7 \n    1.5 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.2 \n    2.9 \n    0 \n    1.4 \n    3.3 \n    2.0 \n    1.3 \n    0.3 \n    1.3 \n    2.2 \n    0.3 \n    0.6 \n  \n  \n    G \n    0.2 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    0.1 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.3 \n    4.0 \n    0 \n    1.4 \n    4.1 \n    3.1 \n    1.8 \n    0.3 \n    2.0 \n    3.5 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.4 \n    0.7 \n    0 \n    0.2 \n    1.1 \n    0.5 \n    0.3 \n    0.0 \n    0.3 \n    0.7 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.7 \n    3.3 \n    0 \n    1.0 \n    3.3 \n    2.3 \n    1.5 \n    0.2 \n    1.6 \n    2.4 \n    0.2 \n    0.5 \n  \n  \n    K \n    1.1 \n    1.5 \n    0 \n    0.4 \n    1.7 \n    0.9 \n    0.7 \n    0.1 \n    1.3 \n    1.3 \n    0.1 \n    0.2 \n  \n  \n    L \n    2.1 \n    3.8 \n    0 \n    1.4 \n    4.1 \n    3.2 \n    1.7 \n    0.7 \n    2.1 \n    3.2 \n    0.7 \n    0.7"
  },
  {
    "objectID": "lesson-sna-status.html#hubbells-tweak-on-katzs-score",
    "href": "lesson-sna-status.html#hubbells-tweak-on-katzs-score",
    "title": "23  Status",
    "section": "23.6 Hubbell’s Tweak on Katz’s Score",
    "text": "23.6 Hubbell’s Tweak on Katz’s Score\nSo far, we have discussed two main ways to measure the status of node in a social network, both based on the similar principle that people gain status from being (directly or indirectly) connected to high-status others (and get less status from being directly or indirectly connected to low status others). There are two ways to get a sense of the status of others. On the exogenous approach, we use some kind of prior ranking or knowledge (see Table 23.2 (b)) on the endogenous approach, we use only information on network connectivity (in-degree or the Katz approach).\nWhat if there was a way to combine both approaches and get the best of both worlds? That is, we count all the paths coming into a node from other nodes—of any length— and we weigh each path so that paths that start with high status nodes (according to our exogenous scores) count for more than paths that have low status nodes as their starting point.\nThis is exactly what was proposed by Hubbell (1965). It revolves around a relatively small tweak on Katz’s approach. The trick is to take the Katz proximity matrix computed according to to Equation 23.9 and pre-multiply it not by a row vector full of ones, like with Equation 23.9, but by the external row vector of status scores \\(\\mathbf{b}\\) itself.\nIn equation form:\n\\[\ns^{hubbell} = \\mathbf{b}(I-\\alpha A)^{-1}\n\\tag{23.10}\\]\nThe resulting Hubbell status scores are shown in Table 23.2 (f) for each node in Figure 23.1. As we can see, incorporating both endogenous and exogenous status information changes the picture, provided by the Katz scores creating more separation between high and low status nodes.\nNow, node \\(E\\) is the indisputable highest status node in the network, followed, at a distant second and third place, by nodes \\(B\\) and \\(F\\). Combining both endogenous and exogenous sources of information does reveal a deeper status inequalities in social networks."
  },
  {
    "objectID": "lesson-sna-status.html#references",
    "href": "lesson-sna-status.html#references",
    "title": "23  Status",
    "section": "References",
    "text": "References\n\n\n\n\nHubbell, Charles H. 1965. “An Input-Output Approach to Clique Identification.” Sociometry, 377–99.\n\n\nKatz, Leo. 1953. “A New Status Index Derived from Sociometric Analysis.” Psychometrika 18 (1): 39–43.\n\n\nVigna, Sebastiano. 2016. “Spectral Ranking.” Network Science 4 (4): 433–45."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#bipartite-graphs",
    "href": "lesson-nets-affiliation-networks.html#bipartite-graphs",
    "title": "22  Affiliation Networks",
    "section": "22.1 Bipartite Graphs",
    "text": "22.1 Bipartite Graphs\nA bipartite graph is useful to represent a network where, rather than ties occurring between nodes of the same kind (e.g., people connected with other people), ties occur only between nodes of different kinds but never between nodes of the same kind. Typically, the two different types of nodes are located at different levels of analysis or aggregation. As such, bipartite graphs are perfect for capturing the sociological concept of affiliation or membership with larger groups or events (Breiger 1974). For instance, actors and the movies they make, scientists and the papers they write, or people and the groups they belong to.1\n\n\n\n\n\nFigure 22.1: A bipartite graph. Circles are people and triangles are the corporate boards they belong to\n\n\n\n\nFor example, people work at companies, so we might say that a worker is connected with the company, rather than any specific individual there. People also connect to sports teams, schools, religious communities, and other organizations which can have an influence in structuring their social world.\nIn the graph theoretic sense, a bipartite graph \\(G_B\\) is a graph featuring two sets of nodes \\(V_1\\) and \\(V_2\\) and one set of edges \\(E\\). Thus a bipartite graph, like a signed and a weighted graph, is a set of three sets:\n\\[\n    G_B = (E, V_1, V_2)\n\\tag{22.1}\\]\n\nrepresents a network diagram of a bipartite graph where circles connect to triangles (with the shapes standing as labels for the two set of nodes). In the Figure, \\(V_1 = \\{A, B, C, D, E\\}\\) and \\(V_2 = \\{1, 2, 3, 4, 5\\}\\). The edge set \\(E\\) is \\(\\{A1, A2, B2, B3, C2, C4, D4, D3, E3, E5\\}\\).\n\nOne common example of two-mode networks that be represented using bipartite graphs in sociology are corporate interlock networks (Mizruchi 1983). If 1) represented such a network, we could think of the circles as members of the company’s board, and the triangles are the board from each company. Because the same executive can be a member of more than one company’s board, board member A is on the board of both companies 1 and 2, while board member B is on the board of companies 2 and 3.\nNote that edges in a bipartite graph are symmetrical and thus bipartite graphs are (generally) undirected. This makes sense, since the relationship affiliation or membership is indeed symmetrical by definition. If person A is a member of the board in company 2 then it is understood that company 2 has person A as a board member.\nIn the same way, note that there is no reason why the cardinality of two node sets in a bipartite graph have to be same (although they are in the example provided). In a real world corporate interlock network, for instance, there will generally be more people than companies, so \\(|V_1| > |V_2|\\)."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "href": "lesson-nets-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "title": "22  Affiliation Networks",
    "section": "22.2 Unipartite Projections of Bipartite Graphs",
    "text": "22.2 Unipartite Projections of Bipartite Graphs\nWhile the information we can glean from looking at the original bipartite graph alone may be useful, you might realize that board members A and B both are on the boards of company 2! In fact, board member C is also on the board of company 2! We might thus conclude that board members A, B, and C all know each other from sitting in the same company board.\n\n\n\n\n\nFigure 22.2: A unipartite graph. People are linked if they serve in the same company board\n\n\n\n\nIf this sort of information was important, we could convert the bipartite into a simple unipartite graph capturing connections between the same level of analysis. This is called a projection of the original bipartite graph. In the projected graph, two board members are joined by a symmetric tie if they both serve on the board of at least one company together.\n\n\n\n\n\nFigure 22.3: Another unipartite graph. Boards are linked if they share members.\n\n\n\n\nThus, we could, as shown in Figure 22.2, create a graph that shows board members who know each other because they work at the same company. The resulting (simple, undirected) graph shows that board members A, B, and C all know each other as a result of serving in the board of company 2 together.\nLikewise, we can transform the bipartite graph into a simple unipartite graph that captures companies that share board members. Company 2 is thus connected to Companies 1 (because of person A), 3 (because of person B), and 4 (because of person 5). This is shown in Figure 22.3). In fact, the reason why these are called interlock networks, is because it is easy to see that, ultimately, by virtue of sharing members across boards, most big corporations in the U.S. (and other countries), end up forming part of a single giant network."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "href": "lesson-nets-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "title": "22  Affiliation Networks",
    "section": "22.3 From Biparite Graph to Affiliation Matrix",
    "text": "22.3 From Biparite Graph to Affiliation Matrix\nConsider the two-mode network shown in Figure 22.4. This is an affiliation network meant to represent the memberships of six students in five college activity clubs. As discussed earlier, we use a bipartite graph to represent the network. The bipartite graph represents the two sets of nodes using different shapes or colors (blue and red nodes in Figure 22.4), and draws a link between the people and the group if the person is affiliated with the group.\n\n\n\n\n\nFigure 22.4: Bipartite graph of a two-mode network of students and clubs.\n\n\n\n\nHow can we translate the graph representation into a matrix?\nThe procedure is the same as that used to build the adjacency matrix of the symmetric graph. We build a rectangular matrix whose number of rows is the same as the number of people in the affiliation network, and whose number of rows is the same as the number of groups. The matrix is rectangular (as opposed to square) because in a two-mode network, there is no restriction that the size of the two vertex sets be the same (although if they happen to be the same then you end up with a square matrix; after all, a square is a special case of a rectangle!).\nIn graph theory terms, this is a matrix that we call A, for affiliation matrix of dimensions \\(R \\times C\\), where the number of rows \\(R = |V_1|\\) is the cardinality of the first vertex set in the bipartite graph (persons in Figure 22.4), and where the number of rows \\(C = |V_2|\\) is the cardinality of the second vertex set (clubs in Figure 22.4)). The cells of the affiliation matrix, \\(a_{ij} = 1\\) if person i belongs to club j (there’s an symmetric edge in the graph linking the person to the group), otherwise, \\(a_{ij} = 0\\).\nFollowing these instructions would yield the affiliation matrix shown in Table 22.1.\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nGabriela\n1\n1\n1\n1\n0\n\n\nParker\n1\n0\n1\n0\n0\n\n\nBrandon\n0\n1\n1\n1\n0\n\n\nMarie\n0\n0\n1\n0\n1\n\n\nRahul\n0\n1\n0\n1\n0\n\n\nMinjoo\n0\n0\n0\n0\n1\n\n\n\nTable 22.1: Affiliation matrix of a bipartite graph.\n\n\nThe affiliation matrix has some interesting properties. For instance, just like the adjacency matrix, it can be used to compute node degree centrality for each set of nodes. But since we have two different sets of nodes, we end up with two different sets of centrality scores; one set of centrality scores for the people and another set for the groups (Faust 1997).\nLet us see how this works."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#group-and-person-centralities",
    "href": "lesson-nets-affiliation-networks.html#group-and-person-centralities",
    "title": "22  Affiliation Networks",
    "section": "22.4 Group and Person Centralities",
    "text": "22.4 Group and Person Centralities\n\n22.4.1 Person Centralities\nIf we wanted to figure out the degree centrality of the people node set (abbreviated P) in the affiliation matrix, we would sum cell entries across the rows, according to the now familiar equation:\n\\[\n    C_P^{DEG} = \\sum_j a_{ij}\n\\tag{22.2}\\]\nWhich leads to the following vector of degree centrality scores for the people:\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\n4\n2\n3\n2\n2\n1\n\n\n\nTable 22.2: Degree centrality scores for the people.\n\n\nThe degree centrality scores for the people can be interpreted as giving us a sense of their joining activity (e.g., high versus low). Some people, (like Gabriela) join a lot of clubs; they have multiple interests spread out across many organizations. Other people, (like Minjoo), just have a single interest, and thus join only one club (the Cheese Club). If centrality is defined using the “more/more principle” discussed in lesson on centrality, then we would say that Gabriela is more central than Minjoo in the affiliation network.\n\n\n22.4.2 Group Centralities\nIn the same way, if wanted to compute the degree centralities of other mode (the club node set, abbreviated as G), then we would calculate the column sums of the affiliation matrix using a slight variation of Equation 22.2), like we did when we switched from outdegree to indegree:\n\\[\n    C_G^{DEG} = \\sum_i a_{ij}\n\\tag{22.3}\\]\nWhich leads to the following degree centrality scores for the clubs:\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\n2\n3\n4\n3\n2\n\n\n\nTable 22.3: Degree centrality scores for the clubs.\n\n\nJust like the people, the centrality scores for the clubs tell us something about the popularity of each group. Some groups are popular (have lots of members), others are less so. So, it seems like in this student group, the Magic Club is definitely the most popular, containing four members. The Cheese and Fashion Clubs on the other hand, seem to be more niche pursuits, with only two members each."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#the-affiliation-matrix-transpose",
    "href": "lesson-nets-affiliation-networks.html#the-affiliation-matrix-transpose",
    "title": "22  Affiliation Networks",
    "section": "22.5 The Affiliation Matrix Transpose",
    "text": "22.5 The Affiliation Matrix Transpose\nAs discussed in Section 16.1.1, it is possible to “flip” the rows and columns of any matrix, so what was previously the rows become the columns, and what was previously the columns become the rows. This is called the matrix transpose and if the original matrix was called A, then the transpose is called A’.2 If the original matrix A was of dimensions \\(R \\times C\\) then the transpose A’ is of dimensions \\(C \\times R\\).\nThe transpose of the affiliation matrix shown in Table 22.1 is shown in Table 22.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nFashion\n1\n1\n0\n0\n0\n0\n\n\nNerdfighters\n1\n0\n1\n0\n1\n0\n\n\nMagic\n1\n1\n1\n1\n0\n0\n\n\nSuper Smash Brs.\n1\n0\n1\n0\n1\n0\n\n\nCheese\n0\n0\n0\n1\n0\n1\n\n\n\nTable 22.4: Transpose of the affiliation Matrix.\n\n\nNote that the transpose of the affiliation matrix contains exactly the same information as the original affiliation matrix. The group affiliations of every person are preserved as are memberships of each group. If we used equations Equation 22.2, and Equation 22.3) to compute the person and group centralities using the affiliation matrix transpose A’, we would get the same results, except that the first equation (summing across the rows) would now give us the group centralities, and the second equation (summing down the columns) would give use the people centralities!\nWe learn from matrix algebra that an important property of rectangular matrices is that you can always multiply a rectangular matrix by its transpose (see Section 16.1). Recall a key condition of matrix multiplication is that the two matrices be conformable so that the columns of the first matrix need to match the number of rows of the second matrix. Well, it’s clear than since any matrix that is of dimensions \\(R \\times C\\), will have a transpose of dimensions \\(C \\times A\\) then the multiplication of the two matrices will be defined:\n\\[\n    A^{}_{R \\times C} \\times A^{'}_{C \\times R} = defined!\n\\tag{22.4}\\]\nIn the same way, the transpose of a matrix can always be multipled by the original matrix:\n\\[\n    A^{'}_{C \\times R} \\times A^{}_{R \\times C} = defined!\n\\tag{22.5}\\]"
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "href": "lesson-nets-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "title": "22  Affiliation Networks",
    "section": "22.6 The Person and Group Overlap Matrices",
    "text": "22.6 The Person and Group Overlap Matrices\nIf the transpose of the affiliation matrix contains the same information as the original why do we care about it? Well the reason is that we can use the multiplication property described in Section 16.1.1 to extract two new matrices that contain new (or at least not obvious, especially for large two-mode networks), information from the original affiliation matrix. The first is called the person overlap matrix (written \\(O^P\\)), this is defined for an original affiliation matrix, in which people are listed in the rows and groups, events, or project, listed in the columns using the following matrix equation:\n\\[\n    O^P = A^{ }_{R \\times C} \\times A^{'}_{C \\times R}\n\\tag{22.6}\\]\n\n22.6.1 The Person Overlap Matrix\nUsing the rules for matrix multiplication discussed Section 16.1, the person overlap matrix obtained using the affiliation matrix shown in Table 22.1 is shown in Table 22.5.\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nGabriela\n4\n2\n3\n1\n2\n0\n\n\nParker\n2\n2\n1\n1\n0\n0\n\n\nBrandon\n3\n1\n3\n1\n2\n0\n\n\nMarie\n1\n1\n1\n2\n0\n1\n\n\nRahul\n2\n0\n2\n0\n2\n0\n\n\nMinjoo\n0\n0\n0\n1\n0\n1\n\n\n\nTable 22.5: Person Overlap Matrix.\n\n\nThe person overlap matrix transforms the initial rectangular affiliation matrix, which has people in the rows and groups in the columns, to a square matrix, which like the usual relationship matrices we have been dealing with, feature people in both the rows and the columns. Each entry in the person overlap matrix \\(o^P_{ij}\\) now gives us the number of groups in which person i and j mutually belong to (Breiger 1974). So we learn that Gabriela and Brandon have three memberships in common (I bet they see one another a lot!) but that Rahul and Parker have no memberships in common (so they are less likely to encounter one another).\nNote also that, in the person overlap matrix, (in contrast to the usual adjacency matrix), there are valid entries along the diagonal cells (\\(o^P_{ii}\\)). These cells now record the total number of memberships that the node corresponding to that row (or column) has. Which we ascertained by computing the node centralities in the original affiliation matrix using Equation 22.2). You can see that the vector of degree centralities shown in Table 22.2) is the same as the vector formed by the diagonal entries in ?tbl-comem).\n\n\n22.6.2 The Group Overlap Matrix\nIn the same way we can compute the person overlap matrix, it is possible to calculate another matrix, called the group overlap matrix (written \\(O^G\\)), this time by multiplying the transpose of the original affiliation matrix times the original We do that using the following equation:\n\\[\n    O^G = A^{'}_{C \\times R} \\times A^{ }_{R \\times C}\n\\tag{22.7}\\]\nRecall from Chapter 16 that matrix multiplication is not commutative (if \\(A\\) is a rectangular matrix, then \\(A \\times A^{'} \\neq A^{'} \\times A\\))), so Equation 22.7 gives you a different answer than Equation 22.6. The result is shown in Table 22.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nFashion\n2\n1\n2\n1\n0\n\n\nNerdfighters\n1\n3\n2\n3\n0\n\n\nMagic\n2\n2\n4\n2\n1\n\n\nSuper Smash Brs.\n1\n3\n2\n3\n0\n\n\nCheese\n0\n0\n1\n0\n2\n\n\n\nTable 22.6: Group Overlap Matrix.\n\n\nThe group overlap matrix (O), like the person overlap matrix, is also square. But this time it has groups in both the rows and columns. Each cell in the group overlap matrix \\(o^P_{ij}\\) records the number of people groups i and groups j have in common (Breiger 1974). Thus, we learn that the Super Smash Brothers group and the Nerdfighters groups share three members in common but that the Super Smash Brothers and the Cheese group have no members in common (pointing to a disaffinity between these activities).\nNote that both the person and group overlap matrices are symmetric. It is easy to see why this is; if I have three group overlaps with you, then you by definition also have three group overlaps with me; if group A has three members in common with group B, then group B has three members in common with group A. This means that if they were to be taken as representing a network, then the resulting graph would be undirected (but weighted because there can be more or less overlap between people and groups). We will see how to do that below."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "href": "lesson-nets-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "title": "22  Affiliation Networks",
    "section": "22.7 Overlapping Node Neighborhoods in Two-Mode Networks",
    "text": "22.7 Overlapping Node Neighborhoods in Two-Mode Networks\nThe notion of overlap used to construct the person and group overlap matrix is the same as the idea of overlapping node neighborhoods for regular networks, discussed in Chapter 6 Thus, while nodes of the same kind cannot be connected in a two-mode network (by construction), they can share neighbors. In a two-mode network if a node belong to one of the vertex sets, let’s say \\(V_1\\), then all of their neighbors have to belong to the other vertex set (\\(V_2\\)) and vice versa.\nFor instance, in Figure 22.4), Gabriela’s node neighborhood is:\n\\[\n    Gab_{NN} = \\{Fashion, Nerdfighters, Magic, SuperSmashBros\\}\n\\]\nRahul’s node neiborhood is:\n\\[\n    Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nThe intersection between their neighborhoods is:\n\\[\n    Gab_{NN} \\cap Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nSo now we can see that the number “2” recorded in the cell that corresponds to Gabriela and Rahul in the person overlap matrix shown in Table 22.5) is the cardinality of the subset formed by the intersection of their two neighborhoods, which in this case contain two members (the Nerdfighters and Super Smash Brothers clubs). The same procedure can be used to figure out the overlap between the node neighborhoods of groups (which happen to be subsets of people in the larger two-mode network)."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "href": "lesson-nets-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "title": "22  Affiliation Networks",
    "section": "22.8 One Mode Projections of two-mode Networks",
    "text": "22.8 One Mode Projections of two-mode Networks\nNote that both the comembership and group overlap matrices, being square matrix with values that go beyond zero and one in the cells, look like a lot like the adjacency matrix that could be obtained from a weighted graph as discussed in the lesson on types of graphs.\n\n\n\n\n\nFigure 22.5: One mode (persons) projection of the original bipartite graph.\n\n\n\n\nSo using formulas Equation 22.6) and Equation 22.7), it is possible to go from a two-mode network in which no links exist between nodes of the same kind, to a weighted graph, in which the links between nodes of the same kind are defined by the overlap of their neighborhoods in the original bipartite graph. As we noted earlier, this is called the one mode projection of the two-mode network. Each two-mode network thus has two one mode projection one for each node set.\n\n\n\n\n\nFigure 22.6: One mode (groups) projection of the original bipartite graph.\n\n\n\n\nThe one mode projection for the person node set of the bipartite graph show in Figure 22.4) is shown in Figure 22.5), this is an undirected weighted graph with the edge weight between people being set to the number of comemberships between each dyad as recorded in the person overlap matrix shown in Table 22.5). In this respect, the number of comemberships can be seen as a proxy of the tie strength between two people, when we only have information on their affiliations. As the Figure shows, Brandon, Gabriela and Rahul form a tightly connected clique, given the number of memberships they share. Minjoo, who does not share many affiliations with anyone, stands toward the periphery of the person-to-person comembership network.\nThe corresponding one-mode projection for the group node set is shown in Figure 22.6). This weighted graph can be read the same way: The thickness of the ties between groups are proportion to the people they share as recorded in the group overlap matrix shown in Table 22.6), thus speaking to the similarity or strength of connectivity between groups.\nSo we see, as we noted before, that Super Smash Brothers and Nerdfighters are tightly connected, but that the Cheese Club is largely peripheral in the group-to-group network. This peripheral status mirrors the marginal status of Minjoo (one of the few members of the Cheese Club) in the person-to-person network.\nThe fact that peripheral people belong peripheral groups and central people belong to central groups encodes a fundamental principle in the analysis of two-mode networks (Breiger 1974) and that is the duality principle.\nThe duality principle in two-mode network analysis says that the position of people in a two-mode network is defined by the positions the groups they affiliate with occupy, and in the same way, the position of the groups in a two-mode network is defined by the positions of the people that belong to them (Bonacich 1991)."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#references",
    "href": "lesson-nets-affiliation-networks.html#references",
    "title": "22  Affiliation Networks",
    "section": "References",
    "text": "References\n\n\n\n\nBonacich, Phillip. 1991. “Simultaneous Group and Individual Centralities.” Social Networks 13 (2): 155–68.\n\n\nBreiger, Ronald L. 1974. “The Duality of Persons and Groups.” Social Forces 53 (2): 181–90.\n\n\nFaust, Katherine. 1997. “Centrality in Affiliation Networks.” Social Networks 19 (2): 157–91.\n\n\nMizruchi, Mark S. 1983. “Who Controls Whom? An Examination of the Relation Between Management and Boards of Directors in Large American Corporations.” Academy of Management Review 8 (3): 426–35."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-mattrans",
    "href": "lesson-matrix-multiplication.html#sec-mattrans",
    "title": "16  Matrix Multiplication",
    "section": "16.2 Multiplying a Matrix Times its Transpose",
    "text": "16.2 Multiplying a Matrix Times its Transpose\n\nBy definition, as discussed in ?sec-trans, the rows of a matrix are equal to the columns of its transpose, and vice versa. The product of a matrix times its transpose and the transpose times the original matrix is always defined, no matter what the dimensions of the original matrix are. Thus,\n\n\\[\n\\mathbf{A} \\times \\mathbf{A}^T = defined!\n\\]\n\\[\n\\mathbf{A}^T \\times \\mathbf{A} = defined!\n\\]\n\nWhen you multiply a matrix times its transpose, the resulting matrix will be a square matrix with number of rows and columns equal to the number of rows of the original matrix. For instance, say matrix \\(\\mathbf{A}_{5 \\times 3}\\) is of dimensions \\(5 \\times 3\\) (like the matrix shown in Table 16.1 (a)). Then its transpose \\(A^T_{3 \\times 5}\\) will be of dimensions \\(3 \\times 5\\) (like the matrix shown in Table 16.1 (b)). That means the product of the matrix times its transpose will be:\n\n\\[\n\\mathbf{A}_{5 \\times 3} \\times \\mathbf{A}_{3 \\times 5}^T = \\mathbf{B}_{5 \\times 5}\n\\tag{16.2}\\]\n\nEquation 16.2 says that a five by three matrix multiplied by its transposed yields a square matrix \\(\\mathbf{B}\\) of dimensions five by five (a square matrix with five rows and five columns). In the same way,\n\n\\[\n\\mathbf{A}_{3 \\times 5}^T \\times \\mathbf{A}_{5 \\times 3} = \\mathbf{B}_{3 \\times 3}\n\\tag{16.3}\\]\n\nEquation 16.3 says that the transpose of a five by three matrix multiplied by the original yields a product matrix \\(\\mathbf{B}\\) of dimensions three by three (a square matrix with three rows and three columns)."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-pows",
    "href": "lesson-matrix-multiplication.html#sec-pows",
    "title": "16  Matrix Multiplication",
    "section": "16.3 Matrix Powers",
    "text": "16.3 Matrix Powers\n\nYou can multiply a matrix times itself to get matrix powers but only if matrix is a square matrix (has the same number of rows and columns). Thus,\n\n\\[\n\\mathbf{A}^2 = \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^3 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^4 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^n = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\ldots\n\\]\n\nFor all square matrices \\(\\mathbf{A}\\) of any dimension. Since matrices used to represent social networks, like the adjacency matrix are square matrices, that means that you can always find the powers of an adjacency matrix.\nWhen you multiply a square matrix times another square matrix of the same dimensions, the resulting matrix is of the same dimensions as the original two matrices. Thus,\n\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = \\mathbf{A}^2_{5 \\times 5}\n\\]\n\n\nTable 16.1: A matrix and its transpose\n\n\n\n\n(a) Original Matrix. \n\n  \n    3 \n    4 \n    5 \n  \n  \n    7 \n    9 \n    3 \n  \n  \n    4 \n    6 \n    2 \n  \n  \n    5 \n    3 \n    4 \n  \n  \n    2 \n    5 \n    4 \n  \n\n\n\n\n\n\n(b) Transposed Matrix. \n\n  \n    3 \n    7 \n    4 \n    5 \n    2 \n  \n  \n    4 \n    9 \n    6 \n    3 \n    5 \n  \n  \n    5 \n    3 \n    2 \n    4 \n    4"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-matvecs",
    "href": "lesson-matrix-multiplication.html#sec-matvecs",
    "title": "16  Matrix Multiplication",
    "section": "16.5 Matrix Multiplication of Vectors",
    "text": "16.5 Matrix Multiplication of Vectors\nRecall from Section 8.1 that a vector is a sequence of numbers of a given length. So for instance, the vector \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\) is a vector of length five.\nWell, and here comes the big reveal, it turns out that another way to think of a vector, is as a special case of matrix. That is, a matrix with one row, and as many columns as the length of the vector! This is a called a row vector. So the row vector \\(\\mathbf{a}\\) vector can be thought of as a matrix of dimensions \\(1 \\times 5\\) (one row and five columns) or \\(\\mathbf{a}_{1 \\times 5}\\).\nIn matrix form:\n\n\n\n\n\n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\nTable 16.5:  Matrix resulting from multiplying a matrix times its transpose \n\n\nSince vectors are matrices, we can perform the same type of matrix operations on them as we did with matrices. For instance, we can compute the transpose of a vector. In the case of \\(\\mathbf{a}\\), the transpose \\(\\mathbf{a}^T\\) is:\n\n\n\n\n\n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\nTable 16.6:  Matrix resulting from multiplying a matrix times its transpose \n\n\nThe transpose of a row vector is called (you may have guessed) a column vector. The column vector in Table 16.6 is a matrix with five rows and one column.\nThis also means that the same rules of matrix multiplication apply. For instance, we can always multiply a row vector times a column vector, because it is the equivalent of multiplying a matrix times its transpose, and we have already seen in Section 16.2, that this can always be done:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{a}^T_{5 \\times 1} = b_{1 \\times 1}\n\\tag{16.4}\\]\nEquation 16.5 says that the product of the \\(1 \\times 5\\) row vector \\(\\mathbf{a}\\) times a \\(5 \\times 1\\) column vector \\(\\mathbf{a}^T\\) is a \\(1 \\times 1\\) “matrix” otherwise known as a scalar (that is, a regular old number). We’ve already seen examples of this, because in regular matrix multiplication, each cell of the product matrix is a scalar obtained from multiplying the corresponding terms taken from a row of the first matrix (which is a row vector) times those of the column of the second matrix (which is a column vector).\nSo in this case this would be:\n\\[\n(2 \\times 2) + (4 \\times 4) + (7 \\times 7) + (2 \\times 2) + (4 \\times 4) =\n\\]\n\\[\n4 + 16 + 49 + 4 + 16 = 89\n\\]\n\nThe first rule of vector matrix multiplication is that you can always multiply a row vector times a column vector (even when their entries are not the same) as long as they are the same length (e.g., the number of columns of the row vector equal the number of rows of the column vector).\nThe second rule of vector matrix multiplication is that when you multiply a row vector times another a column vector the result is always scalar (a single number).\n\nNow notice that if we change the order, and multiply the transpose of a vector times the original? This should be allowed because it conforms to the rules that we have already discussed:\n\\[\n\\mathbf{a}^T_{5 \\times 1} \\times \\mathbf{a}_{1 \\times 5} = B_{5 \\times 5}\n\\tag{16.5}\\]\nThis matrix multiplication is defined because the inner dimensions of the two matrices (the column and row vectors) are the same (one). But note that, according to the rules of matrix multiplication, when you multiply the transpose of a vector times the original, the result is a square matrix, with dimensions \\(n \\times n\\) where \\(n\\) is the length of the original row vector (the number of columns). In our example if the original vector is \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\), then \\(\\mathbf{a}^T \\times \\mathbf{a}\\) is equal to the matrix shown in Table 16.7.\n\n\n\n\n\n\n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n  \n    14 \n    28 \n    49 \n    14 \n    28 \n  \n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n\n\n\nTable 16.7:  Matrix resulting from multiplying a matrix times its transpose \n\n\n\nSo, the third and final rule of vector matrix multiplication is that when you multiply a column vector times a row vector of the same length, the result is a square matrix of row and column dimensions equal to the length of the original vectors."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-identitymat",
    "href": "lesson-matrix-multiplication.html#sec-identitymat",
    "title": "16  Matrix Multiplication",
    "section": "16.7 The Identity Matrix",
    "text": "16.7 The Identity Matrix\nThe last “interesting” matrix we will cover is called the identity matrix. This is a square matrix, usually written using the symbol \\(\\mathbf{I}\\) of dimensions \\(n \\times n\\). This matrix will have “1” in every diagonal cell, and “0” in every off-diagonal cell. For instance, an identity matrix of dimensions \\(5 \\times 5\\) is shown Table 16.13.\n\n\n\n\n\n\n  \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\nTable 16.13:  A 5 X 5 Identity Matrix. \n\n\nThe interesting thing about this matrix is that when you multiply it times another square matrix of the same dimensions, the result is always the original matrix! So it plays the role that the number “1” plays in regular number multiplication, in matrix algebra. This means that, for any square matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} \\times \\mathbf{I} = \\mathbf{A}\n\\tag{16.10}\\]\nAnd also,\n\\[\n\\mathbf{I} \\times \\mathbf{A} = \\mathbf{A}\n\\tag{16.11}\\]\nNeat!"
  },
  {
    "objectID": "lesson-sna-status.html#katz-status-index",
    "href": "lesson-sna-status.html#katz-status-index",
    "title": "23  Status",
    "section": "23.5 Katz Status Index",
    "text": "23.5 Katz Status Index\nIn the mid-twentieth century, the great statistician Leo Katz set out to develop a measure of status in social networks that took into account indirect connections. He first observed that the new matrix that results from taking the original adjacency matrix and raising it to a power (using matrix multiplication) has a clear interpretation. For instance, as we saw in Section 16.3, if we raise the adjacency matrix \\(A\\) shown in Table 23.1 to the third power (\\(\\mathbf{A}^3\\)) the resulting matrix will contain, as cell entries \\(a^3_{ij}\\), all the directed paths of length \\(l = 3\\) that have row node \\(i\\) as the starting column node and node \\(j\\) as the destination node, the same goes for any number \\(\\mathbf{A}^k\\). This means that the column sums of this matrix—or the row sums of the transpose \\(A^T\\)—will contain the number of paths of length three that are directed from all other nodes to a given node.\nKatz saw this as a way to incorporate indirect connections to construct a measure of status using only endogenous information. The idea would be to say that your total status is the sum of number of other people who choose you (or think you are great, or a great source or advice or whatever). However, among the people that choose you the ones that are chosen by many others should count for more. Those are people who are two-steps away from you. But the same should apply to the people who choose those others (people three-steps away from you). Overall, you should get more status from people who choose the people who choose the people, who choose the people…who choose you and you should get more status from the more people who are most likely to be chosen by those others, across any number of steps.\nTo accomplish this, we need to construct a new matrix that incorporates all this information about people’s one step, two-step, three-step, connections to others, then sum columns of that matrix (or the rows of the transpose). The resulting vector (\\(s^{Katz}_i\\)) would contain the desired score for each node \\(i\\).\nOne way to proceed would be to create the matrix by calculating the following infinite sum:\n\\[\nI + A + A^2 + A^3 + A^4 + \\ldots A^{\\infty} =\n\\]\n\\[\n\\mathbf{1} \\sum_{k = 0}^{k = \\infty} A^k\n\\tag{23.6}\\]\nWith the understanding that \\(A^0 = I\\) (raising a matrix to the zero power returns the identity matrix), and \\(\\mathbf{1}\\) is a row vector full of ones \\(\\mathbf{1} = \\{1, 1, 1, ... 1\\}\\) of the same length as the number of rows in \\(A\\), which when multiplied by \\(\\sum A^k\\) would return the column sums of that matrix (see Section 16.6.3) which would be our vector of Katz centrality scores for each node.\nThere are a couple of problems with Equation 23.6. First this sum keeps getting bigger and bigger and it does not have a natural end point (keeps going forever). This is because it is counting the direct connections (\\(A\\)) as much as the very indirect connections, like \\(A^5\\), or the number of indirect links connecting you to others five steps away.\nWhat we want is a way to count the first-step links the most, and then discount the longer-step links, with the discount getting larger the longer the chain. So that three-steps links count for less than two-steps links but count for more than four step links to others and so forth.\nKatz’s great idea is to multiply the original adjacency matrix and its powers by a number \\(\\alpha\\) that was larger than zero, but less than one. This leads to:\n\\[\nI + \\alpha A + \\alpha^2 A^2 + \\alpha^3 A^3 + \\alpha^4 A^4 + \\ldots \\alpha^\\infty A^{\\infty} =\n\\]\n\\[\n\\mathbf{1} \\sum_{k = 0}^{k = \\infty} \\alpha^k A^k\n\\tag{23.7}\\]\nWith the understanding that \\(\\alpha^0 A^0 = 1 \\times I = I\\), the first term in the sequence is once again the identity matrix. Now the difference between Equation 23.6 and Equation 23.7, is that as we saw before (see Equation 23.5), while the sum in Equation 23.6 keeps getting bigger and bigger (the technical term is “diverges”), the one in Equation 23.7, will stop growing, because raising a number less than one and more than zero (like \\(\\alpha\\)) to a bigger and bigger power will result in a tinier and tinier number, until we get close to zero. The sum will converge rather than diverge.\nMoreover, Katz knew his math, and noted that there is a version of {Equation 23.5} that applies to matrices, and which can be calculated without adding up an infinity of numbers. This is:\n\\[\n\\mathbf{1} \\sum_{k = 0}^{k = \\infty} \\alpha^k A^k = \\mathbf{1} (I - \\alpha A)^{-1}\n\\tag{23.8}\\]\nWhere \\(I\\) refers to the identity matrix (see Section 16.7). What Katz (1953) proposed is that we can turn the infinite sum part of Equation 23.8 (\\(I + \\alpha A + \\alpha A^2 + \\alpha A^3 + \\alpha A^4 + \\ldots \\alpha A^{\\infty})\\) into just \\((I- \\alpha A)^{-1}\\) following the principle outlined earlier in Equation 23.5. Just like the endless sum of squares of a number \\(x\\) between \\(-1\\) and and \\(1\\) just turns into just \\((1-x)^{-1}\\), the endless sums of a matrix containing numbers between \\(-1\\) and \\(1\\) as its entries \\(\\alpha A\\) turns into \\((I-\\alpha A)^{-1}\\), with \\(I\\) playing the role of \\(1\\) and raising \\(I-\\alpha A\\) to the power of \\(-1\\) playing the role of taking the reciprocal.1\nKatz showed that the new matrix, \\(A^* = (I-\\alpha A)^{-1}\\) contains all the information we need, as it condenses the sums of all the status that a persons gets from all their connections both direct and indirect, regardless of how indirect, and it weighs each person’s contribution to each other person’s status by the status of those people (which is calculated in the same way). The column sums of this matrix \\(\\mathbf{1}(I-\\alpha A)^{-1}\\)will give us the Katz status scores we need as the contain information on all the indirect paths that start with other nodes and end in the focal node. Math magic to the rescue!\nLet’s see how it works, step by step:\n\nFirst we create the \\(12 \\times 12\\) identity matrix \\(I_{12 \\times 12}\\), show in Table 23.4 (a). As noted, this matrix has twelve ones across the diagonals and zeroes everywhere else. Then we choose a value for alpha. There are obscure mathematical reasons for why this value cannot be too big (depending on \\(A\\)), but for this example \\(\\alpha = 0.45\\) will work.\nSecond, we multiply \\(\\alpha\\) times the original adjacency matrix (shown in Figure 23.1) to get \\(\\alpha A\\). This new matrix is shown in Table 23.4 (b). In the new \\(\\alpha A\\) matrix, for every cell in which there is one in \\(A\\), the value 0.45 now appears in \\(\\alpha A\\).\nThird, we subtract \\(I\\) from \\(\\alpha A\\) , to get \\(I-\\alpha A\\). This new matrix is shown in Table 23.4 (c). Note that what this does is to add ones to the diagonals of \\(\\alpha A\\) and change all the other non-zero entries from positive to negative.\nFourth, we find the matrix that equals the reciprocal of \\(I-\\alpha A\\) (also called the matrix inverse of \\(I-\\alpha A\\)) to get \\((I-\\alpha A)^{-1}\\). The matrix inverse is somewhat involved to calculate for larger matrices like \\(I-\\alpha A\\), so, for now, chalk the numbers in Table 23.4 (d) up to math magic. Essentially you are trying to find a new matrix \\(W\\) such that when you multiply it by \\((I-\\alpha A)\\) you get \\(I\\) as the result; that is a matrix \\(W\\) is the inverse of another matrix \\((I-\\alpha A)\\) if and only if \\(W(I-\\alpha A) = I\\). This is just like the the reciprocal of any number \\(x\\), namely, \\(\\frac{1}{x}\\) always equals to one when multiplied by that number: \\(\\frac{1}{x} \\times x = \\frac{x}{x} = 1\\). This new matrix, called the Katz proximity matrix is shown in Table 23.4 (d).2 In this matrix, the larger the number in the cell, the more node \\(i\\) is connected to node \\(j\\) via indirect connections.\nFinally, we compute the column sums of the Katz proximity matrix to get the Katz status scores for each node. In equation form:\n\n\\[\ns^{katz}_i = \\mathbf{1} (I - \\alpha A)^{-1}\n\\tag{23.9}\\]\nThe resulting scores are shown in Table 23.2 (e) for each node of Figure 23.1.\nAs we can see, according to the Katz’s status score, node \\(E\\) is still the highest status node in the network. They are followed by nodes \\(B\\) and \\(J\\), closely agreeing with the endogenous status scores obtained using the in-degree (Table 23.2 (d)). This makes sense, since the Katz scores can be seen as a generalization of the endogenous degree measure, with the latter taken into account only the first step links, and Katz’s taking into account all the indirect links regardless of lengths (but counting the really long ones very little, and counting the first step ones the most). In this way, the Katz approach is the most comprehensive way to compute the status of nodes using only endogenous network information.\n\n\nTable 23.4: Example of estimating status in social networks using exogeneous and endogeneous information for nodes.\n\n\n\n\n(a) Twelve by twelve identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Adjacency matrix multiplied by alpha \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0.00 \n    0.45 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    B \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n  \n  \n    E \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    F \n    0.00 \n    0.45 \n    0 \n    0.45 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    H \n    0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    J \n    0.45 \n    0.45 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    K \n    0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    L \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n    0.45 \n    0.00 \n    0.00 \n    0.45 \n    0.00 \n  \n\n\n\n\n\n\n\n\n(c) Adjacency matrix multiplied by alpha subtracted from the identity matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    1.00 \n    -0.45 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    B \n    0.00 \n    1.00 \n    0 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    1 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n  \n  \n    E \n    0.00 \n    0.00 \n    0 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    F \n    0.00 \n    -0.45 \n    0 \n    -0.45 \n    -0.45 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    H \n    -0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    1.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n    0.00 \n  \n  \n    J \n    -0.45 \n    -0.45 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    0.00 \n    1.00 \n    0.00 \n    0.00 \n  \n  \n    K \n    -0.45 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    1.00 \n    0.00 \n  \n  \n    L \n    0.00 \n    0.00 \n    0 \n    0.00 \n    0.00 \n    -0.45 \n    0.00 \n    -0.45 \n    0.00 \n    0.00 \n    -0.45 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(d) Katz proximity matrix (inverse of adjacency matrix multiplied by alpha subtracted from the identity matrix) \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2.1 \n    2.7 \n    0 \n    0.7 \n    2.6 \n    1.5 \n    1.2 \n    0.1 \n    1.6 \n    2.2 \n    0.1 \n    0.3 \n  \n  \n    B \n    0.4 \n    1.8 \n    0 \n    0.3 \n    1.4 \n    0.6 \n    0.8 \n    0.1 \n    0.6 \n    0.8 \n    0.1 \n    0.1 \n  \n  \n    C \n    1.8 \n    3.5 \n    1 \n    1.3 \n    3.9 \n    3.0 \n    1.6 \n    0.7 \n    2.1 \n    2.9 \n    0.3 \n    0.6 \n  \n  \n    D \n    1.5 \n    3.0 \n    0 \n    2.3 \n    3.3 \n    2.8 \n    1.4 \n    0.5 \n    1.5 \n    2.4 \n    0.5 \n    1.0 \n  \n  \n    E \n    0.8 \n    1.5 \n    0 \n    0.5 \n    2.5 \n    1.0 \n    0.7 \n    0.1 \n    0.7 \n    1.5 \n    0.1 \n    0.2 \n  \n  \n    F \n    1.2 \n    2.9 \n    0 \n    1.4 \n    3.3 \n    3.0 \n    1.3 \n    0.3 \n    1.3 \n    2.2 \n    0.3 \n    0.6 \n  \n  \n    G \n    0.2 \n    0.3 \n    0 \n    0.1 \n    0.5 \n    0.2 \n    1.1 \n    0.0 \n    0.6 \n    0.3 \n    0.0 \n    0.0 \n  \n  \n    H \n    2.3 \n    4.0 \n    0 \n    1.4 \n    4.1 \n    3.1 \n    1.8 \n    1.3 \n    2.0 \n    3.5 \n    0.3 \n    0.6 \n  \n  \n    I \n    0.4 \n    0.7 \n    0 \n    0.2 \n    1.1 \n    0.5 \n    0.3 \n    0.0 \n    1.3 \n    0.7 \n    0.0 \n    0.1 \n  \n  \n    J \n    1.7 \n    3.3 \n    0 \n    1.0 \n    3.3 \n    2.3 \n    1.5 \n    0.2 \n    1.6 \n    3.4 \n    0.2 \n    0.5 \n  \n  \n    K \n    1.1 \n    1.5 \n    0 \n    0.4 \n    1.7 \n    0.9 \n    0.7 \n    0.1 \n    1.3 \n    1.3 \n    1.1 \n    0.2 \n  \n  \n    L \n    2.1 \n    3.8 \n    0 \n    1.4 \n    4.1 \n    3.2 \n    1.7 \n    0.7 \n    2.1 \n    3.2 \n    0.7 \n    1.7"
  },
  {
    "objectID": "lesson-sna-eigenvector.html",
    "href": "lesson-sna-eigenvector.html",
    "title": "21  Getting Centrality from Others",
    "section": "",
    "text": "In Chapter 20 we discussed the idea of centrality mainly from a “positional” perspective. That is, a node’s centrality depends on where they are located in the connectivity structure of the graph (Borgatti 2005). Either by being “close” to many others, either directly—as with degree centrality—or indirectly—as with closeness centrality—or being in the “middle” of the connections of many others—as with betweennness centrality.\nBut there is another way of thinking about centrality, and that relates more to the idea that to be central is to be connected to central people, not just to many other people (or being in-between many people). Under this alternative definition, the ultimate way of being central resolves into being connected to many people who are also central.\nFrom this perspective, you get the centrality of the people you are connected to, so that your centrality \\(C_i\\) should be a function of the centrality of the people that are adjacent to you in the graph.\nExpressed as a formula:\n\\[\nC_i = \\sum_j a_{ij}C_j\n\\tag{21.1}\\]\nNow, this looks like it does what we want. The centrality of node \\(i\\), expressed as \\(C_i\\), is a sum of the centrality \\(C_j\\) of the people they are connected to, since the right-hand side of the sum wil be a number that does not equal zero only when \\(a_{ij} = 1\\), that is when node \\(i\\) is connected to node \\(j\\) in the network.\nThe one issue with Equation 21.1 is that we have a chicken and the egg problem. Which centrality comes first? That of node \\(i\\) or node \\(j\\)? We don’t know since the centrality term appears on both sides of the equation. So while Equation 21.1 is conceptually appealing, it’s like a snake eating it’s own tail.\nNevertheless, not all is lost. We can make progress in coming up with a centrality measure in which your centrality is a function of the centrality of others, or more accurately, in which the people you are connected to “give” their centrality to you (and that total amounts makes up your centrality score), by just assigning everybody some “amount” of centrality “points” at the beginning and then using Equation 21.1 to “loop” through the network to calculate everyone else’s centrality.\nLet’s see how this would work.\n\n\n\n\n\nFigure 21.1: A simple graph\n\n\n\n\nFigure 21.1 shows a graph with associated adjacency matrix shown in Table 22.1. Our task is to come up with a centrality metric for all the nodes in this graph, that more or less follows the spirit of Equation 21.1. The idea is to compute a “fist pass” set of centrality scores for each node in the graph, by first “initializing” everyone’s centrality some arbitrary positive number and computing the centralities of each node as the sum of the centralities of the nodes that they are connected to.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n  \n \n\n  \n    A \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    D \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    E \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n\n\n\nTable 21.1:  Adjacency matrix of a simple graph. \n\n\n\n\n\nFor instance, let’s say that we give everyone one centrality point to “give out” everyone they are connected to in the first step. In that case, the centrality of node \\(E\\) in Figure 21.1. We know from looking at Table 22.1 that \\(E\\)’s neighbor set is equal to \\(\\{A, D, F, G, H, J\\}\\). If at the beginning, \\(E\\) gets one “centrality point” from each of their neighbors, then their starting centrality would be equal to:\n\\[\nC(1)_E = C(0)_A + C(0)_D + C(0)_F + C(0)_G + C(0)_H + C(0)_J =\n\\]\n\\[\nC(1)_E = 1 + 1 + 1 + 1 + 1 + 1 = 6\n\\]\nWhere \\(C(0)\\) indicates everyone’s centralities at “step zero” (the centralities everyone begins with), and \\(C(1)\\) indicates everyone’s centralities at the first step.\nAs we can see because everyone just has one centrality point to give out, \\(E\\) initial centrality \\(C(1)_E\\) is just their degree and the same for everybody else!\nWe can record these initial centralities in a table for each node, shown as Table 21.2 (a). Note that because we don’t care about the specific number (which once again is just everyone’s degree), but only about the rank order of nodes, we divide the initial centralities by the maximum observed (the graph’s maximum degree) so that nodes with the maximum degree gets a score of 1.0—in this case \\(D\\) and \\(E\\)—and everyone gets a smaller number depending on how many centrality points they got from their neighbors at step 1.\n\n\nTable 21.2: .\n\n\n\n\n(a) Centrality at step 1 \n\n  \n    A \n    0.50 \n  \n  \n    B \n    0.50 \n  \n  \n    C \n    0.33 \n  \n  \n    D \n    1.00 \n  \n  \n    E \n    1.00 \n  \n  \n    F \n    0.67 \n  \n  \n    G \n    0.33 \n  \n  \n    H \n    0.50 \n  \n  \n    I \n    0.50 \n  \n  \n    J \n    0.67 \n  \n\n\n\n\n\n\n(b) Centrality at step 2 \n\n  \n    0.65 \n  \n  \n    0.48 \n  \n  \n    0.30 \n  \n  \n    1.00 \n  \n  \n    0.96 \n  \n  \n    0.74 \n  \n  \n    0.43 \n  \n  \n    0.57 \n  \n  \n    0.57 \n  \n  \n    0.74 \n  \n\n\n\n\n\n\n(c) Centrality at step 3 \n\n  \n    0.59 \n  \n  \n    0.47 \n  \n  \n    0.29 \n  \n  \n    1.00 \n  \n  \n    1.00 \n  \n  \n    0.72 \n  \n  \n    0.41 \n  \n  \n    0.55 \n  \n  \n    0.56 \n  \n  \n    0.68 \n  \n\n\n\n\n\n\n\n\n(d) Centrality at step 4 \n\n  \n    A \n    0.62 \n  \n  \n    B \n    0.47 \n  \n  \n    C \n    0.29 \n  \n  \n    D \n    1.00 \n  \n  \n    E \n    0.98 \n  \n  \n    F \n    0.74 \n  \n  \n    G \n    0.43 \n  \n  \n    H \n    0.56 \n  \n  \n    I \n    0.56 \n  \n  \n    J \n    0.71 \n  \n\n\n\n\n\n\n(e) Centrality at step 5 \n\n  \n    0.60 \n  \n  \n    0.47 \n  \n  \n    0.29 \n  \n  \n    1.00 \n  \n  \n    0.99 \n  \n  \n    0.73 \n  \n  \n    0.42 \n  \n  \n    0.55 \n  \n  \n    0.56 \n  \n  \n    0.69 \n  \n\n\n\n\n\n\n(f) Centrality at step 6 \n\n  \n    0.61 \n  \n  \n    0.47 \n  \n  \n    0.29 \n  \n  \n    1.00 \n  \n  \n    0.99 \n  \n  \n    0.74 \n  \n  \n    0.43 \n  \n  \n    0.56 \n  \n  \n    0.56 \n  \n  \n    0.70 \n  \n\n\n\n\n\n\n\n\n(g) Centrality at step 7 \n\n  \n    A \n    0.60 \n  \n  \n    B \n    0.47 \n  \n  \n    C \n    0.29 \n  \n  \n    D \n    1.00 \n  \n  \n    E \n    0.99 \n  \n  \n    F \n    0.73 \n  \n  \n    G \n    0.42 \n  \n  \n    H \n    0.55 \n  \n  \n    I \n    0.56 \n  \n  \n    J \n    0.70 \n  \n\n\n\n\n\n\n(h) Centrality at step 8 \n\n  \n    0.61 \n  \n  \n    0.47 \n  \n  \n    0.29 \n  \n  \n    1.00 \n  \n  \n    0.99 \n  \n  \n    0.73 \n  \n  \n    0.43 \n  \n  \n    0.56 \n  \n  \n    0.56 \n  \n  \n    0.70 \n  \n\n\n\n\n\n\n(i) Centrality at step 9 \n\n  \n    0.61 \n  \n  \n    0.47 \n  \n  \n    0.29 \n  \n  \n    1.00 \n  \n  \n    0.99 \n  \n  \n    0.73 \n  \n  \n    0.42 \n  \n  \n    0.55 \n  \n  \n    0.56 \n  \n  \n    0.70 \n  \n\n\n\n\n\n\nNow, of course, we can keep going, and apply Equation 21.1 again, but this time the centrality “points” each node gets are the \\(C(1)\\) scores recorded in Table 21.2 (a). So in this step, node \\(E\\)’s centrality is simply:\n\\[\nC(2)_E = C(1)_A + C(1)_D + C(1)_F + C(1)_G + C(1)_H + C(1)_J =\n\\]\n\\[\nC(1)_E = 0.5 + 1 + 0.7 + 0.3 + 0.5 + 0.7 = 3.7\n\\]\nOnce again, because we only care about the ranking and not the exact number, we divide each score by the maximum observed at this step. The result are shown in Table 21.2 (b). As we can see, node \\(D\\) is still at the top, but node \\(E\\) has slipped to second place. The reason for that is that \\(E\\) gets less centrality “bang” (the points given by others) for their “buck” (their number of connections). While both \\(D\\) and \\(E\\) are connected to the same number of others, \\(E\\) is connected to less central others than \\(D\\). The same happened to \\(A\\) and \\(B\\) who were tied at step one, but are now separated at step two, with \\(A\\) being more central than \\(B\\).\nOf course, we can keep going and calculate centralities at step three \\(C(3)\\), four \\(C(4)\\) and so on. The results are shown in Table 21.2 (c) through Table 21.2 (i).\nNote one interesting that happens as we iterate through these steps. The rank order of the centrality scores stop changing! While there are some minute differences between Table 21.2 (h) and Table 21.2 (i), the order in which nodes are arranged in terms of who has the top centrality, and the second biggest, and the third biggest, all the way down to the last, is pretty much the same.\nThat means that as we proceed further and further until the ranks begin to “freeze.” So we can decide to stop after sum of the (absolute value) of the differences between the centrality score at some step \\(k\\), expressed as \\(C(k)\\) and those at the previous step \\(C(k-1)\\) are smaller than some criterion (e.g., \\(e = 0.0001\\)). In which case we have arrived at the centrality scores we wanted! These are shown in Table 21.3.\n\n\nTable 21.3: Centrality scores At the final step versus the eigvenctor centralities.\n\n\n\n\n(a) Centrality at final step \n\n  \n    D \n    1.00 \n  \n  \n    E \n    0.99 \n  \n  \n    F \n    0.73 \n  \n  \n    J \n    0.70 \n  \n  \n    A \n    0.61 \n  \n  \n    I \n    0.56 \n  \n  \n    H \n    0.55 \n  \n  \n    B \n    0.47 \n  \n  \n    G \n    0.42 \n  \n  \n    C \n    0.29 \n  \n\n\n\n\n\n\n(b) Eigenvector Centralities \n\n  \n    D \n    -0.473 \n  \n  \n    E \n    -0.468 \n  \n  \n    F \n    -0.347 \n  \n  \n    J \n    -0.330 \n  \n  \n    A \n    -0.286 \n  \n  \n    I \n    -0.267 \n  \n  \n    H \n    -0.262 \n  \n  \n    B \n    -0.220 \n  \n  \n    G \n    -0.201 \n  \n  \n    C \n    -0.136 \n  \n\n\n\n\n\n\nNow it turns out that there is a way to come up with a set of centrality scores that will arrange the nodes in the graph from top to bottom just like we did in Table 21.3 but without going through all the work of iterating through various steps using Equation 21.1. Imagine that there is magical unknown number—let’s call it \\(\\lambda\\)—and a magical vector—let’s call it \\(\\mathbf{b}\\)— containing the scores that we want. The numbers in the vector will arrange the nodes in the graph exactly like in Table 21.3, only if the following equation is satisfied:\n\\[\nA \\mathbf{b} = \\lambda \\mathbf{b}\n\\tag{21.2}\\]\nEquation 21.2 says that the magical vector containing the centrality scores exists, only if there is a set of numbers we can fill out the vector with, so that when multiply the network adjacency matrix times this vector (which as we know from Chapter 16 results in another vector of the same length as \\(\\mathbf{b}\\)) we get the same answer as multiplying the vector times the magical number \\(\\lambda\\).\nIt turns out that there is a way to use mathematical magic to solve Equation 21.2 using methods from linear algebra. In this field, the vector \\(\\mathbf{b}\\) that satisfies equation Equation 21.2 is called an eigenvector of the matrix \\(A\\). In the same way the magical number \\(lambda\\) is called an eigenvalue of the same matrix. The method from linear algebra that allows us to find \\(\\mathbf{b}\\) and \\(\\lambda\\) is called the eigen-decomposition of the adjacency matrix \\(A\\) (I know, all terrible names). We\nWhen eigen-decompose a matrix, the aim is to find to other matrices, \\(\\Lambda\\) and \\(U\\) such that the following matrix multiplication equation is satisfied:\n\\[\nA = U \\Lambda U^T\n\\tag{21.3}\\]\nWhen \\(A\\) is a square adjacency matrix, the matrix \\(U\\) will be of same dimensions as \\(A\\) (same number of rows and columns), and so will be the matrix \\(\\Lambda\\). The matrix \\(U^T\\) is just the transpose of \\(U\\). The main difference between \\(U\\) and \\(\\Lambda\\) is that \\(U\\) is going to be full of numbers (contain non-zero entries on each cell). Each column of \\(U\\) can be considered separately as a vector, and each of them counts as an eigenvector of \\(A\\).\nThe matrix \\(\\Lambda\\), on the other hand, is going to have zeros in each cell except the diagonals, which will contain a series of numbers \\(\\lambda_1, \\lambda_2, \\lambda_3 \\ldots \\lambda_k\\), where \\(k\\) is equal to the number of rows or columns of the original matrix.\n\n\nTable 21.4: Centrality scores At the final step versus the eigvenctor centralities.\n\n\n\n\n(a) Eigenvectors of an adjacency matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n  \n \n\n  \n    A \n    -0.29 \n    -0.32 \n    -0.34 \n    0.07 \n    -0.47 \n    -0.18 \n    0.44 \n    -0.32 \n    -0.37 \n    -0.04 \n  \n  \n    B \n    -0.22 \n    -0.55 \n    -0.23 \n    -0.09 \n    0.17 \n    -0.37 \n    -0.44 \n    0.33 \n    -0.03 \n    0.33 \n  \n  \n    C \n    -0.14 \n    -0.43 \n    0.27 \n    0.12 \n    0.60 \n    -0.11 \n    0.46 \n    -0.09 \n    0.21 \n    -0.28 \n  \n  \n    D \n    -0.47 \n    -0.14 \n    -0.20 \n    -0.26 \n    -0.06 \n    0.46 \n    -0.32 \n    -0.09 \n    0.23 \n    -0.52 \n  \n  \n    E \n    -0.47 \n    0.17 \n    0.04 \n    0.40 \n    -0.27 \n    -0.01 \n    0.19 \n    0.25 \n    0.58 \n    0.29 \n  \n  \n    F \n    -0.35 \n    0.38 \n    -0.29 \n    -0.06 \n    0.37 \n    0.14 \n    0.27 \n    0.48 \n    -0.44 \n    -0.01 \n  \n  \n    G \n    -0.20 \n    0.33 \n    -0.21 \n    0.49 \n    0.30 \n    -0.32 \n    -0.36 \n    -0.48 \n    -0.07 \n    -0.11 \n  \n  \n    H \n    -0.26 \n    0.17 \n    0.54 \n    -0.14 \n    -0.25 \n    -0.52 \n    -0.12 \n    0.23 \n    -0.17 \n    -0.41 \n  \n  \n    I \n    -0.27 \n    0.25 \n    0.04 \n    -0.67 \n    0.15 \n    -0.19 \n    0.13 \n    -0.40 \n    0.18 \n    0.38 \n  \n  \n    J \n    -0.33 \n    -0.15 \n    0.55 \n    0.18 \n    0.03 \n    0.42 \n    -0.17 \n    -0.19 \n    -0.40 \n    0.36 \n  \n\n\n\n\n\n\n\n\n(b) Eigenvalues of an adjacency matrix \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n  \n \n\n  \n    A \n    4.06 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    B \n    0.00 \n    1.62 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    C \n    0.00 \n    0.00 \n    1.17 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    D \n    0.00 \n    0.00 \n    0.00 \n    0.69 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    E \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.34 \n    0.00 \n    0.00 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    F \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -0.43 \n    0.00 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    G \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -1.31 \n    0.00 \n    0.0 \n    0.00 \n  \n  \n    H \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -1.52 \n    0.0 \n    0.00 \n  \n  \n    I \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    -2.1 \n    0.00 \n  \n  \n    J \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.00 \n    0.0 \n    -2.52 \n  \n\n\n\n\n\n\nFinally, the first column of \\(U\\) contains the centrality scores we seek (those that go into the vector \\(\\mathbf{b}\\) and therefore satisfy Equation 21.2) and the number in the first row and first column of the matrix \\(\\Lambda\\) is the eigenvalue that satifies Equation 21.2 (\\(\\lambda\\)). Because of this, the centrality scores \\(\\mathbf{b}\\) obtained via this method are called the eigenvector centralities of the nodes in the network represented by adjacency matrix \\(A\\) (Bonacich 1972).\n\n\n\n\nBonacich, Phillip. 1972. “Factoring and Weighting Approaches to Status Scores and Clique Identification.” Journal of Mathematical Sociology 2 (1): 113–20.\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71."
  },
  {
    "objectID": "lesson-sna-eigenvector.html#references",
    "href": "lesson-sna-eigenvector.html#references",
    "title": "21  Getting Centrality from Others",
    "section": "References",
    "text": "References\n\n\n\n\nBonacich, Phillip. 1972. “Factoring and Weighting Approaches to Status Scores and Clique Identification.” Journal of Mathematical Sociology 2 (1): 113–20.\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71."
  },
  {
    "objectID": "lesson-sna-hubs-and-authorities.html",
    "href": "lesson-sna-hubs-and-authorities.html",
    "title": "22  Hubs and Authorities",
    "section": "",
    "text": "In Chapter 21 we saw a way to think of centrality as being “reflected” in the connections you have with others. Rather than being a property of the person, centrality is a property of the relations that a given person has with central others. We studied that idea in the case of an undirected graph, which gave us a conception of this type of reflective centrality as eigenvector centrality.\nIn this chapter we extend this idea to the case of asymmetric relations represented as a directed graph. The added complexity is that we have to think of two ways people’s centrality is reflected from their connections to others. On the one hand, a person can be central because many people seek their advice or services, or think of them as smart, capable, likable, and so on. That is, people are central when they receive many directed ties from other people. We will call these people authorities (Kleinberg 1999).\nAt the same time, there are people who while not being authorities themselves, serve as a conduit to authorities. These people get their centrality by pointing to authorities, and you get centrality by pointing to them (because that puts you just a few steps away from authorities). Thus a person who direct ties to many other people who are themselves authorities has a certain knowledge of the social structure that is valuable (they know the people who others need to know). We will call these people hubs (Kleinberg 1999).\nAt first pass, the distinction between hubs and authorities thus boils down to this:\n\nAuthorities are people who are pointed to by many other people.\nHubs are people who point to authorities.\n\nNevertheless, as we discussed in Chapter 23, not all people are created the same. A really good authority should be a person who gets lots of incoming nominations, not just from everyone, but from people who are also considered good hubs by other people. And what is a good hub? Well a good hub is a person who points to really good authorities. Thus, your status as an authority or a hub is both reflective and recursive. The reason for this is that the “goodness” of authorities and hubs is defined in terms of one another, just like the snake eating its own tail we found in Chapter 21.\nThus, if \\(\\mathbf{u}\\) is a \\(n \\times 1\\) column vector containing the authority scores of a set of nodes in a graph—a set of numbers ranking nodes from top to bottom in terms of their authoritativeness–and \\(\\mathbf{v}\\) is a column vector of the same length containing the hub scores—a set of numbers ranking nodes from top to bottom in “hubness”—then they \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) should be linked by the following mathematical relationship:\n\\[\n\\mathbf{u} = A^T \\mathbf{v}\n\\tag{22.1}\\]\n\\[\n\\mathbf{v} = A \\mathbf{u}\n\\tag{22.2}\\]\nWhere \\(A\\) is the adjacency matrix. That is, the authority scores should be a function of the authority scores of the incoming ties to a node, and the hub scores should be a function of authority scores of the outgoing ties to a node.\nNote that in Equation 22.1 and Equation 22.2 we face the same chicken and the egg problem we encountered in Chapter 21. Because the authorities scores are defined in terms of the hub scores, we can’t figure those out until we find the hub scores, but because the hub scores are defined in terms of the authority scores we can’t figure those out until we find the authority scores!\nThe solution to our dilemma is similar to the one we used in Chapter 21. We can give each node in the network the same number of “starting” authoritativeness and hubness points for free and then calculate the authority and hub scores by iterating. A node’s authority score is the sum of the hub scores of the nodes that point to it, and a node’s hub score is the sum of the authority scores of the nodes they point to. We rinse, repeat and stop once the rank order of nodes on both scores stops changing.\nHow would this work in a social network? Let us see. Figure 23.1 shows a directed graph representing a set of asymmetric ties between people (this is the same graph from Chapter 23) and Table 23.1 is the corresponding adjacency matrix.\n\n\n\n\n\nFigure 22.1: A directed graph.\n\n\n\n\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    H \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    1 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 22.1:  Adjacency matrix corresponding to a directed graph. \n\n\n\n\n\n\nKleinberg, Jon M. 1999. “Authoritative Sources in a Hyperlinked Environment.” Journal of the ACM (JACM) 46 (5): 604–32."
  },
  {
    "objectID": "lesson-sna-hubs-and-authorities.html#references",
    "href": "lesson-sna-hubs-and-authorities.html#references",
    "title": "22  Hubs and Authorities",
    "section": "References",
    "text": "References\n\n\n\n\nBonacich, Phillip. 1972. “Factoring and Weighting Approaches to Status Scores and Clique Identification.” Journal of Mathematical Sociology 2 (1): 113–20.\n\n\nKleinberg, Jon M. 1999. “Authoritative Sources in a Hyperlinked Environment.” Journal of the ACM (JACM) 46 (5): 604–32."
  },
  {
    "objectID": "lesson-sna-degree-centrality.html#degree-centrality",
    "href": "lesson-sna-degree-centrality.html#degree-centrality",
    "title": "20  Centralities based on Degree",
    "section": "20.1 Degree Centrality",
    "text": "20.1 Degree Centrality\nThe most basic way of defining centrality is simply as a measure of how many alters an ego is connected to. This simply takes a node’s degree as introduced in Chapter 7, and begins to consider this measure as a reflection of importance of the node in the network. The logic is that those with more direct connections to others, compared to those with fewer, hold a more prominent place in the network.\nOnce we have constructed the adjacency matrix for the network (A), then degree centrality is easy to calculate. As Equation 20.1 for a given node i the degree centrality is given by summing the entries of its corresponding row.\n\\[\n  C_i^{DEG} = \\sum_{j= 1}^{n}a_{ij}\n\\tag{20.1}\\]\nEquation 20.1 thus ranks each node in the graph based on the number of other nodes that it is adjacent to. Just like real life, some nodes will be popular (they will be adjacent to lots of other nodes), while others will be unpopular.\nAlthough it might seem a simple task to just add up the number of connections of each node, that is essentially what the below mathematical equation is doing! Mathematical notation plays an important role in expressing network measures in succinct formats.\nFor instance, if we were to use Equation 20.1 to calculate the degree centrality of each node from the symmetric adjacency matrix corresponding to the graph shown in Figure 4.1 then we would end up with the following degree centralities for each node:\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 20.1: Degree centralities of nodes in an undirected graph."
  },
  {
    "objectID": "lesson-sna-degree-centrality.html#indegree-and-outdegree-centrality",
    "href": "lesson-sna-degree-centrality.html#indegree-and-outdegree-centrality",
    "title": "20  Centralities based on Degree",
    "section": "20.2 Indegree and Outdegree Centrality",
    "text": "20.2 Indegree and Outdegree Centrality\nIf we are talking about a directed graph, then there are two types of degree centralities that can be calculated. On the one hand, we may be interested in how central a node is in terms of sociability or expansiveness that is how many other nodes in the graph a given node sends links to. This is called the outdegree centrality of that node, written as \\(C_i^{OUT}\\). As with the undirected case, this is computed by summing across the rows of the asymmetric adjacency matrix corresponding to the directed graph in question, using Equation 20.1:\n\\[\n  C_i^{OUT} = \\sum_ja_{ij}\n\\tag{20.2}\\]\nHowever, in a directed graph, we may also be interested in how popular or sought after by others a given node is. That is, how many other actors send ties to that node. In which case we need to sum across the columns of the asymmetric adjacency matrix, and modify the formula as follows:\n\\[\n  C_j^{IN} = \\sum_ia_{ij}\n\\tag{20.3}\\]\nNote that in this version of the equation, we are summing over j (the columns) not over i (the rows) as given by subscript under the \\(\\sum\\) symbol.\nFor instance, if we were to use equations Equation 20.2 and Equation 20.2 to calculate the outdegree and indegree centrality of each node from the asymmetric adjacency matrix corresponding to the graph shown in Figure 4.2), then we would up with the following centralities for each node:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nOutdegre\n2\n2\n1\n1\n2\n1\n2\n\n\nIndegree\n2\n3\n1\n3\n0\n2\n0\n\n\n\nTable 20.2: Out and Indegree centralities of nodes in a directed graph.\n\n\nJust like the degree centrality for undirected graphs, the outdegree and indegree centralities rank each node in a directed graph. The first, outdegree centrality, ranks each node based on the number of other nodes that they are connected to. This is a kind of popularity based on sociability, or the tendency to seek out the company of others. The second, indegree centrality, ranks each node in the graph based on the number of other nodes that connect to that node. This is a kind of popularity based on on being sought after a kind of status.\n\n20.2.1 Normalized Degree Centrality\nWhen we compute the degree centrality of a node, are counting the number of other nodes that they are connected to. Obviously, the more nodes there are to connect to, the more opportunities there will be to reach a larger number. But what happens if we wanted to compare the degree centrality of nodes in two very different networks?\nFor instance, if your high-school has one thousand people and you have twenty friends, that’s very different from having twenty friends in a high-school of only one hundred people. It seems like the second person, with twenty friends (covering 20% of the population) in a high-school of one-hundred people is definitely more popular than the second person with twenty friends (covering 2% of the population), in a high school with one thousand people.\nThat’s why Freeman Freeman (1979) proposed normalizing the degree centrality of each node by the maximum possible it can take in a given network. As you may have guessed, the maximum degree in a network is \\(N-1\\) the order of the graph minus one. Essentialy, everyone but you!\nWe can compute the normalized degree centrality using the following equation:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{C_{i}^{DEG}}{N-1}\n\\tag{20.4}\\]\nWhere we just divide the regular degree centrality computed using Equation 20.1 by the order of the graph minus one. This will be equal to \\(1.0\\) if a person knows everyone and \\(0\\) is a person knows no one. For all the other nodes it will be a number between zero and one.\nMoreover, this measure is sensitive to the order of the graph. Thus, for a person with twenty friends in a high-school of a thousand people, the normalized degree centrality is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{1000-1}= 0.02\n\\tag{20.5}\\]\nBut for the person with the same twenty friends in a high-school of one-hundred people, it is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{100-1}= 0.20\n\\tag{20.6}\\]\nIndicating that a person with the same number of friends in the smaller place is indeed more central!"
  },
  {
    "objectID": "lesson-sna-degree-centrality.html#references",
    "href": "lesson-sna-degree-centrality.html#references",
    "title": "20  Centralities based on Degree",
    "section": "References",
    "text": "References\n\n\n\n\nFreeman, Linton C. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39."
  },
  {
    "objectID": "lesson-sna-closeness.html#closeness-centrality",
    "href": "lesson-sna-closeness.html#closeness-centrality",
    "title": "21  Centralities based on the Geodesic Distance",
    "section": "21.1 Closeness Centrality",
    "text": "21.1 Closeness Centrality\nSometimes it not important how many people you directly connected to. Instead, what is important is that you are indirectly connected to a lot of others. As we saw in the lesson on indirect connectivity, the best way to conceptualize indirect connectivity in social networks is via the idea of shortest paths. So if you can reach the most other people in the network via shortest paths with only a few hops, then you are better connected that someone who has to use longer paths to reach the same other people.\n\n\n\n\n\nFigure 21.1: An undirected graph showing the node with the maximum closeness centrality (in red).\n\n\n\n\nThis insight serves as an inspiration for a measure of centrality based on closeness. The closeness between two nodes is the inverse of the geodesic distance them (Bavelas 1950). Recall from Chapter 9 that the geodesic distance is given by the length of the shortest path linking two nodes in the graph. The smallest the length of the shortest path separating two nodes in the graph, the closer the two nodes and vice versa.\nRemember that for any number \\(n\\), the mathematical operation of taking the inverse simply means dividing one by that number. So, the inverse of \\(n\\) is \\(\\frac{1}{n}\\). This means that if \\(d_{ij}\\) is the geodesic distance between nodes i and j in graph \\(G\\), then the closeness between two nodes is \\(\\frac{1}{d_{ij}}\\).\nThe information on the pairwise geodesic distances between every pair of nodes in a given graph is captured in the geodesic distance matrix. For instance, take the graph shown in Figure 21.1. The distance matrix for this graph is shown in Table 21.1.\n\n\n\n\n\n\n\n\nE\nA\nM\nL\nB\nJ\nG\nN\nF\nH\nD\nC\nK\nI\n\n\n\n\nE\n0\n1\n2\n1\n2\n1\n2\n1\n2\n1\n3\n2\n2\n2\n\n\nA\n1\n0\n3\n1\n3\n2\n2\n2\n3\n2\n2\n1\n1\n2\n\n\nM\n2\n3\n0\n3\n1\n2\n2\n1\n2\n2\n1\n3\n2\n4\n\n\nL\n1\n1\n3\n0\n3\n2\n1\n2\n2\n2\n2\n1\n2\n1\n\n\nB\n2\n3\n1\n3\n0\n1\n2\n2\n1\n2\n2\n2\n3\n4\n\n\nJ\n1\n2\n2\n2\n1\n0\n2\n1\n1\n2\n2\n1\n3\n3\n\n\nG\n2\n2\n2\n1\n2\n2\n0\n3\n1\n2\n1\n2\n2\n2\n\n\nN\n1\n2\n1\n2\n2\n1\n3\n0\n2\n1\n2\n2\n2\n3\n\n\nF\n2\n3\n2\n2\n1\n1\n1\n2\n0\n1\n1\n2\n2\n3\n\n\nH\n1\n2\n2\n2\n2\n2\n2\n1\n1\n0\n2\n3\n1\n3\n\n\nD\n3\n2\n1\n2\n2\n2\n1\n2\n1\n2\n0\n3\n1\n3\n\n\nC\n2\n1\n3\n1\n2\n1\n2\n2\n2\n3\n3\n0\n2\n2\n\n\nK\n2\n1\n2\n2\n3\n3\n2\n2\n2\n1\n1\n2\n0\n3\n\n\nI\n2\n2\n4\n1\n4\n3\n2\n3\n3\n3\n3\n2\n3\n0\n\n\n\nTable 21.1: Geodesic distance matrix for an undirected graph.\n\n\nAs shown in Table 21.1, a node like I, who seems to be at the outskirts of the network, also shows up as having the largest geodesic distances from other nodes in the graph. Other nodes, like E, G, and L seem to be “closer” to others, in terms of having to traverse smaller geodesic distances to reach them.\nThat means that we can use the distance table to come up with a measure of centrality called closeness centrality for each node. We can do that by adding up the entries corresponding to each row in the distance matrix (\\(\\sum_j d_{ij}\\)), to get a summary the total pairwise distances separating the node corresponding to row i in the matrix from the other nodes listed in each column j.\nNote that because closeness is better than “farness,” we would want the node with highest closeness centrality to be the one with the smallest sum of pairwise distances. This can be calculated using the following equation:\n\\[\n  C_i^{CLOS} = \\frac{1}{\\sum_jd_{ij}}\n\\tag{21.1}\\]\nIn Equation 21.1, the denominator is the sum across each column j, for each row i in Table 21.1 which corresponds to the distance between node i and each of the other nodes in the graph j (skipping the diagonal cell when \\(i=j\\), because the geodesic distance of node to itself is always zero!).\nAs noted, we take the mathematical inverse of this quantity, dividing one by the sum of the distances, so that way, the smallest number comes out on top and the bigger number comes out on the bottom (since, as we said, we want to measure closeness not “farness.”)\nLet’s see how this work for the graph in Figure 21.1. First, we get the row sums of geodesic distances from Table 21.1. These are shown in the first column of Table 21.2, under the heading “Sum of Distances.” This seems to work; node \\(E\\) has the smallest number here (\\(\\sum_j d_{Ej} = 22\\)) suggesting it can reach the most nodes via the shortest paths. Node \\(I\\) has the largest number (\\(\\sum_j d_{Ij} = 35\\)) indicating it is the most isolated from the other nodes.\n\n\n\n\n\n \n  \n      \n    Sum of Distances (d) \n    Inverse (1/d) \n    Normalized (N-1/d) \n  \n \n\n  \n    E \n    22 \n    0.045 \n    0.59 \n  \n  \n    A \n    25 \n    0.040 \n    0.52 \n  \n  \n    M \n    28 \n    0.036 \n    0.46 \n  \n  \n    L \n    23 \n    0.043 \n    0.57 \n  \n  \n    B \n    28 \n    0.036 \n    0.46 \n  \n  \n    J \n    23 \n    0.043 \n    0.57 \n  \n  \n    G \n    24 \n    0.042 \n    0.54 \n  \n  \n    N \n    24 \n    0.042 \n    0.54 \n  \n  \n    F \n    23 \n    0.043 \n    0.57 \n  \n  \n    H \n    24 \n    0.042 \n    0.54 \n  \n  \n    D \n    25 \n    0.040 \n    0.52 \n  \n  \n    C \n    26 \n    0.038 \n    0.50 \n  \n  \n    K \n    26 \n    0.038 \n    0.50 \n  \n  \n    I \n    35 \n    0.029 \n    0.37 \n  \n\n\n\nTable 21.2:  Sum of geodesic distances for each node in an undirected graph and its inverse. \n\n\nBut we want closeness, not farness, so the second column of Table 21.2 shows what happens when we divide one by the number in the second column. Now, node \\(E\\) has the largest score \\(CC^{CLOS}_E = 0.045\\) which is what we want.\nHowever, because we are dividing one by a relatively large number, we end up with a bunch of small decimal numbers as centrality scores, and like it happened with degree, this number is sensitive to how big the network is (the larger the network, the more likely there is to be really long short paths). So Freeman (1979) proposes a normalized version of closeness that takes into account network size. It is a variation of Equation 21.1:\n\\[\n  C_i^{CLOS} = \\frac{N-1}{\\sum_jd_{ij}}\n\\tag{21.2}\\]\nEquation 21.2 is the same as Equation 21.1, except that instead of dividing one by the sum of distances, we divide \\(N-1\\) by the sum of distances, where \\(N\\) is the order of the graph (the number of nodes). In this case, \\(N=14\\).\nNormalizing the sum of distances shown in the second column of Table 21.2 according to Equation 21.2, gives us the centrality scores shown in the fourth column of the table, under the heading “Normalized.” These scores range from zero to one, with one being the maximum possible closeness centrality score for that graph.\nThe normalized closeness centrality scores listed in the fourth column of Table 21.2 agree with our informal impressions. Node I comes out at the bottom (\\(CC_I^{CLOS} = 0.37\\)), showing it to be the one with the least closeness centrality, given the relatively large geodesic distances separating it from the other nodes in the graph. Node E (marked red in Figure 21.1) comes out on top (\\(CC_E^{CLOS} = 0.59\\)), given its relative geodesic proximity to other nodes in the graph.\nAs we will see later, having closeness centrality information for nodes in a graph can be useful. For instance, if Figure 21.1 was a social network, and we wanted to spread an innovation or a new product among the actors in the fastest amount of time, we would want to give it to node E first. Note however that if something bad (like a disease) was spreading across the network, then it would also be very bad if actor E got it first!1"
  },
  {
    "objectID": "lesson-sna-closeness.html#houston-we-have-a-problem",
    "href": "lesson-sna-closeness.html#houston-we-have-a-problem",
    "title": "21  Centralities based on the Geodesic Distance",
    "section": "21.2 Houston, We Have a Problem",
    "text": "21.2 Houston, We Have a Problem\nSo far, so good. Closeness seems to be a great measure of node importance, giving us a sense of who can reach most others in a network in the most efficient way. However, what would happen if we tried to compute closeness centrality for a disconnected graph like the one shown in Figure Figure 11.1 (b)? Well, the shortest paths distance matrix for that graph looks like the one in Table 21.3.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\nInf\nInf\nInf\nInf\n\n\nB\n1\n0\n1\n1\n2\nInf\nInf\nInf\nInf\n\n\nC\n1\n1\n0\n1\n1\nInf\nInf\nInf\nInf\n\n\nD\n1\n1\n1\n0\n1\nInf\nInf\nInf\nInf\n\n\nE\n1\n2\n1\n1\n0\nInf\nInf\nInf\nInf\n\n\nF\nInf\nInf\nInf\nInf\nInf\n0\n1\n1\n1\n\n\nG\nInf\nInf\nInf\nInf\nInf\n1\n0\n1\n1\n\n\nH\nInf\nInf\nInf\nInf\nInf\n1\n1\n0\n1\n\n\nI\nInf\nInf\nInf\nInf\nInf\n1\n1\n1\n0\n\n\n\nTable 21.3: Geodesic distance matrix for an undirected, disconnected graph.\n\n\nNote that in Table 21.3, pairs of nodes that cannot reach one another in the disconnected graph, get a geodesic distance of “Inf” (infinity) in the respective cell of the geodesic distance matrix. This is a problem because when we compute the row sums of the geodesic distance matrix to try to calculate centrality according to Equation 21.1, we get the “numbers” shown in Table 21.4.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n\nTable 21.4: Row sums of a geodesic distance matrix from a disconnected graph.\n\n\nSo that’s a bummer since all the “numbers” in Table 21.4, are just infinity. Not to get too philosophical, but the problem is that when you add any number to “infinity,” the answer is, well, infinity.2 This means that closeness centrality is only defined for connected graphs. When it comes to disconnected graphs, we are out of luck.\nThankfully, there is a solution develoed by Beauchamp (1965). It consists of a modification of Equation 21.1 called harmonic closeness centrality. The formula goes as follows:\n\\[\n  C_i^{HARM} = \\frac{1}{N-1}\\sum_j\\frac{1}{d_{ij}}\n\\tag{21.3}\\]\nNow, this might seem like we just re-arranged the stuff in Equation 21.2, and indeed that’s what we did! But the re-arrangement matters a lot, because it changes the order in which we do the various arithmetic operations (Boldi and Vigna 2014).\nSo, in English, while Equation 21.2 says “first sum the geodesic distances for each node (to get the denominator), and then divide \\(N-1\\) by this sum,” Equation 21.3 says “first divide one by the geodesic distance, and then sum the result of all these divisions, and then multiply this sum by one over \\(N-1\\).\nOnce again, the philosophy of mathematical infinity kicks in here, since the main difference is that one divided by infinity is actually a real number: zero.3\nSo let’s check by taking every entry in Table 21.3 and dividing one by the number in each cell (except for the diagonals, which we don’t care about). The results are shown in Table 21.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1.0\n1\n1\n1.0\n0\n0\n0\n0\n\n\nB\n1\n0.0\n1\n1\n0.5\n0\n0\n0\n0\n\n\nC\n1\n1.0\n0\n1\n1.0\n0\n0\n0\n0\n\n\nD\n1\n1.0\n1\n0\n1.0\n0\n0\n0\n0\n\n\nE\n1\n0.5\n1\n1\n0.0\n0\n0\n0\n0\n\n\nF\n0\n0.0\n0\n0\n0.0\n0\n1\n1\n1\n\n\nG\n0\n0.0\n0\n0\n0.0\n1\n0\n1\n1\n\n\nH\n0\n0.0\n0\n0\n0.0\n1\n1\n0\n1\n\n\nI\n0\n0.0\n0\n0\n0.0\n1\n1\n1\n0\n\n\n\nTable 21.5: Reciprocal of the geodesic distance matrix for an undirected, disconnected graph.\n\n\nBeautiful! Now, instead of weird “Inf”s we have zeroes, which is great because we can add stuff to zero and get a real number back. We can then apply Equation 21.3 to the numbers in Table 21.5 (e.g., computing the sum of each row and then multiplying that by \\(\\frac{1}{N-1}\\)) to get the harmonic closeness centrality for each node in Figure 11.1 (b). These are shown in Table 21.6.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.5\n0.44\n0.5\n0.5\n0.44\n0.38\n0.38\n0.38\n0.38\n\n\n\nTable 21.6: Harmonic Closeness Centrality scores for nodes in a disconnected, undirected graph.\n\n\nGreat! Now we have a measure of closeness centrality we can apply to all kinds of graphs, whether they are connected or disconnected."
  },
  {
    "objectID": "lesson-sna-closeness.html#references",
    "href": "lesson-sna-closeness.html#references",
    "title": "21  Centralities based on the Geodesic Distance",
    "section": "References",
    "text": "References\n\n\n\n\nBavelas, Alex. 1950. “Communication Patterns in Task-Oriented Groups.” The Journal of the Acoustical Society of America 22 (6): 725–30.\n\n\nBeauchamp, Murray A. 1965. “An Improved Index of Centrality.” Behavioral Science 10 (2): 161–63.\n\n\nBoldi, Paolo, and Sebastiano Vigna. 2014. “Axioms for Centrality.” Internet Mathematics 10 (3-4): 222–62.\n\n\nFreeman, Linton C. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39."
  },
  {
    "objectID": "lesson-sna-betweenness.html#betweenness-centrality",
    "href": "lesson-sna-betweenness.html#betweenness-centrality",
    "title": "22  Centralities based on Shortest Paths",
    "section": "22.2 Betweenness Centrality",
    "text": "22.2 Betweenness Centrality\nWhile Shimbel’s Stress Centrality idea goes a long way to capturing the idea of who the most intermediary nodes are, it can be improved upon. This is what was noted by Anthonisse and later Linton Freeman in a landmark series of papers, where they developed a centrality metric aimed to measure node intermediation called betweenness centrality Freeman (1980)\nThe basic observation behind betweenness centrality is pretty simple. It rests on the idea that rather than counting the raw number of shortest paths a particular node sits on as an inner node, it is better to look at the percentage (or proportion) of such paths.\nLet us return to our earlier example featuring nodes \\(K\\) and \\(J\\) as the end nodes. To measure the proportion of shortest paths featuring nodes in the inner node set \\(\\{A, C, D, E, F, H, N\\}\\) standing between \\(K\\) and \\(J\\), we simplyt take \\(n_{K(x)J}\\) and divide it by the total number of shortests paths between \\(K\\) and \\(J\\), whih we already noted was six. Let us call this new quantity \\(p_{K(x)J}\\) which can be read as “the proportion of paths between K and J featuring node \\(x\\) as an inner node.” This is shown in the third column of Table 22.1.\nLet’s say we wanted to calculate \\(p_{K(x)J}\\) for node \\(H\\), written \\(p_{K(H)J}\\). We can write this in equation form like this:\n\\[\n  p_{K(H)J} = \\frac{n_{K(H)J}}{n_{KJ}} = \\frac{3}{6} = 0.5\n\\tag{22.2}\\]\nJust as before, in Equation 22.2, \\(n_{K(H)J}\\) is the number of shortest paths linking K and J featuring H as an inner node, and \\(n_{KJ}\\) is the total number of shortest paths linking K and J.\nFreeman (1980) calls the measure in Equation 22.2 the pair-dependency of actor K on actor H to reach a given node J. In this case, \\(n_{K(H)J} = 3\\) and \\(n_{KJ} = 6\\), which means that actor K depends on actor H for fifty percent of their shortest path access to J. Making H the actor in the network J depends on the most to be able to reach J. The third column of Table 22.1 shows the \\(p_{K(x)J}\\) scores for all the nodes in the inner node set \\(\\{A, C, D, E, F, H, N\\}\\) standing between \\(K\\) and \\(J\\).\nGeneralizing this approach, we can do the same for each triplet of actors i, j, and k in Figure 22.1 This is the basis for calculating betweenness centrality. That is, we can count the number of times k stands on the shortest path between two other actors i and j, we can then divide it by the total number of shortest paths linking actors i and j in the network.\nThis ratio, written \\(\\frac{n_{i(k)j}}{n_{ij}}\\) then gives us the proportion of shortest paths in the network that have i and j as the end nodes and that feature k as an intermediary inner node, written \\(p_{i(k)j}\\). This can range from zero (no shortest paths between i and j feature node k as an intermediary) to one (all the shortest paths between i and j feature node k as an intermediary).\nWe can then use the following equation to compute the average of this proportion for each node k across each pair of actors in the network i and j:\n\\[\n  C_k^{BET} = \\sum_{i \\neq k} \\sum_{j \\neq k} p_{i(k)j}\n\\tag{22.3}\\]\nComputing this quantity for the graph shown in Figure 22.1, yields the betweenness centrality scores shown in Table 22.3.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nE\nA\nM\nL\nB\nJ\nG\nN\nF\nH\nD\nC\nK\nI\n\n\n\n\n11.4\n5\n2.8\n16.4\n1.5\n10.8\n6.3\n5.3\n9.3\n4.8\n6.8\n3\n3.7\n0\n\n\n\nTable 22.3: Betweenness centrality scores.\n\n\nThe numbers in the Table can be readily interpreted as percentages (or probabilities). Thus, the fact that node J has a a betweenness centrality score of 10.8 tells us that they stand in about 11% of the shortest paths between pairs of nodes in the graph. Alternatively, it says that if pick a shortest path betweenn any two pair of random nodes \\(i\\) and \\(j\\) there’s an 11% chance that \\(J\\) will be one of the inner nodes in that path.\nInterestingly, as shown in Figure 22.1, the node that ends up with the highest betweenness score is L (\\(C_L^{BET} = 16.4\\)), which is different from the result we got using the stress centralities scores in Table 22.2. This is mostly due to the fact that node I, who has the lowest possible betweenness score of zero, depends on this node for access to every other actor in the network.\nNote also that two different nodes end up being ranked first on closeness—as we saw on ?sec-closeness—and betweenness centrality in the same network (compare the red nodes in Figure 21.1 and Figure 22.1). This tells us that closeness and betweenness are analytically distinct measures of node position. One (closeness) gets at reachability, and the other (betweenness) gets at intermediation potential."
  },
  {
    "objectID": "lesson-sna-betweenness.html#references",
    "href": "lesson-sna-betweenness.html#references",
    "title": "22  Centralities based on Shortest Paths",
    "section": "References",
    "text": "References\n\n\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41.\n\n\n———. 1980. “The Gatekeeper, Pair-Dependency and Structural Centrality.” Quality and Quantity 14 (4): 585–92.\n\n\nGould, Roger V, and Roberto M Fernandez. 1989. “Structures of Mediation: A Formal Approach to Brokerage in Transaction Networks.” Sociological Methodology 19: 89–126.\n\n\nMarsden, Peter V. 1983. “Restricted Access in Networks and Models of Power.” American Journal of Sociology 88 (4): 686–717.\n\n\nShimbel, Alfonso. 1953. “Structural Parameters of Communication Networks.” The Bulletin of Mathematical Biophysics 15: 501–7."
  },
  {
    "objectID": "lesson-sna-bigthree.html#the-big-three-centrality-metrics",
    "href": "lesson-sna-bigthree.html#the-big-three-centrality-metrics",
    "title": "23  The “Big Three” Centrality Metrics",
    "section": "23.1 The “big three” centrality metrics",
    "text": "23.1 The “big three” centrality metrics\nLinton Freeman (1979) defined the “big three” classic centrality metrics, roughly corresponding to the extent that a node accumulates one of the three network goods mentioned above. - So the degree centrality metric deal with nodes that have more edges directly incident upon them (Nieminen 1974). - The closeness centrality metric has to do with nodes that can reach more nodes via smallest shortest paths and thus accumulate as many of these paths in which they figure as the origin node as possible (Sabidussi 1966). - Finally, the betweenness centrality metric has to do with a node’s accumulation of the largest share of shortest paths in which they intermediate between two other nodes, and thus featuring them as one of the inner nodes in the paths between others (Freeman 1977).\nOther centrality metrics can be seen as generalizations or special cases of any of these three basic notions (Borgatti 2005).\nImportantly, Freeman showed that the big three centrality metrics reach their theoretical maximum for the central node in a star graph, such as the one shown in Figure 23.1).\n\n\n\n\n\nFigure 23.1: A star graph\n\n\n\n\nA star graph is a graph containing a central or inner node (in Figure 23.1, node A), who is connected to all the other nodes in the graph, called the satellite or outer nodes (in Figure 23.1, nodes B through F). These nodes in contrast have only one connection and that is to the central node, none among themselves.\nBecause of these restrictions, it is easy to see that if \\(G = (V, E)\\) is a star graph of order \\(n\\), then we know that that graph size \\(m = |E|\\) (the size of the edge set), has to be \\(n-1\\). So in the example shown in Figure 23.1, \\(n =7\\) and \\(m = n-1 = 7-1=6\\). Neat!"
  },
  {
    "objectID": "lesson-sna-bigthree.html#the-big-three-centralities-in-the-star-graph",
    "href": "lesson-sna-bigthree.html#the-big-three-centralities-in-the-star-graph",
    "title": "23  The “Big Three” Centrality Metrics",
    "section": "23.2 The Big Three Centralities in the Star Graph",
    "text": "23.2 The Big Three Centralities in the Star Graph\nDegree, Closeness, and Betweenness centralities have an interesting property that provides a conceptual connection between them (Freeman 1979). Consider the star graph shown in Figure 23.1 with central node A. The degree, closeness, and betweenness centralities of the different nodes are shown in Table 23.1).\nOf course, by definition, we know beforehand that the central node in a star graph has to have the highest degree, since the degree of peripheral nodes is fixed to one and the degree of the central node is always \\(n-1\\), where \\(n\\) is the graph order.\nHowever, note also that the central node has to have the highest closeness, since it is connected by a path of length one (an edge) to every peripheral node, but each peripheral node can only reach other peripheral nodes in the graph by a path of length two. They are farther away from other nodes than the central node.\nFinally, note that the central node in the star will also always have the highest betweenness because each of the paths of length two connecting every pair of peripheral nodes to one another has to include the central node as an inner node in the path. So it serves as the intermediary between any communication between peripheral nodes.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nDegree\n6.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nCloseness\n8.2\n4.5\n4.5\n4.5\n4.5\n4.5\n4.5\n\n\nBetwenness\n15.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\nTable 23.1: Centralities in a star graph of order 7.\n\n\nThe mathematical sociologist Linton Freeman (1979) thus thinks that the “big three” centrality measures are the big three precisely because they are maximized for the central node in a star graph."
  },
  {
    "objectID": "lesson-sna-bigthree.html#references",
    "href": "lesson-sna-bigthree.html#references",
    "title": "23  The “Big Three” Centrality Metrics",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nBorgatti, Stephen P, and Martin G Everett. 2006. “A Graph-Theoretic Perspective on Centrality.” Social Networks 28 (4): 466–84.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41.\n\n\n———. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39.\n\n\nNieminen, Juhani. 1974. “On the Centrality in a Graph.” Scandinavian Journal of Psychology 15 (1): 332–36.\n\n\nSabidussi, Gert. 1966. “The Centrality Index of a Graph.” Psychometrika 31 (4): 581–603."
  },
  {
    "objectID": "lesson-sna-betweenness.html#stress-centrality",
    "href": "lesson-sna-betweenness.html#stress-centrality",
    "title": "22  Centralities based on Shortest Paths",
    "section": "22.1 Stress Centrality",
    "text": "22.1 Stress Centrality\nFor instance, let’s say you were actor K in the network shown in Figure 22.1, and you wanted to know who is the person that you depend on the most to communicate with actor J. Here dependence means that you are forced to “go through them.” One way K could figure this out is by listing every shortest path having them as the origin node and having J as the destination node.\nAfter you have this list, you can see which of other other nodes shows up as an inner node—an intermediary or gatekeeper—in those paths the most times. Remember that two actors can be indirectly linked by multiple shortest paths of the same length, and that we can figure out how many short paths links pairs of actors in the network using the shortest paths matrix.\n\n\n\n\n\nFigure 22.1: An undirected graph showing the node with the maximum betweenness centrality (in red)\n\n\n\n\nThis shortest path list would look like this:\n\n\\(\\{KH, HF, FJ\\}\\)\n\\(\\{KD, DF, FJ\\}\\)\n\\(\\{KH, HN, NJ\\}\\)\n\\(\\{KA, AC, CJ\\}\\)\n\\(\\{KA, AE, EJ\\}\\)\n\\(\\{KH, HE, EJ\\}\\)\n\nThere are six shortest paths (in this case of length \\(l = 3\\)) indirectly connecting actors K and J in Figure 22.1, with nodes \\(\\{A, C, D, E, F, H, N\\}\\) showing up as an inner node in at least one of those paths. To see which other actor in the network is the most frequent intermediary between K and J, we can create a list with the number of times each of these nodes shows up as an inner node in the shortest path list shown earlier.\nWe will call each of these numbers \\(n_{K(x)J}\\), where the little \\(x\\) in parentheses indicates the inner node we are talking about standing between \\(K\\) and \\(J\\). For instance, \\(n_{K(A)J}\\) refers to the number of times node \\(A\\) stands on a shortest path linking \\(K\\) and \\(J\\), and so forth for each of the nodes in the inner node set \\(\\{A, C, D, E, F, H, N\\}\\).\nThe table containing this information would look like that shown in Table 22.1.\n\n\n\n\n\n\n\nNode\nFreq.\nProp.\n\n\n\n\nA\n2\n0.33\n\n\nC\n1\n0.17\n\n\nD\n1\n0.17\n\n\nE\n2\n0.33\n\n\nF\n2\n0.33\n\n\nH\n3\n0.50\n\n\nN\n1\n0.17\n\n\n\nTable 22.1: Intermediaries between nodes J and K\n\n\nSo it looks like, looking at the second column of Table 22.1, that H is the other actor that J depends on the most to reach K, since they show up three times in the paths linking K and J—meaning that \\(n_{K(H)J} = 3\\)—beating out all the other nodes.\nWe can use this information—aggregated across all pairs of nodes in the graph for every other node—to construct a centrality metric. That is, for each node in the graph \\(k\\) we can ask how often they show up as an intermediary between every other pair of nodes in the graph \\(i\\) and \\(j\\) and then we sum up this quantity across all pairs. This centrality index was first proposed by Alfonso Shimbel (1953) and it is called the stress centrality.\nIn equation form you write the stress centrality index like this:\n\\[\nC^{STRESS}_k = \\sum_{i \\neq k} \\sum_{j \\neq k} n_{i(k)j}\n\\tag{22.1}\\]\nWhich says that the stress centrality of node \\(k\\) is the sum of the number of times they stand in a shortest path between each pair of nodes in the graph, excluding the pairs that include \\(k\\) (recall that \\(i \\neq k\\) means “\\(i\\) does not equal \\(k\\)” in English).\n\n\n\n\n\n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n  \n \n\n  \n    22 \n    6 \n    16 \n    26 \n    48 \n    40 \n    22 \n    28 \n    0 \n    48 \n    20 \n    50 \n    12 \n    22 \n  \n\n\n\nTable 22.2:  Stress centrality scores. \n\n\nTable 22.2 shows the results of computing the stress centralities for all the nodes in Figure 22.1. It is clear that according to this metric, nodes \\(E\\) and \\(J\\) win out, standing in the middle of a whopping \\(n_{i(E)j} = n_{i(J)j} = 48\\) shortest paths between other nodes in Figure 22.1. Node \\(I\\) receives a stress centrality score of \\(n_{i(I)j} = 0\\) because they are not an inner node between any other pair of nodes."
  }
]