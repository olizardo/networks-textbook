[
  {
    "objectID": "lesson-matrix-multiplication.html#sec-multrules",
    "href": "lesson-matrix-multiplication.html#sec-multrules",
    "title": "16  Matrix Multiplication",
    "section": "16.1 Matrix Multiplication Rules",
    "text": "16.1 Matrix Multiplication Rules\nFirst, we will let out the basic rules of matrix multiplication:\n\nYou can always multiply two matrices as long as the number of columns of the first matrix equal the rows of the second matrix. To check whether this is the case, all you have to do is put the two matrices side by side and list their dimensions.\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6}\n\\] - The two little “fives” in bold are called the inner dimensions of the two matrices. The little “three” on the left and the little “six” on the right are called the outer dimensions. So another way of stating the first rule of matrix multiplication is that the product of two matrices is defined as long as their inner dimensions equal to one another when you line them up from left to right.\n\nWhen the number of columns of a matrix equal the number of rows of another matrix so that their inner dimensions match we say that the the two matrices are conformable. When this is not the case, we say the matrices are non-conformable.\nThus, another way of stating the first rule is that only the product of conformable matrices is defined. If the matrices are not conformable then their product is not defined (e.g., there is no answer to the question of what we get if we multiply them!).\nThis means that unlike numbers or the matrix dot product, where the order of the two things you are multiplying doesn’t matter (\\(4 \\times 3 = 3 \\times 4\\) or \\(\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{B} \\cdot \\mathbf{A}\\)), in matrix multiplication it does matter. Alas, for any two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\),\n\n\\[\n\\mathbf{A} \\times \\mathbf{B} \\neq \\mathbf{B} \\times \\mathbf{A}\n\\]\n\nWhen you multiply a matrix times another matrix, the resulting matrix will have number of rows equal to the number of rows of the first matrix and number of columns equal to the number of columns of the second matrix. Thus:\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6} = \\mathbf{C}_{3 \\times 6}\n\\tag{16.1}\\]\n\nEquation 16.1 says that the product of a three by five matrix \\(\\mathbf{A}\\) (three rows and five columns) times a five by six matrix \\(\\mathbf{B}\\) (five rows and six columns) is a third matrix \\(\\mathbf{C}\\) with three rows and six columns. Another way of saying this last rule is that the product of two conformable matrices will have dimensions equal to their outer dimensions.\n\n\n16.1.1 Multiplying a Matrix Times its Transpose\n\nBy definition, as discussed in Section 16.1.1, the rows of a matrix are equal to the columns of its transpose, and vice versa. The product of a matrix times its transpose and the transpose times the original matrix is always defined, no matter what the dimensions of the original matrix are. Thus,\n\n\\[\n\\mathbf{A} \\times \\mathbf{A}^T = defined!\n\\]\n\\[\n\\mathbf{A}^T \\times \\mathbf{A} = defined!\n\\]\n\nWhen you multiply a matrix times its transpose, the resulting matrix will be a square matrix with number of rows and columns equal to the number of rows of the original matrix. For instance, say matrix \\(\\mathbf{A}_{5 \\times 3}\\) is of dimensions \\(5 \\times 3\\) (like the matrix shown in Table 16.1 (a)). Then its transpose \\(A^T_{3 \\times 5}\\) will be of dimensions \\(3 \\times 5\\) (like the matrix shown in Table 16.1 (b)). That means the product of the matrix times its transpose will be:\n\n\\[\n\\mathbf{A}_{5 \\times 3} \\times \\mathbf{A}_{3 \\times 5}^T = \\mathbf{B}_{5 \\times 5}\n\\tag{16.2}\\]\n\nEquation 16.2 says that a five by three matrix multiplied by its transposed yields a square matrix \\(\\mathbf{B}\\) of dimensions five by five (a square matrix with five rows and five columns). In the same way,\n\n\\[\n\\mathbf{A}_{3 \\times 5}^T \\times \\mathbf{A}_{5 \\times 3} = \\mathbf{B}_{3 \\times 3}\n\\tag{16.3}\\]\n\nEquation 16.3 says that the transpose of a five by three matrix multiplied by the original yields a product matrix \\(\\mathbf{B}\\) of dimensions three by three (a square matrix with three rows and three columns).\n\n\n\n16.1.2 Matrix Powers\n\nYou can multiply a matrix times itself to get matrix powers but only if matrix is a square matrix (has the same number of rows and columns). Thus,\n\n\\[\n\\mathbf{A}^2 = \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^3 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^4 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^n = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\ldots\n\\]\n\nFor all square matrices \\(\\mathbf{A}\\) of any dimension. Since matrices used to represent social networks, like the adjacency matrix are square matrices, that means that you can always find the powers of an adjacency matrix.\nWhen you multiply a square matrix times another square matrix of the same dimensions, the resulting matrix is of the same dimensions as the original two matrices. Thus,\n\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = \\mathbf{A}^2_{5 \\times 5}\n\\]\n\n\nTable 16.1: A matrix and its transpose\n\n\n\n\n(a) Original Matrix. \n\n  \n    3 \n    4 \n    5 \n  \n  \n    7 \n    9 \n    3 \n  \n  \n    4 \n    6 \n    2 \n  \n  \n    5 \n    3 \n    4 \n  \n  \n    2 \n    5 \n    4 \n  \n\n\n\n\n\n\n(b) Transposed Matrix. \n\n  \n    3 \n    7 \n    4 \n    5 \n    2 \n  \n  \n    4 \n    9 \n    6 \n    3 \n    5 \n  \n  \n    5 \n    3 \n    2 \n    4 \n    4"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#sec-matmultex",
    "href": "lesson-matrix-multiplication.html#sec-matmultex",
    "title": "16  Matrix Multiplication",
    "section": "16.2 Matrix Multiplication Examples",
    "text": "16.2 Matrix Multiplication Examples\nNow let’s see some examples of how matrix multiplication works. Table 16.2 shows the result of multiplying the matrix shown in Table 16.1 (a) times its transpose, shown in Table 16.1 (b).\n\n\n\n\n\n\n  \n    50 \n    72 \n    46 \n    47 \n    46 \n  \n  \n    72 \n    139 \n    88 \n    74 \n    71 \n  \n  \n    46 \n    88 \n    56 \n    46 \n    46 \n  \n  \n    47 \n    74 \n    46 \n    50 \n    41 \n  \n  \n    46 \n    71 \n    46 \n    41 \n    45 \n  \n\n\n\nTable 16.2:  Matrix resulting from multiplying a matrix times its transpose \n\n\nNow where the heck did these numbers come from? Don’t panic. We’ll break it down. First, let’s begin with the number \\(50\\) in cell corresponding to the first row and first column of Table 16.2. To find out where this number came from, let’s look at the first row of Table 16.1 (a), composed of the vector \\(\\{3, 4, 5\\}\\), and the first-column of Table 16.1 (b), composed of the same vector \\(\\{3, 4, 5\\}\\). Now, the number \\(50\\) comes from the fact that we multiply each of the corresponding entries of the two vectors, and then add them up, as follows:\n\\[\n(3 \\times 3) + (4 \\times 4) + (5 \\times 5) = 9 + 16 + 25 = 50\n\\]\nNeat! Now let’s see where the number \\(74\\) in the fourth row and second column of Table 16.2 came from. For that we look at the entries in the fourth row of Table 16.1 (a), composed of the vector \\(\\{5, 3, 4\\}\\) and the second column of Table 16.1 (b) composed of the vector \\(\\{7, 9, 3\\}\\). Like before, we take the first number of the first vector and multiply it by the first number of the second vector, the second number of the first vector and multiply it by the second number of the second vector, and the third number of the first vector and multiply it by the third number of the second vector and add up the results:\n\\[\n(5 \\times 7) + (3 \\times 9) + (4 \\times 3) = 35 + 27 + 12 = 74\n\\] And we keep on going like this to get each of the twenty five numbers in Table 16.2 (there are twenty five numbers because Table 16.2 has five rows and five columns and five times five equal twenty five). In general terms, the number in the \\(i^{th}\\) row and \\(j^{th}\\) column of Table 16.2 is equal to the sum of the products of the numbers in the \\(i^{th}\\) row of the Table 16.1 (a) and the \\(j^{th}\\) column of Table 16.1 (b).\nNote that the resulting product matrix shown in Table 16.2 is symmetric. The same numbers that appear in the upper-triangle also appear in the lower triangle, such that \\(b_{ij} = b_{ji}\\). So once you know the numbers in one of the triangles, you can fill up the numbers in the other one without having to do all the multiplying and adding up!\nNow, let’s multiply the matrix in Table 16.1 (b) times the matrix in Table 16.1 (a). As the rules of matrix multiplication show, this will result in a matrix of dimensions \\(3 \\times 3\\) because Table 16.1 (b) has three rows and Table 16.1 (a) has three columns. This is shown in Table 16.3.\n\n\n\n\n\n\n  \n    103 \n    124 \n    72 \n  \n  \n    124 \n    167 \n    91 \n  \n  \n    72 \n    91 \n    70 \n  \n\n\n\nTable 16.3:  Matrix resulting from multiplying a matrix times its transpose \n\n\nLike before, if we want to figure out where the number \\(72\\) in the third row and first column of Table 16.3 came from, we go to the first row of Table 16.1 (b) composed of the vector \\(\\{5, 3, 2, 4, 4\\}\\) and the first column of Table 16.1 (a), composed of the vector \\(\\{3, 7, 4, 5, 2\\}\\) match up each number in terms of order, multiplying them and add up the result:\n\\[\n(5 \\times 3) + (3 \\times 7) + (2 \\times 4) + (4 \\times 5) + (4 \\times 2) =\n\\]\n\\[\n15 + 21 + 8 + 20 + 8 = 72\n\\]\n\n\nTable 16.4: Powers of a matrix.\n\n\n\n\n(a) A matrix. \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) Matrix squared. \n\n  \n    1 \n    1 \n    2 \n    0 \n  \n  \n    1 \n    1 \n    2 \n    1 \n  \n  \n    2 \n    1 \n    2 \n    2 \n  \n  \n    1 \n    1 \n    1 \n    2 \n  \n\n\n\n\n\n\n(c) Matrix cubed. \n\n  \n    2 \n    2 \n    3 \n    3 \n  \n  \n    3 \n    2 \n    4 \n    3 \n  \n  \n    4 \n    3 \n    5 \n    4 \n  \n  \n    3 \n    2 \n    4 \n    2 \n  \n\n\n\n\n\n\nMatrix powers work the same as regular matrix multiplication, except that we are working on just one matrix not two. So for instance, the number \\(2\\) in the first row and third column of Table 16.4 (b) comes from the numbers in the first row of Table 16.4 (a) (\\(\\{0, 1, 0, 1\\}\\)) and the numbers in the third column of Table 16.4 (a) (\\(\\{0, 1, 1, 1\\}\\)). We line them up, multiplying them, and add them:\n\\[\n(0 \\times 1) + (1 \\times 1) + (0 \\times 1) + (1 \\times 1) = 0 + 1 + 0 + 1 = 2\n\\] Since we are working with a binary matrix, the product of each of the cell entries will be either a zero (when at least one of the entries is zero) or a one (when both entries are one).\nTo get the cubed entries in Table 16.4 (c), we just take Table 16.4 (b) as the first matrix and Table 16.4 (a) as the second matrix, and do matrix multiplication magic. Thus, to get the number \\(4\\) in the third row and fourth column of Table 16.4 (c), we take the numbers in the third row of Table 16.4 (b) \\(\\{2, 1, 2, 2\\}\\) and the numbers in the fourth column of Table 16.4 (a) \\(\\{1, 0, 1, 0\\}\\), line them up, multiply them, and add them:\n\\[\n(2 \\times 1) + (1 \\times 0) + (2 \\times 1) + (1 \\times 0) = 2 + 0 + 2 + 0 = 4\n\\]\nPretty easy!"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#matrix-multiplication-of-vectors",
    "href": "lesson-matrix-multiplication.html#matrix-multiplication-of-vectors",
    "title": "16  Matrix Multiplication",
    "section": "16.3 Matrix Multiplication of Vectors",
    "text": "16.3 Matrix Multiplication of Vectors\nRecall from Section 8.1 that a vector is a sequence of numbers of a given length. So for instance, the vector \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\) is a vector of length five.\nWell, and here comes the big reveal, it turns out that another way to think of a vector, is as a special case of matrix. That is, a matrix with one row, and as many columns as the length of the vector! This is a called a row vector. So the row vector \\(\\mathbf{a}\\) vector can be thought of as a matrix of dimensions \\(1 \\times 5\\) (one row and five columns) or \\(\\mathbf{a}_{1 \\times 5}\\).\nIn matrix form:\n\n\n\n\n\n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\nTable 16.5:  Matrix resulting from multiplying a matrix times its transpose \n\n\nSince vectors are matrices, we can perform the same type of matrix operations on them as we did with matrices. For instance, we can compute the transpose of a vector. In the case of \\(\\mathbf{a}\\), the transpose \\(\\mathbf{a}^T\\) is:\n\n\n\n\n\n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\nTable 16.6:  Matrix resulting from multiplying a matrix times its transpose \n\n\nThe transpose of a row vector is called (you may have guessed) a column vector. The column vector in Table 16.6 is a matrix with five rows and one column.\nThis also means that the same rules of matrix multiplication apply. For instance, we can always multiply a row vector times a column vector, because it is the equivalent of multypling a matrix times its transpose, and we have already seen in Section 16.1.1, that this can always be done:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{a}^T_{5 \\times 1} = b_{1 \\times 1}\n\\tag{16.4}\\]\nEquation 16.5 says that the product of the \\(1 \\times 5\\) row vector \\(\\mathbf{a}\\) times a \\(5 \\times 1\\) column vector \\(\\mathbf{a}^T\\) is a \\(1 \\times 1\\) “matrix” otherwise known as a scalar (that is, a regular old number). We’ve already seen examples of this, because in regular matrix multiplication, each cell of the product matrix is a scalar obtained from multiplying the corresponding terms taken from a row of the first matrix (which is a row vector) times those of the column of the second matrix (which is a column vector).\nSo in this case this would be:\n\\[\n(2 \\times 2) + (4 \\times 4) + (7 \\times 7) + (2 \\times 2) + (4 \\times 4) =\n\\]\n\\[\n4 + 16 + 49 + 4 + 16 = 89\n\\]\n\nThe first rule of vector matrix multiplication is that you can always multiply a row vector times a column vector (even when their entries are not the same) as long as they are the same length (e.g., the number of columns of the row vector equal the number of rows of the column vector).\nThe second rule of vector matrix multiplication is that when you multiply a row vector times another a column vector the result is always scalar (a single number).\n\nNow notice that if we change the order, and multiply the transpose of a vector times the original? This should be allowed because it conforms to the rules that we have already discussed:\n\\[\n\\mathbf{a}^T_{5 \\times 1} \\times \\mathbf{a}_{1 \\times 5} = B_{5 \\times 5}\n\\tag{16.5}\\]\nThis matrix multiplication is defined because the inner dimensions of the two matrices (the column and row vectors) are the same (one). But note that, according to the rules of matrix multiplication, when you multiply the transpose of a vector times the original, the result is a square matrix, with dimensions \\(n \\times n\\) where \\(n\\) is the length of the original row vector (the number of columns). In our example if the original vector is \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\), then \\(\\mathbf{a}^T \\times \\mathbf{a}\\) is equal to the matrix shown in Table 16.7.\n\n\n\n\n\n\n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n  \n    14 \n    28 \n    49 \n    14 \n    28 \n  \n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n\n\n\nTable 16.7:  Matrix resulting from multiplying a matrix times its transpose \n\n\n\nSo, the third and final rule of vector matrix multiplication is that when you multiply a column vector times a row vector of the same length, the result is a square matrix of row and column dimensions equal to the length of the original vectors."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "href": "lesson-matrix-multiplication.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "title": "16  Matrix Multiplication",
    "section": "16.4 Multiplying a Vector Times A Matrix (and Vice Versa)",
    "text": "16.4 Multiplying a Vector Times A Matrix (and Vice Versa)\nSince vectors are just matrices, it means that we can always multiply a vector times a matrix (and a matrix times a vector), as long as we follow the matrix multiplication rules laid out in Section 16.1.\n\n16.4.1 Row Vector Times Matrix\nFor instance, take the row vector \\(\\mathbf{b} = \\{4, 9, 3, 5\\}\\) and the binary matrix \\(\\mathbf{A}\\) shown in Table 16.4 (a). Because the row vector \\(\\mathbf{b}\\) is of dimensions \\(1 \\times 4\\) and matrix \\(\\mathbf{A}\\) is of dimensions \\(4 \\times 4\\), it is possible to multiply the vector times the matrix as follows:\n\\[\n\\mathbf{b}_{1 \\times 4} \\times \\mathbf{A}_{4 \\times 4} = \\mathbf{c}_{1 \\times 4}\n\\tag{16.6}\\]\nEquation 16.6 says that the product of a \\(1 \\times 4\\) row vector times a \\(4 \\times 4\\) square matrix is another vector of dimensions equal to the original row vector. The result for this example is shown in Table 16.8.\n\n\nTable 16.8: Row vector resulting from multiplying a row vector times a square matrix\n\n\n\n\n(a) 1 X 4 row vector \n\n  \n    4 \n    9 \n    3 \n    5 \n  \n\n\n\n\n\n\n\n\n(b) 4 X 4 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 4 product vector \n\n  \n    8 \n    13 \n    17 \n    7 \n  \n\n\n\n\n\n\nOf course, it is also possible to multiply a row vector times a rectangular matrix (where the number of rows is not necessarily equal to the number of columns), as long as the number of rows of the rectangular matrix equals the length of the original row vector. For instance, take a row vector \\(\\mathbf{a}_{1 \\times 5}\\) shown in Table 16.9 (a) and a matrix \\(B_{5 \\times 3}\\). Its product would be given by:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{B}_{5 \\times 3} = \\mathbf{c}_{1 \\times 3}\n\\tag{16.7}\\]\nEquation 16.7 says that the product of a row vector of dimensions \\(1 \\times 5\\) and a matrix of dimensions \\(5 \\times 3\\) is another row vector \\(\\mathbf{c}\\) of dimensions (\\(1 \\times 3\\)). A numerical example corresponding to this situation is shown in Table 16.9.\n\n\nTable 16.9: Row vector resulting from multiplying row vector times a rectangular matrix\n\n\n\n\n(a) 1 X 5 row vector \n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\n\n\n\n\n\n(b) 5 x 3 matrix \n\n  \n    17 \n    14 \n    18 \n  \n  \n    17 \n    14 \n    11 \n  \n  \n    2 \n    4 \n    0 \n  \n  \n    15 \n    1 \n    9 \n  \n  \n    6 \n    13 \n    6 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 3 product vector \n\n  \n    170 \n    166 \n    122 \n  \n\n\n\n\n\n\nTo get the “179” entry in row one and column one of Table 16.9, we take the entries of the row vector shown in Table 16.9 (a) and multiply them by the corresponding entries in the first column of the matrix shown in Table 16.9 (b) and add up the results:\n\\[\n(2 \\times 2) + (4 \\times 5) + (7 \\times 11) + (2 \\times 15) + (4 \\times 12) =\n\\]\n\\[\n4 + 20 + 77 + 30 + 48 = 179\n\\]\nAnd so on for the other two entries in Table 16.9 (c). So the main rule of multiplying a row vector times a matrix with number of rows equal to the length of the row vector is that the result will always be another row vector of length equal to the number of columns of the matrix."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#multiplying-a-matrix-times-column-vector",
    "href": "lesson-matrix-multiplication.html#multiplying-a-matrix-times-column-vector",
    "title": "16  Matrix Multiplication",
    "section": "16.5 Multiplying a Matrix Times Column Vector",
    "text": "16.5 Multiplying a Matrix Times Column Vector\nIn the same way, we can always multiply a matrix times a column vector, as long as the the number of columns of the matrix is equal to the length of the column vector. For instance, take the binary square matrix \\(A_{5 \\times 5}\\) shown in Table 16.10 (a) and the column vector \\(\\mathbf{b}_{5 \\times 1}\\) shown in Table 16.9 (b). Their product \\(\\mathbf{c}\\) would be given by:\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{b}_{5 \\times 1} = \\mathbf{c}_{5 \\times 1}\n\\tag{16.8}\\]\nEquation 16.8 says that the product of a matrix of dimensions \\(5 \\times 5\\) and a column vector of dimensions \\(5 \\times 1\\) is another column vector \\(\\mathbf{c}\\) of dimensions equal to the original column vector (\\(5 \\times 1\\)). A numerical example of this situation is shown in Table 16.10.\n\n\nTable 16.10: Column vector resulting from multiplying a square matrix times a column vector\n\n\n\n\n(a) 5 x 5 square matrix \n\n  \n    2 \n    11 \n    14 \n    5 \n    15 \n  \n  \n    14 \n    19 \n    20 \n    6 \n    6 \n  \n  \n    11 \n    19 \n    13 \n    12 \n    15 \n  \n  \n    1 \n    9 \n    14 \n    12 \n    0 \n  \n  \n    19 \n    14 \n    1 \n    16 \n    18 \n  \n\n\n\n\n\n\n(b) 5 x 1 column vector \n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\n\n\n\n(c) 5 X 1 product vector \n\n  \n    216 \n  \n  \n    280 \n  \n  \n    273 \n  \n  \n    160 \n  \n  \n    205"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#multiplying-matrices-times-the-all-ones-vector",
    "href": "lesson-matrix-multiplication.html#multiplying-matrices-times-the-all-ones-vector",
    "title": "16  Matrix Multiplication",
    "section": "16.6 Multiplying Matrices Times the All Ones Vector",
    "text": "16.6 Multiplying Matrices Times the All Ones Vector\nIn matrix multiplication, there is a special row and column vector called the all ones vector. As you may have guessed this is a vector of all ones, of some length \\(n\\). For instance and all ones row vector of length five is \\(\\mathbf{1}_{1 \\times 5} = \\{1, 1, 1, 1, 1\\}\\) (the symbol for the all ones vector is a boldface “1”). We can also get the transpose of this all ones row vector to get the all ones column vector \\(\\mathbf{1}^T\\).\nWhy do we care about vectors full of ones? Well, it turns out that the all one row and column vectors have a neat property when we multiplied by matrices. We already know, from the rules of vector matrix multiplication reviewed earlier, that the product of a row vector times a square matrix is always a row vector of the same length as the original, and the product of a square matrix times a column vector is always a column vector of the same length as the original.\nLet’s say we a matrix \\(\\mathbf{A}\\) of dimensions \\(5 \\times 5\\), and we multiplied the all ones row vector of length five times this matrix, which would result in the row vector \\(\\mathbf{b}\\). This would be given by the formula:\n\\[\n\\mathbf{1}_{1 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = b_{1 \\times 5}\n\\tag{16.9}\\]\nA numerical example of the situation depicted in Equation 16.9 is shown in Table 16.11.\n\n\nTable 16.11: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 1 X 5 all ones row vector \n\n  \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 5 product row vector \n\n  \n    3 \n    5 \n    2 \n    3 \n    1 \n  \n\n\n\n\n\n\nIf you look at the resulting row vector in Table 16.11 (c), we can see that the result of multiplying the all ones row vector times a matrix is a vector that contains the column sums of the matrix entries! So the “2” in position 1 of Table 16.11 (c) comes from adding up the numbers in the first column of the matrix, the “1” in position 2 of Table 16.11 (c) comes from adding the numbers in the second column and so forth.\n\n\nTable 16.12: Row vector resulting from multiplying the all ones row vector times a square binary matrix\n\n\n\n\n(a) 5 X 5 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    1 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) 5 X 1 all ones column vector \n\n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n  \n    1 \n  \n\n\n\n\n\n\n(c) 5 X 1 product column vector \n\n  \n    2 \n  \n  \n    3 \n  \n  \n    4 \n  \n  \n    2 \n  \n  \n    3 \n  \n\n\n\n\n\n\nIn the same way, if we multiply the same matrix times the all ones column vector, we get the results shown in Table 16.12. We can see that the result of multiplying a matrix times the all ones column vector, is another column vector contains the row sums of the original matrix! So, the “2” in the first position of the column vector comes from adding the numbers in the first row of the matrix, the “3” comes from adding the numbers in the second row, and so forth."
  },
  {
    "objectID": "lesson-matrix-multiplication.html#the-identity-matrix",
    "href": "lesson-matrix-multiplication.html#the-identity-matrix",
    "title": "16  Matrix Multiplication",
    "section": "16.7 The Identity Matrix",
    "text": "16.7 The Identity Matrix\nThe last “interesting” matrix we will cover is called the identity matrix. This is a square matrix, usually written using the symbol \\(\\mathbf{I}\\) of dimensions \\(n \\times n\\). This matrix will have “1” in every diagonal cell, and “0” in every off-diagonal cell. For instance, an identity matrix of dimensions \\(5 \\times 5\\) is shown Table 16.13.\n\n\n\n\n\n\n  \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n\n\n\nTable 16.13:  A 5 X 5 Identity Matrix. \n\n\nThe interesting thing about this matrix is that when you multiply it times another square matrix of the same dimensions, the result is always the original matrix! So it plays the role that the number “1” plays in regular number multiplication, in matrix algebra. This means that, for any square matrix \\(\\mathbf{A}\\):\n\\[\n\\mathbf{A} \\times \\mathbf{I} = \\mathbf{A}\n\\tag{16.10}\\]\nAnd also,\n\\[\n\\mathbf{I} \\times \\mathbf{A} = \\mathbf{A}\n\\tag{16.11}\\] Neat!"
  },
  {
    "objectID": "lesson-matrix-multiplication.html#references",
    "href": "lesson-matrix-multiplication.html#references",
    "title": "16  Matrix Multiplication",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Social Networks",
    "section": "",
    "text": "Welcome\nThis is an introductory textbook on social networks to be used in conjunction with an undergraduate lecture class on social networks (SOCIOL 111) taught in the department of sociology at UCLA."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#bipartite-graphs",
    "href": "lesson-nets-affiliation-networks.html#bipartite-graphs",
    "title": "22  Affiliation Networks",
    "section": "22.1 Bipartite Graphs",
    "text": "22.1 Bipartite Graphs\nA bipartite graph is useful to represent a network where, rather than ties occurring between nodes of the same kind (e.g., people connected with other people), ties occur only between nodes of different kinds but never between nodes of the same kind. Typically, the two different types of nodes are located at different levels of analysis or aggregation. As such, bipartite graphs are perfect for capturing the sociological concept of affiliation or membership with larger groups or events (Breiger 1974). For instance, actors and the movies they make, scientists and the papers they write, or people and the groups they belong to.1\n\n\n\n\n\nFigure 22.1: A bipartite graph. Circles are people and triangles are the corporate boards they belong to\n\n\n\n\nFor example, people work at companies, so we might say that a worker is connected with the company, rather than any specific individual there. People also connect to sports teams, schools, religious communities, and other organizations which can have an influence in structuring their social world.\nIn the graph theoretic sense, a bipartite graph \\(G_B\\) is a graph featuring two sets of nodes \\(V_1\\) and \\(V_2\\) and one set of edges \\(E\\). Thus a bipartite graph, like a signed and a weighted graph, is a set of three sets:\n\\[\n    G_B = (E, V_1, V_2)\n\\tag{22.1}\\]\n\nrepresents a network diagram of a bipartite graph where circles connect to triangles (with the shapes standing as labels for the two set of nodes). In the Figure, \\(V_1 = \\{A, B, C, D, E\\}\\) and \\(V_2 = \\{1, 2, 3, 4, 5\\}\\). The edge set \\(E\\) is \\(\\{A1, A2, B2, B3, C2, C4, D4, D3, E3, E5\\}\\).\n\nOne common example of two-mode networks that be represented using bipartite graphs in sociology are corporate interlock networks (Mizruchi 1983). If 1) represented such a network, we could think of the circles as members of the company’s board, and the triangles are the board from each company. Because the same executive can be a member of more than one company’s board, board member A is on the board of both companies 1 and 2, while board member B is on the board of companies 2 and 3.\nNote that edges in a bipartite graph are symmetrical and thus bipartite graphs are (generally) undirected. This makes sense, since the relationship affiliation or membership is indeed symmetrical by definition. If person A is a member of the board in company 2 then it is understood that company 2 has person A as a board member.\nIn the same way, note that there is no reason why the cardinality of two node sets in a bipartite graph have to be same (although they are in the example provided). In a real world corporate interlock network, for instance, there will generally be more people than companies, so \\(|V_1| > |V_2|\\)."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "href": "lesson-nets-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "title": "22  Affiliation Networks",
    "section": "22.2 Unipartite Projections of Bipartite Graphs",
    "text": "22.2 Unipartite Projections of Bipartite Graphs\nWhile the information we can glean from looking at the original bipartite graph alone may be useful, you might realize that board members A and B both are on the boards of company 2! In fact, board member C is also on the board of company 2! We might thus conclude that board members A, B, and C all know each other from sitting in the same company board.\n\n\n\n\n\nFigure 22.2: A unipartite graph. People are linked if they serve in the same company board\n\n\n\n\nIf this sort of information was important, we could convert the bipartite into a simple unipartite graph capturing connections between the same level of analysis. This is called a projection of the original bipartite graph. In the projected graph, two board members are joined by a symmetric tie if they both serve on the board of at least one company together.\n\n\n\n\n\nFigure 22.3: Another unipartite graph. Boards are linked if they share members.\n\n\n\n\nThus, we could, as shown in Figure 22.2, create a graph that shows board members who know each other because they work at the same company. The resulting (simple, undirected) graph shows that board members A, B, and C all know each other as a result of serving in the board of company 2 together.\nLikewise, we can transform the bipartite graph into a simple unipartite graph that captures companies that share board members. Company 2 is thus connected to Companies 1 (because of person A), 3 (because of person B), and 4 (because of person 5). This is shown in Figure 22.3). In fact, the reason why these are called interlock networks, is because it is easy to see that, ultimately, by virtue of sharing members across boards, most big corporations in the U.S. (and other countries), end up forming part of a single giant network."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "href": "lesson-nets-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "title": "22  Affiliation Networks",
    "section": "22.3 From Biparite Graph to Affiliation Matrix",
    "text": "22.3 From Biparite Graph to Affiliation Matrix\nConsider the two-mode network shown in Figure 22.4. This is an affiliation network meant to represent the memberships of six students in five college activity clubs. As discussed earlier, we use a bipartite graph to represent the network. The bipartite graph represents the two sets of nodes using different shapes or colors (blue and red nodes in Figure 22.4), and draws a link between the people and the group if the person is affiliated with the group.\n\n\n\n\n\nFigure 22.4: Bipartite graph of a two-mode network of students and clubs.\n\n\n\n\nHow can we translate the graph representation into a matrix?\nThe procedure is the same as that used to build the adjacency matrix of the symmetric graph. We build a rectangular matrix whose number of rows is the same as the number of people in the affiliation network, and whose number of rows is the same as the number of groups. The matrix is rectangular (as opposed to square) because in a two-mode network, there is no restriction that the size of the two vertex sets be the same (although if they happen to be the same then you end up with a square matrix; after all, a square is a special case of a rectangle!).\nIn graph theory terms, this is a matrix that we call A, for affiliation matrix of dimensions \\(R \\times C\\), where the number of rows \\(R = |V_1|\\) is the cardinality of the first vertex set in the bipartite graph (persons in Figure 22.4), and where the number of rows \\(C = |V_2|\\) is the cardinality of the second vertex set (clubs in Figure 22.4)). The cells of the affiliation matrix, \\(a_{ij} = 1\\) if person i belongs to club j (there’s an symmetric edge in the graph linking the person to the group), otherwise, \\(a_{ij} = 0\\).\nFollowing these instructions would yield the affiliation matrix shown in Table 22.1.\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nGabriela\n1\n1\n1\n1\n0\n\n\nParker\n1\n0\n1\n0\n0\n\n\nBrandon\n0\n1\n1\n1\n0\n\n\nMarie\n0\n0\n1\n0\n1\n\n\nRahul\n0\n1\n0\n1\n0\n\n\nMinjoo\n0\n0\n0\n0\n1\n\n\n\nTable 22.1: Affiliation matrix of a bipartite graph.\n\n\nThe affiliation matrix has some interesting properties. For instance, just like the adjacency matrix, it can be used to compute node degree centrality for each set of nodes. But since we have two different sets of nodes, we end up with two different sets of centrality scores; one set of centrality scores for the people and another set for the groups (Faust 1997).\nLet us see how this works."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#group-and-person-centralities",
    "href": "lesson-nets-affiliation-networks.html#group-and-person-centralities",
    "title": "22  Affiliation Networks",
    "section": "22.4 Group and Person Centralities",
    "text": "22.4 Group and Person Centralities\n\n22.4.1 Person Centralities\nIf we wanted to figure out the degree centrality of the people node set (abbreviated P) in the affiliation matrix, we would sum cell entries across the rows, according to the now familiar equation:\n\\[\n    C_P^{DEG} = \\sum_j a_{ij}\n\\tag{22.2}\\]\nWhich leads to the following vector of degree centrality scores for the people:\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\n4\n2\n3\n2\n2\n1\n\n\n\nTable 22.2: Degree centrality scores for the people.\n\n\nThe degree centrality scores for the people can be interpreted as giving us a sense of their joining activity (e.g., high versus low). Some people, (like Gabriela) join a lot of clubs; they have multiple interests spread out across many organizations. Other people, (like Minjoo), just have a single interest, and thus join only one club (the Cheese Club). If centrality is defined using the “more/more principle” discussed in lesson on centrality, then we would say that Gabriela is more central than Minjoo in the affiliation network.\n\n\n22.4.2 Group Centralities\nIn the same way, if wanted to compute the degree centralities of other mode (the club node set, abbreviated as G), then we would calculate the column sums of the affiliation matrix using a slight variation of Equation 22.2), like we did when we switched from outdegree to indegree:\n\\[\n    C_G^{DEG} = \\sum_i a_{ij}\n\\tag{22.3}\\]\nWhich leads to the following degree centrality scores for the clubs:\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\n2\n3\n4\n3\n2\n\n\n\nTable 22.3: Degree centrality scores for the clubs.\n\n\nJust like the people, the centrality scores for the clubs tell us something about the popularity of each group. Some groups are popular (have lots of members), others are less so. So, it seems like in this student group, the Magic Club is definitely the most popular, containing four members. The Cheese and Fashion Clubs on the other hand, seem to be more niche pursuits, with only two members each."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#the-affiliation-matrix-transpose",
    "href": "lesson-nets-affiliation-networks.html#the-affiliation-matrix-transpose",
    "title": "22  Affiliation Networks",
    "section": "22.5 The Affiliation Matrix Transpose",
    "text": "22.5 The Affiliation Matrix Transpose\nAs discussed in Section 16.1.1, it is possible to “flip” the rows and columns of any matrix, so what was previously the rows become the columns, and what was previously the columns become the rows. This is called the matrix transpose and if the original matrix was called A, then the transpose is called A’.2 If the original matrix A was of dimensions \\(R \\times C\\) then the transpose A’ is of dimensions \\(C \\times R\\).\nThe transpose of the affiliation matrix shown in Table 22.1 is shown in Table 22.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nFashion\n1\n1\n0\n0\n0\n0\n\n\nNerdfighters\n1\n0\n1\n0\n1\n0\n\n\nMagic\n1\n1\n1\n1\n0\n0\n\n\nSuper Smash Brs.\n1\n0\n1\n0\n1\n0\n\n\nCheese\n0\n0\n0\n1\n0\n1\n\n\n\nTable 22.4: Transpose of the affiliation Matrix.\n\n\nNote that the transpose of the affiliation matrix contains exactly the same information as the original affiliation matrix. The group affiliations of every person are preserved as are memberships of each group. If we used equations Equation 22.2, and Equation 22.3) to compute the person and group centralities using the affiliation matrix transpose A’, we would get the same results, except that the first equation (summing across the rows) would now give us the group centralities, and the second equation (summing down the columns) would give use the people centralities!\nWe learn from matrix algebra that an important property of rectangular matrices is that you can always multiply a rectangular matrix by its transpose (see Section 16.1). Recall a key condition of matrix multiplication is that the two matrices be conformable so that the columns of the first matrix need to match the number of rows of the second matrix. Well, it’s clear than since any matrix that is of dimensions \\(R \\times C\\), will have a transpose of dimensions \\(C \\times A\\) then the multiplication of the two matrices will be defined:\n\\[\n    A^{}_{R \\times C} \\times A^{'}_{C \\times R} = defined!\n\\tag{22.4}\\]\nIn the same way, the transpose of a matrix can always be multipled by the original matrix:\n\\[\n    A^{'}_{C \\times R} \\times A^{}_{R \\times C} = defined!\n\\tag{22.5}\\]"
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "href": "lesson-nets-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "title": "22  Affiliation Networks",
    "section": "22.6 The Person and Group Overlap Matrices",
    "text": "22.6 The Person and Group Overlap Matrices\nIf the transpose of the affiliation matrix contains the same information as the original why do we care about it? Well the reason is that we can use the multiplication property described in Section 16.1.1 to extract two new matrices that contain new (or at least not obvious, especially for large two-mode networks), information from the original affiliation matrix. The first is called the person overlap matrix (written \\(O^P\\)), this is defined for an original affiliation matrix, in which people are listed in the rows and groups, events, or project, listed in the columns using the following matrix equation:\n\\[\n    O^P = A^{ }_{R \\times C} \\times A^{'}_{C \\times R}\n\\tag{22.6}\\]\n\n22.6.1 The Person Overlap Matrix\nUsing the rules for matrix multiplication discussed Section 16.1, the person overlap matrix obtained using the affiliation matrix shown in Table 22.1 is shown in Table 22.5.\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nGabriela\n4\n2\n3\n1\n2\n0\n\n\nParker\n2\n2\n1\n1\n0\n0\n\n\nBrandon\n3\n1\n3\n1\n2\n0\n\n\nMarie\n1\n1\n1\n2\n0\n1\n\n\nRahul\n2\n0\n2\n0\n2\n0\n\n\nMinjoo\n0\n0\n0\n1\n0\n1\n\n\n\nTable 22.5: Person Overlap Matrix.\n\n\nThe person overlap matrix transforms the initial rectangular affiliation matrix, which has people in the rows and groups in the columns, to a square matrix, which like the usual relationship matrices we have been dealing with, feature people in both the rows and the columns. Each entry in the person overlap matrix \\(o^P_{ij}\\) now gives us the number of groups in which person i and j mutually belong to (Breiger 1974). So we learn that Gabriela and Brandon have three memberships in common (I bet they seen another a lot!) but that Rahul and Parker have no memberships in common (so they are less likely to encounter one another).\nNote also that, in the person overlap matrix, (in contrast to the usual adjacency matrix), there are valid entries along the diagonal cells (\\(o^P_{ii}\\)). These cells now record the total number of memberships that the node corresponding to that row (or column) has. Which we ascertained by computing the node centralities in the original affiliation matrix using Equation 22.2). You can see that the vector of degree centralities shown in Table 22.2) is the same as the vector formed by the diagonal entries in ?tbl-comem).\n\n\n22.6.2 The Group Overlap Matrix\nIn the same way we can compute the person overlap matrix, it is possible to calculate another matrix, called the group overlap matrix (written \\(O^G\\)), this time by multiplying the transpose of the original affiliation matrix times the original We do that using the following equation:\n\\[\n    O^G = A^{'}_{C \\times R} \\times A^{ }_{R \\times C}\n\\tag{22.7}\\]\nRecall from Chapter 16 that matrix multiplication is not commutative (if \\(A\\) is a rectangular matrix, then \\(A \\times A^{'} \\neq A^{'} \\times A\\))), so Equation 22.7 gives you a different answer than Equation 22.6. The result is shown in Table 22.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nFashion\n2\n1\n2\n1\n0\n\n\nNerdfighters\n1\n3\n2\n3\n0\n\n\nMagic\n2\n2\n4\n2\n1\n\n\nSuper Smash Brs.\n1\n3\n2\n3\n0\n\n\nCheese\n0\n0\n1\n0\n2\n\n\n\nTable 22.6: Group Overlap Matrix.\n\n\nThe group overlap matrix (O), like the person overlap matrix, is also square. But this time it has groups in both the rows and columns. Each cell in the group overlap matrix \\(o^P_{ij}\\) records the number of people groups i and groups j have in common (Breiger 1974). Thus, we learn that the Super Smash Brothers group and the Nerdfighters groups share three members in common but that the Super Smash Brothers and the Cheese group have no members in common (pointing to a disaffinity between these activities).\nNote that both the person and group overlap matrices are symmetric. It is easy to see why this is; if I have three group overlaps with you, then you by definition also have three group overlaps with me; if group A has three members in common with group B, then group B has three members in common with group A. This means that if they were to be taken as representing a network, then the resulting graph would be undirected (but weighted because there can be more or less overlap between people and groups). We will see how to do that below."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "href": "lesson-nets-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "title": "22  Affiliation Networks",
    "section": "22.7 Overlapping Node Neighborhoods in Two-Mode Networks",
    "text": "22.7 Overlapping Node Neighborhoods in Two-Mode Networks\nThe notion of overlap used to construct the person and group overlap matrix is the same as the idea of overlapping node neighborhoods for regular networks, discussed in Chapter 6 Thus, while nodes of the same kind cannot be connected in a two-mode network (by construction), they can share neighbors. In a two-mode network if a node belong to one of the vertex sets, let’s say \\(V_1\\), then all of their neighbors have to belong to the other vertex set (\\(V_2\\)) and vice versa.\nFor instance, in Figure 22.4), Gabriela’s node neighborhood is:\n\\[\n    Gab_{NN} = \\{Fashion, Nerdfighters, Magic, SuperSmashBros\\}\n\\]\nRahul’s node neiborhood is:\n\\[\n    Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nThe intersection between their neighborhoods is:\n\\[\n    Gab_{NN} \\cap Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nSo now we can see that the number “2” recorded in the cell that corresponds to Gabriela and Rahul in the person overlap matrix shown in Table 22.5) is the cardinality of the subset formed by the intersection of their two neighborhoods, which in this case contain two members (the Nerdfighters and Super Smash Brothers clubs). The same procedure can be used to figure out the overlap between the node neighborhoods of groups (which happen to be subsets of people in the larger two-mode network)."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "href": "lesson-nets-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "title": "22  Affiliation Networks",
    "section": "22.8 One Mode Projections of two-mode Networks",
    "text": "22.8 One Mode Projections of two-mode Networks\nNote that both the comembership and group overlap matrices, being square matrix with values that go beyond zero and one in the cells, look like a lot like the adjacency matrix that could be obtained from a weighted graph as discussed in the lesson on types of graphs.\n\n\n\n\n\nFigure 22.5: One mode (persons) projection of the original bipartite graph.\n\n\n\n\nSo using formulas Equation 22.6) and Equation 22.7), it is possible to go from a two-mode network in which no links exist between nodes of the same kind, to a weighted graph, in which the links between nodes of the same kind are defined by the overlap of their neighborhoods in the original bipartite graph. As we noted earlier, this is called the one mode projection of the two-mode network. Each two-mode network thus has two one mode projection one for each node set.\n\n\n\n\n\nFigure 22.6: One mode (groups) projection of the original bipartite graph.\n\n\n\n\nThe one mode projection for the person node set of the bipartite graph show in Figure 22.4) is shown in Figure 22.5), this is an undirected weighted graph with the edge weight between people being set to the number of comemberships between each dyad as recorded in the person overlap matrix shown in Table 22.5). In this respect, the number of comemberships can be seen as a proxy of the tie strength between two people, when we only have information on their affiliations. As the Figure shows, Brandon, Gabriela and Rahul form a tightly connected clique, given the number of memberships they share. Minjoo, who does not share many affiliations with anyone, stands toward the periphery of the person-to-person comembership network.\nThe corresponding one-mode projection for the group node set is shown in Figure 22.6). This weighted graph can be read the same way: The thickness of the ties between groups are proportion to the people they share as recorded in the group overlap matrix shown in Table 22.6), thus speaking to the similarity or strength of connectivity between groups.\nSo we see, as we noted before, that Super Smash Brothers and Nerdfighters are tightly connected, but that the Cheese Club is largely peripheral in the group-to-group network. This peripheral status mirrors the marginal status of Minjoo (one of the few members of the Cheese Club) in the person-to-person network.\nThe fact that peripheral people belong peripheral groups and central people belong to central groups encodes a fundamental principle in the analysis of two-mode networks (Breiger 1974) and that is the duality principle.\nThe duality principle in two-mode network analysis says that the position of people in a two-mode network is defined by the positions the groups they affiliate with occupy, and in the same way, the position of the groups in a two-mode network is defined by the positions of the people that belong to them (Bonacich 1991)."
  },
  {
    "objectID": "lesson-nets-affiliation-networks.html#references",
    "href": "lesson-nets-affiliation-networks.html#references",
    "title": "22  Affiliation Networks",
    "section": "References",
    "text": "References\n\n\n\n\nBonacich, Phillip. 1991. “Simultaneous Group and Individual Centralities.” Social Networks 13 (2): 155–68.\n\n\nBreiger, Ronald L. 1974. “The Duality of Persons and Groups.” Social Forces 53 (2): 181–90.\n\n\nFaust, Katherine. 1997. “Centrality in Affiliation Networks.” Social Networks 19 (2): 157–91.\n\n\nMizruchi, Mark S. 1983. “Who Controls Whom? An Examination of the Relation Between Management and Boards of Directors in Large American Corporations.” Academy of Management Review 8 (3): 426–35."
  }
]