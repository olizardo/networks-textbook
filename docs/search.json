[
  {
    "objectID": "9-1-lesson-matrix-operations.html#matrix-addition",
    "href": "9-1-lesson-matrix-operations.html#matrix-addition",
    "title": "16  Matrix Operations",
    "section": "16.1 Matrix Addition",
    "text": "16.1 Matrix Addition\nPerhaps the simplest operation we can do with matrices is add them up. To add two matrices, we simply add up the corresponding entries in each cell. In matrix notation:\n\\[\n\\mathbf{H} + \\mathbf{C} = h_{ij} + c_{ij}\n\\tag{16.1}\\]\nWhere \\(h_{ij}\\) is the corresponding entry for nodes i and j in the hanging out adjacency matrix \\(\\mathbf{H}\\), and \\(c_{ij}\\) is the same entry in the co-working adjacency matrix \\(\\mathbf{C}\\).\nWhy would we want to do this? Well, if we were studying the network shown in Figure 16.1, we might be interested in which dyads have uniplex (or single-stranded) relations, and which ones have multiplex (or multi-stranded) relations. That is, while some actors in the network either hang out together or work together, some of the do both. Adding up the adjacency matrices shown in Table 16.1, will tell us who these are. The result is shown in Table 16.2.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    1 \n    2 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    B \n    1 \n    -- \n    2 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    C \n    2 \n    2 \n    -- \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    1 \n    1 \n    -- \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    1 \n    -- \n    2 \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    1 \n    2 \n    -- \n    0 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    -- \n    2 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    1 \n    0 \n    0 \n    1 \n    2 \n    2 \n    -- \n    1 \n    0 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    1 \n    -- \n    1 \n    2 \n  \n  \n    K \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    1 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    2 \n    1 \n    -- \n  \n\n\n\nTable 16.2:  Uniplex and Multiplex relationship matrix. \n\n\nTable 16.2 shows that the \\(BC\\) dyad has a multiplex relation (there is a “2” in the corresponding cell entry) and so does the \\(AC\\), \\(FH\\), \\(GH\\), \\(EF\\), and \\(JL\\) dyads."
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#the-matrix-dot-product",
    "href": "9-1-lesson-matrix-operations.html#the-matrix-dot-product",
    "title": "16  Matrix Operations",
    "section": "16.2 The Matrix Dot Product",
    "text": "16.2 The Matrix Dot Product\nAnother way of figuring out which pairs of people in a network have multiplex ties is to compute the matrix dot product (symbol: \\(\\cdot\\)). Just like matrix addition, we find the matrix dot product by multiplying the corresponding entries in each of the matrices. In matrix format:\n\\[\n\\mathbf{H} \\mathbf{\\cdot} \\mathbf{C} = h_{ij} \\times c_{ij}\n\\tag{16.2}\\]\nIf we take the dot product of two adjacency matrices like \\(\\mathbf{H}\\) and \\(\\mathbf{C}\\), then the resulting matrix will have a one in a given cell only if \\(h_{ij} = 1\\) and \\(c_{ij} = 1\\). Otherwise, it will have a zero. This means that the dot product of two adjacency matrices will retain only the multiplex ties and erase all the other ones. The result of the dot products of the adjancency matrices shown in Table 16.1 is shown in Table 16.3.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    -- \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    -- \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    -- \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    -- \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n    1 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    -- \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    -- \n  \n\n\n\nTable 16.3:  Multiplex relationship matrix. \n\n\nAs we can see, the only dyads that have non-zero entries in Table 16.3 are the multiplex dyads in Table 16.2. The resulting network, composed of the combined “hanging + co-working” relation is shown in Figure 16.1 (c). Note that this network is much more sparse than either of the other two, since there’s an edge between nodes only when they are adjacent in both the Figure 16.1 (a) and Figure 16.1 (b) networks."
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#sec-trans",
    "href": "9-1-lesson-matrix-operations.html#sec-trans",
    "title": "16  Matrix Operations",
    "section": "16.3 The Matrix Transpose",
    "text": "16.3 The Matrix Transpose\nOne thing we can do with a matrix is “turn it 90 degrees” so that the rows of the new matrix are equal to the columns of the resulting matrix and the columns of the first matrix equal the rows of the resulting matrix. This is called the matrix transpose (symbol: \\(^T\\)).\nFor instance, if we have a matrix \\(\\mathbf{A}_{4 \\times 5}\\) of dimensions \\(4 \\times 5\\) (four rows and five columns), then the transpose \\(A^T_{5 \\times 4}\\) will have five rows and four columns, with the respective entries in each matrix given by the formula:\n\\[\na_{ij} = a^T_{ji}\n\\] That is the number that in the first matrix appears in the \\(i^{th}\\) row and \\(j^{th}\\) column now appears in the transposed version of the matrix in the \\(j^{th}\\) row and \\(i^{th}\\) column.\nAn example of a matrix and its tranpose is shown in Table 16.4.\n\n\nTable 16.4: A matrix and its transpose\n\n\n\n\n(a) Original Matrix. \n\n  \n    3 \n    4 \n    5 \n  \n  \n    7 \n    9 \n    3 \n  \n  \n    4 \n    6 \n    2 \n  \n  \n    5 \n    3 \n    4 \n  \n  \n    2 \n    5 \n    4 \n  \n\n\n\n\n\n\n(b) Transposed Matrix. \n\n  \n    3 \n    7 \n    4 \n    5 \n    2 \n  \n  \n    4 \n    9 \n    6 \n    3 \n    5 \n  \n  \n    5 \n    3 \n    2 \n    4 \n    4 \n  \n\n\n\n\n\n\nSo let’s check out how the transpose works. The original matrix in Table 16.4 (a) has five rows and three columns. The transposed matrix has three rows and five columns. We can find the same numbers in the original and transposed matrices by switching the rows and columns. Thus, in the original matrix, the number in third row and second column is a six (\\(a_{32} = 6\\)). In the transposed version of the matrix, that same six is in second row and third column (\\(a^T_{23} = 6\\)). If you check, you’ll see that’s the case for each number! Thus, the transposed version of a matrix has the same information as the original, it is just that the rows and columns are switched. While this might seem like a totally useless thing to do (or learn) at the moment, we will see in Chapter 18 that the matrix transpose comes in very handy in the analysis of social networks, and particular in the analysis of two mode networks and cliques."
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#sec-matmult",
    "href": "9-1-lesson-matrix-operations.html#sec-matmult",
    "title": "16  Matrix Operations",
    "section": "16.4 Matrix Multiplication",
    "text": "16.4 Matrix Multiplication\nMatrix multiplication (symbol: \\(\\times\\)) is perhaps the more complex of the matrix algebra operations we will cover. It is a bit involved, but relatively easy once you get the hang of it. We will begin with a simple example before doing more complicated stuff.\n\n16.4.1 Matrix Multiplication Rules\nFirst, we will let out the basic rules of matrix multiplication:\n\nYou can always multiply two matrices as long as the number of columns of the first matrix equal the rows of the second matrix. To check whether this is the case, all you have to do is put the two matrices side by side and list their dimensions.\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6}\n\\] - The two little “fives” in bold are called the inner dimensions of the two matrices. The little “three” on the left and the little “six” on the right are called the outer dimensions. So another way of stating the first rule of matrix multiplication is that the product of two matrices is defined as long as their inner dimensions equal to one another when you line them up from left to right.\n\nWhen the number of columns of a matrix equal the number of rows of another matrix so that their inner dimensions match we say that the the two matrices are conformable. When this is not the case, we say the matrices are non-conformable.\nThus, another way of stating the first rule is that only the product of conformable matrices is defined. If the matrices are not conformable then their product is not defined (e.g., there is no answer to the question of what we get if we multiply them!).\nThis means that unlike numbers or the matrix dot product, where the order of the two things you are multiplying doesn’t matter (\\(4 \\times 3 = 3 \\times 4\\) or \\(\\mathbf{A} \\cdot \\mathbf{B} = \\mathbf{B} \\cdot \\mathbf{A}\\)), in matrix multiplication it does matter. Alas, for any two matrices \\(\\mathbf{A}\\) and \\(\\mathbf{B}\\),\n\n\\[\n\\mathbf{A} \\times \\mathbf{B} \\neq \\mathbf{B} \\times \\mathbf{A}\n\\]\n\nWhen you multiply a matrix times another matrix, the resulting matrix will have number of rows equal to the number of rows of the first matrix and number of columns equal to the number of columns of the second matrix. Thus:\n\n\\[\n\\mathbf{A}_{3 \\times \\mathbf{5}} \\times \\mathbf{B}_{\\mathbf{5} \\times 6} = \\mathbf{C}_{3 \\times 6}\n\\tag{16.3}\\]\n\nEquation 16.3 says that the product of a three by five matrix \\(\\mathbf{A}\\) (three rows and five columns) times a five by six matrix \\(\\mathbf{B}\\) (five rows and six columns) is a third matrix \\(\\mathbf{C}\\) with three rows and six columns. Another way of saying this last rule is that the product of two conformable matrices will have dimensions equal to their outer dimensions.\n\n\n\n16.4.2 Multiplying a Matrix Times its Transpose\n\nBy definition, as discussed in Section 16.4.2, the rows of a matrix are equal to the columns of its transpose, and vice versa. The product of a matrix times its transpose and the transpose times the original matrix is always defined, no matter what the dimensions of the original matrix are. Thus,\n\n\\[\n\\mathbf{A} \\times \\mathbf{A}^T = defined!\n\\]\n\\[\n\\mathbf{A}^T \\times \\mathbf{A} = defined!\n\\]\n\nWhen you multiply a matrix times its transpose, the resulting matrix will be a square matrix with number of rows and columns equal to the number of rows of the original matrix. For instance, say matrix \\(\\mathbf{A}_{5 \\times 3}\\) is of dimensions \\(5 \\times 3\\) (like the matrix shown in Table 16.4 (a)). Then its transpose \\(A^T_{3 \\times 5}\\) will be of dimensions \\(3 \\times 5\\) (like the matrix shown in Table 16.4 (b)). That means the product of the matrix times its transpose will be:\n\n\\[\n\\mathbf{A}_{5 \\times 3} \\times \\mathbf{A}_{3 \\times 5}^T = \\mathbf{B}_{5 \\times 5}\n\\tag{16.4}\\]\n\nEquation 16.4 says that a five by three matrix multiplied by its transposed yields a square matrix \\(\\mathbf{B}\\) of dimensions five by five (a square matrix with five rows and five columns). In the same way,\n\n\\[\n\\mathbf{A}_{3 \\times 5}^T \\times \\mathbf{A}_{5 \\times 3} = \\mathbf{B}_{3 \\times 3}\n\\tag{16.5}\\]\n\nEquation 16.5 says that the transpose of a five by three matrix multiplied by the original yields a product matrix \\(\\mathbf{B}\\) of dimensions three by three (a square matrix with three rows and three columns).\n\n\n\n16.4.3 Matrix Powers\n\nYou can multiply a matrix times itself to get matrix powers but only if matrix is a square matrix (has the same number of rows and columns). Thus,\n\n\\[\n\\mathbf{A}^2 = \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^3 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^4 = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A}\n\\] \\[\n\\mathbf{A}^n = \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\times \\mathbf{A} \\ldots\n\\]\n\nFor all square matrices \\(\\mathbf{A}\\) of any dimension. Since matrices used to represent social networks, like the adjacency matrix are square matrices, that means that you can always find the powers of an adjacency matrix.\nWhen you multiply a square matrix times another square matrix of the same dimensions, the resulting matrix is of the same dimensions as the original two matrices. Thus,\n\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = \\mathbf{A}^2_{5 \\times 5}\n\\]"
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#sec-matmultex",
    "href": "9-1-lesson-matrix-operations.html#sec-matmultex",
    "title": "16  Matrix Operations",
    "section": "16.5 Matrix Multiplication Examples",
    "text": "16.5 Matrix Multiplication Examples\nNow let’s see some examples of how matrix multiplication works. Table 16.5 shows the result of multiplying the matrix shown in Table 16.4 (a) times its transpose, shown in Table 16.4 (b).\n\n\n\n\n\n\n  \n    50 \n    72 \n    46 \n    47 \n    46 \n  \n  \n    72 \n    139 \n    88 \n    74 \n    71 \n  \n  \n    46 \n    88 \n    56 \n    46 \n    46 \n  \n  \n    47 \n    74 \n    46 \n    50 \n    41 \n  \n  \n    46 \n    71 \n    46 \n    41 \n    45 \n  \n\n\n\nTable 16.5:  Matrix resulting from multiplying a matrix times its transpose \n\n\nNow where the heck did these numbers come from? Don’t panic. We’ll break it down. First, let’s begin with the number \\(50\\) in cell corresponding to the first row and first column of Table 16.5. To find out where this number came from, let’s look at the first row of Table 16.4 (a), composed of the vector \\(\\{3, 4, 5\\}\\), and the first-column of Table 16.4 (b), composed of the same vector \\(\\{3, 4, 5\\}\\). Now, the number \\(50\\) comes from the fact that we multiply each of the corresponding entries of the two vectors, and then add them up, as follows:\n\\[\n(3 \\times 3) + (4 \\times 4) + (5 \\times 5) = 9 + 16 + 25 = 50\n\\]\nNeat! Now let’s see where the number \\(74\\) in the fourth row and second column of Table 16.5 came from. For that we look at the entries in the fourth row of Table 16.4 (a), composed of the vector \\(\\{5, 3, 4\\}\\) and the second column of Table 16.4 (b) composed of the vector \\(\\{7, 9, 3\\}\\). Like before, we take the first number of the first vector and multiply it by the first number of the second vector, the second number of the first vector and multiply it by the second number of the second vector, and the third number of the first vector and multiply it by the third number of the second vector and add up the results:\n\\[\n(5 \\times 7) + (3 \\times 9) + (4 \\times 3) = 35 + 27 + 12 = 74\n\\] And we keep on going like this to get each of the twenty five numbers in Table 16.5 (there are twenty five numbers because Table 16.5 has five rows and five columns and five times five equal twenty five). In general terms, the number in the \\(i^{th}\\) row and \\(j^{th}\\) column of Table 16.5 is equal to the sum of the products of the numbers in the \\(i^{th}\\) row of the Table 16.4 (a) and the \\(j^{th}\\) column of Table 16.4 (b).\nNote that the resulting product matrix shown in Table 16.5 is symmetric. The same numbers that appear in the upper-triangle also appear in the lower triangle, such that \\(b_{ij} = b_{ji}\\). So once you know the numbers in one of the triangles, you can fill up the numbers in the other one without having to do all the multiplying and adding up!\nNow, let’s multiply the matrix in Table 16.4 (b) times the matrix in Table 16.4 (a). As the rules of matrix multiplication show, this will result in a matrix of dimensions \\(3 \\times 3\\) because Table 16.4 (b) has three rows and $tbl-trans-1 has three columns. This is shown in Table 16.6.\n\n\n\n\n\n\n  \n    103 \n    124 \n    72 \n  \n  \n    124 \n    167 \n    91 \n  \n  \n    72 \n    91 \n    70 \n  \n\n\n\nTable 16.6:  Matrix resulting from multiplying a matrix times its transpose \n\n\nLike before, if we want to figure out where the number \\(72\\) in the third row and first column of Table 16.6 came from, we go to the first row of Table 16.4 (b) composed of the vector \\(\\{5, 3, 2, 4, 4\\}\\) and the first column of Table 16.4 (a), composed of the vector \\(\\{3, 7, 4, 5, 2\\}\\) match up each number in terms of order, multiplying them and add up the result:\n\\[\n(5 \\times 3) + (3 \\times 7) + (2 \\times 4) + (4 \\times 5) + (4 \\times 2) =\n\\]\n\\[\n15 + 21 + 8 + 20 + 8 = 72\n\\]\n\n\nTable 16.7: Powers of a matrix.\n\n\n\n\n(a) A matrix. \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n(b) Matrix squared. \n\n  \n    1 \n    1 \n    2 \n    0 \n  \n  \n    1 \n    1 \n    2 \n    1 \n  \n  \n    2 \n    1 \n    2 \n    2 \n  \n  \n    1 \n    1 \n    1 \n    2 \n  \n\n\n\n\n\n\n(c) Matrix cubed. \n\n  \n    2 \n    2 \n    3 \n    3 \n  \n  \n    3 \n    2 \n    4 \n    3 \n  \n  \n    4 \n    3 \n    5 \n    4 \n  \n  \n    3 \n    2 \n    4 \n    2 \n  \n\n\n\n\n\n\nMatrix powers work the same as regular matrix multiplication, except that we are working on just one matrix not two. So for instance, the number \\(2\\) in the first row and third column of Table 16.7 (b) comes from the numbers in the first row of Table 16.7 (a) (\\(\\{0, 1, 0, 1\\}\\)) and the numbers in the third column of Table 16.7 (a) (\\(\\{0, 1, 1, 1\\}\\)). We line them up, multiplying them, and add them:\n\\[\n(0 \\times 1) + (1 \\times 1) + (0 \\times 1) + (1 \\times 1) = 0 + 1 + 0 + 1 = 2\n\\] Since we are working with a binary matrix, the product of each of the cell entries will be either a zero (when at least one of the entries is zero) or a one (when both entries are one).\nTo get the cubed entries in Table 16.7 (c), we just take Table 16.7 (b) as the first matrix and Table 16.7 (a) as the second matrix, and do matrix multiplication magic. Thus, to get the number \\(4\\) in the third row and fourth column of Table 16.7 (c), we take the numbers in the third row of Table 16.7 (b) \\(\\{2, 1, 2, 2\\}\\) and the numbers in the fourth column of Table 16.7 (a) \\(\\{1, 0, 1, 0\\}\\), line them up, multiply them, and add them:\n\\[\n(2 \\times 1) + (1 \\times 0) + (2 \\times 1) + (1 \\times 0) = 2 + 0 + 2 + 0 = 4\n\\]\nPretty easy!"
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#references",
    "href": "9-1-lesson-matrix-operations.html#references",
    "title": "16  Matrix Operations",
    "section": "References",
    "text": "References"
  },
  {
    "objectID": "9-2-lesson-applied-matrix-operations.html#matrix-powers-and-cohesive-groups",
    "href": "9-2-lesson-applied-matrix-operations.html#matrix-powers-and-cohesive-groups",
    "title": "17  Applied Matrix Operations",
    "section": "17.1 Matrix Powers and Cohesive Groups",
    "text": "17.1 Matrix Powers and Cohesive Groups\nIt turns out that the matrix powers operation discussed in Section 16.4.3 was one of the earliest applications of formal social network analysis in the social sciences, discovered about the same time by mathematicians and social psychologists Duncan Luce, Albert Perry and Leon Festinger Festinger (1949). The basic idea is that when we obtain the powers of an adjacency matrix the resulting matrix has an intuitive interpretation in terms of indirect connections between people (see #sec-indirect), which gives us a sense of how strongly related in a formal sense pairs of nodes in the graph are.\nLet’s see an example.\n\n\nTable 17.1: An adjancency matrix and its powers.\n\n\n\n\n(a) Original adjacency matrix. \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    D \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    K \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\n(b) Adjacency matrix squared. \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    3 \n    2 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    B \n    2 \n    2 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    4 \n    2 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    D \n    1 \n    1 \n    2 \n    3 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    0 \n    3 \n    1 \n    1 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    3 \n    2 \n    1 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    G \n    1 \n    1 \n    0 \n    1 \n    1 \n    2 \n    3 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    1 \n    0 \n    2 \n    1 \n    1 \n    3 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    3 \n    1 \n    0 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    3 \n    1 \n    1 \n  \n  \n    K \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    2 \n    1 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    2 \n  \n\n\n\n\n\n\n\n\n(c) Adjacency matrix cubed. \n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    2 \n    2 \n    7 \n    6 \n    1 \n    0 \n    1 \n    1 \n    0 \n    1 \n    4 \n    1 \n  \n  \n    B \n    2 \n    2 \n    6 \n    5 \n    1 \n    0 \n    1 \n    1 \n    0 \n    0 \n    2 \n    0 \n  \n  \n    C \n    7 \n    6 \n    4 \n    6 \n    1 \n    2 \n    6 \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    D \n    6 \n    5 \n    6 \n    4 \n    1 \n    0 \n    2 \n    1 \n    1 \n    0 \n    1 \n    0 \n  \n  \n    E \n    1 \n    1 \n    1 \n    1 \n    4 \n    6 \n    6 \n    5 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    2 \n    0 \n    6 \n    2 \n    2 \n    6 \n    1 \n    5 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    6 \n    2 \n    6 \n    2 \n    2 \n    6 \n    0 \n    2 \n    1 \n    0 \n  \n  \n    H \n    1 \n    1 \n    1 \n    1 \n    5 \n    6 \n    6 \n    4 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    I \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    0 \n    1 \n    2 \n    5 \n    4 \n    4 \n  \n  \n    J \n    1 \n    0 \n    0 \n    0 \n    1 \n    5 \n    2 \n    1 \n    5 \n    2 \n    1 \n    4 \n  \n  \n    K \n    4 \n    2 \n    1 \n    1 \n    0 \n    1 \n    1 \n    0 \n    4 \n    1 \n    0 \n    1 \n  \n  \n    L \n    1 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n    4 \n    4 \n    1 \n    2"
  },
  {
    "objectID": "9-2-lesson-applied-matrix-operations.html#the-squared-adjacency-matrix",
    "href": "9-2-lesson-applied-matrix-operations.html#the-squared-adjacency-matrix",
    "title": "17  Applied Matrix Operations",
    "section": "17.2 The Squared Adjacency Matrix",
    "text": "17.2 The Squared Adjacency Matrix\nTable 17.1 (a) shows the adjacency matrix (\\(\\mathbf{A}\\)) corresponding to the “hangout” network in Figure 16.1 (a). Table 17.1 (b) shows the entries in \\(\\mathbf{A}^2\\) computed like we did earlier in the examples shown in Table 16.8. What is the meaning of the entries in each cell of Table 17.1 (b)?\nWell for the off-diagonal entries, the numbers in each cell tell us the number of indirect connections (specifically the number of walks; see Chapter 12) of length two between each pair of nodes.\nSo, for instance, we learn that node \\(A\\) can reach node \\(B\\) via two walks of length two, and can reach nodes \\(C\\), \\(D\\), \\(G\\), and \\(I\\) via one walk of length two. Remember from Chapter 12 that an indirect connection of a given length (in this case two) joins two nodes when it features them as the end nodes of the sequence of nodes and edges. Looking back at Figure 16.1 (a), we can see that the two walks of length two joining nodes \\(A\\) and \\(B\\) are \\(\\{AC, CB\\}\\) and \\(\\{AD, DB\\}\\), and that the walks of length two joining \\(A\\) to nodes \\(\\{C, D, G, I\\}\\) are:\n\\[\n\\{AD, DC\\}, \\{AC, CD\\}, \\{AC, CG\\}, \\{AK, KI\\}\n\\] The diagonal entries of Table 17.1 (b), on the other hand, give us the number of walks of length two that begin and end in the same node. Now what is the meaning of this? If you think of it, a walk of length two that starts in a node and goes to another node, and then comes back to the same node is just an edge in a symmetric graph! So the diagonals of \\(\\mathbf{A}^2\\) just count the number of edges incident to a node, which is the same as the degree of each node!"
  },
  {
    "objectID": "9-2-lesson-applied-matrix-operations.html#the-cubed-adjacency-matrix",
    "href": "9-2-lesson-applied-matrix-operations.html#the-cubed-adjacency-matrix",
    "title": "17  Applied Matrix Operations",
    "section": "17.3 The Cubed Adjacency Matrix",
    "text": "17.3 The Cubed Adjacency Matrix\nTable 17.1 (c) shows the corresponding entries for \\(\\mathbf{A}^3\\). What do these numbers mean? Well, you may have guessed. For the off-diagonal cells they are the number of walks of length three linking each pair of nodes. For instance, the “1” in the cell corresponding to nodes \\(H\\) and \\(F\\) tells us that there is walk of length three linking these two nodes. Looking at Figure 16.1 (a), we can see that this is given by: \\(\\{HG, GE, EF\\}\\).\nWhat do the numbers in the diagonal cells of Table 17.1 (c) mean? Well, as you may have guessed, they are actually the number of walks of length three that begin and end in that same node! As you may recall from Chapter 12, this is called a cycle of length three. So, the “2” in the diagonal cell entry corresponding to node \\(A\\) tells us that there are two cycles of length three featuring node \\(A\\) as its end nodes. Looking at Figure 16.1 (a), we can see that these are given by the sequences: \\(\\{AC, CD, DA\\}\\), \\(\\{AD, DC, DA\\}\\). In the same way, the number “4” in the diagonal cell for node \\(C\\) tells us that there are four cycles of length three that begin and end in that node. These are given by the sequences: \\(\\{CA, AD, DC\\}\\), \\(\\{CD, DA, AC\\}\\), \\(\\{CB, BD, DC\\}\\), and \\(\\{CD DB, BC\\}\\).\nNote that the edge sequence corresponding to cycles of length three is the same as that which corresponding to a clique of size three. So the diagonals in Table 17.1 (c), counts the number of cliques of size three that node belongs to. It actually counts twice the number of cliques of size three, because each clique is counted twice, once going in one direction (e.g., \\(\\{CA, AD, DC\\}\\)) and once going in the other direction \\(\\{CD, DA, AC\\}\\), so in Table 17.1 (c), the diagonal cell divided by two gives us the number of cliques of size three that node belongs to. When a node belongs to no clique, like node \\(K\\) in Figure 16.1 (a), then it gets a zero entry in the corresponding diagonal cell of \\(\\mathbf{A}^3\\).\n\n\n\n\n\nFigure 17.1: Graph with weighted edges representing number of indirect connections of length three between nodes\n\n\n\n\nFigure 17.1 shows the same graph as Figure 16.1 (a), but this time with the connections between nodes in the graph drawn as weighted edge with size and color intensity proportional to the entries in Table 17.1 (c) (the larger the number, the thicker and darker the edge), recording the number of walks of length three between each pair of nodes. As we can see, this reveals distinct hangout cliques like nodes \\(\\{A, B, C, D\\}\\) and nodes \\(\\{E, F, G, H\\}\\) that share multiple indirect connections with one another.\n\n17.3.1 To Infinity and Beyond!\nMore generally, for any adjacency matrix \\(\\mathbf{A}\\), the \\(n^{th}\\) power of the adjacency matrix gives us a symmetric matrix (\\(\\mathbf{A}^{n}\\)) whose off-diagonal entries \\(a^{n}_{ij}\\) record the number of walks of length \\(n\\) featuring the \\(i^{th}\\) node as the starting node and the \\(j^{th}\\) node as the end node, and whose diagonal entries \\(a^{n}_{ii}\\) record the the number of cycles of length \\(n\\) that begin and end with that node."
  },
  {
    "objectID": "9-2-lesson-applied-matrix-operations.html#to-infinity-and-beyond",
    "href": "9-2-lesson-applied-matrix-operations.html#to-infinity-and-beyond",
    "title": "17  Applied Matrix Operations",
    "section": "17.4 To Infinity and Beyond!",
    "text": "17.4 To Infinity and Beyond!\nMore generally, for any adjacency matrix \\(\\mathbf{A}\\), the \\(n^{th}\\) power of the adjacency matrix gives us a symmetric matrix (\\(\\mathbf{A}^{n}\\)) whose off-diagonal entries \\(a^{n}_{ij}\\) record the number of walks of length \\(n\\) featuring the \\(i^{th}\\) node as the starting node and the \\(j^{th}\\) node as the end node, and whose diagonal entries \\(a^{n}_{ii}\\) record the the number of cycles of length \\(n\\) that begin and end with that node."
  },
  {
    "objectID": "9-2-lesson-applied-matrix-operations.html#matrix-multiplication-and-common-neighbors",
    "href": "9-2-lesson-applied-matrix-operations.html#matrix-multiplication-and-common-neighbors",
    "title": "17  Applied Matrix Operations",
    "section": "17.4 Matrix Multiplication and Common Neighbors",
    "text": "17.4 Matrix Multiplication and Common Neighbors\nAs we noted in Section 16.4.2, it is always possible to multiply any matrix times its transpose. Well, the adjacency matrix of a network (\\(\\mathbf{A}\\)) like Table 17.1 (a) is a matrix. That means it call always be multiplied times its transpose (\\(\\mathbf{A}^T\\)), resulting in some other matrix \\(\\mathbf{B}\\), of the same dimensions as the original adjacency matrix:\n\\[\n\\mathbf{A} \\times \\mathbf{A}^T = \\mathbf{B}\n\\] The entries corresponding to \\(\\mathbf{B}\\) computed according to the matrix multiplication rules laid out in Section 16.5, are shown in Table 17.2.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    A \n    3 \n    2 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    B \n    2 \n    2 \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    4 \n    2 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    D \n    1 \n    1 \n    2 \n    3 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    0 \n    3 \n    1 \n    1 \n    2 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    3 \n    2 \n    1 \n    1 \n    0 \n    0 \n    1 \n  \n  \n    G \n    1 \n    1 \n    0 \n    1 \n    1 \n    2 \n    3 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    1 \n    0 \n    2 \n    1 \n    1 \n    3 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    I \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    3 \n    1 \n    0 \n    1 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    3 \n    1 \n    1 \n  \n  \n    K \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    2 \n    1 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n    1 \n    2 \n  \n\n\n\nTable 17.2:  Adjacency matrix multiplied by its transpose. \n\n\nWhat are the entries in Table 17.2? Well, first note one thing, the diagonal entries of Table 17.2 are the same as the diagonal entries of Table 17.1 (b). That means that it is counting the degree of each node.\nWhat are the off-diagonal entries of \\(\\mathbf{B}\\) though? Let’s see where they come from using the rules of matrix multiplication (see Section 16.5). Let’s take the entry corresponding to nodes \\(A\\) and \\(C\\) (the cell corresponding to the first row and third column) in Table 17.2. We see there is a “1” there. We know it must have come from matching the numbers in the first row of Table 17.1 (a) with the numbers in the third column of the same table. These are:\n\n\nTable 17.3: Entries from an adjacency matrix\n\n\n\n\n(a) First row entries (node A) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(b) Third column entries (node C) \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    1 \n    1 \n    0 \n    1 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) Product of first row entries and third column entries \n \n  \n    A \n    B \n    C \n    D \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n  \n \n\n  \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\n\n\n\nTo get the entry in cell \\(b_{13}\\) of matrix \\(\\mathbf{B}\\) all we need to do is multiply each of the zeros and ones in Table 17.3 (a) and Table 17.3 (b) and add up the result. When we do that we get the numbers in Table 17.3 (c). We see that the only lonely “1” in Table 17.3 (c) corresponds to node \\(D\\), note that this happens to be the only common neighbor shared by nodes \\(A\\) and \\(C\\).\nSo we cracked the mystery of the off-diagonal entries of Table 17.2! When we multiply an adjacency matrix times its transpose, we end up with a matrix whose off-diagonal cells count the number of common neighbors shared by the row node and the column node, and whose diagonal entries count the total number of neighbors of that node."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#bipartite-graphs",
    "href": "10-lesson-affiliation-networks.html#bipartite-graphs",
    "title": "18  Affiliation Networks",
    "section": "18.1 Bipartite Graphs",
    "text": "18.1 Bipartite Graphs\nA bipartite graph is useful to represent a network where, rather than ties occurring between nodes of the same kind (e.g., people connected with other people), ties occur only between nodes of different kinds but never between nodes of the same kind. Typically, the two different types of nodes are located at different levels of analysis or aggregation. As such, bipartite graphs are perfect for capturing the sociological concept of affiliation or membership with larger groups or events (Breiger 1974). For instance, actors and the movies they make, scientists and the papers they write, or people and the groups they belong to.1\n\n\n\n\n\nFigure 18.1: A bipartite graph. Circles are people and triangles are the corporate boards they belong to\n\n\n\n\nFor example, people work at companies, so we might say that a worker is connected with the company, rather than any specific individual there. People also connect to sports teams, schools, religious communities, and other organizations which can have an influence in structuring their social world.\nIn the graph theoretic sense, a bipartite graph \\(G_B\\) is a graph featuring two sets of nodes \\(V_1\\) and \\(V_2\\) and one set of edges \\(E\\). Thus a bipartite graph, like a signed and a weighted graph, is a set of three sets:\n\\[\n    G_B = (E, V_1, V_2)\n\\tag{18.1}\\]\n\nrepresents a network diagram of a bipartite graph where circles connect to triangles (with the shapes standing as labels for the two set of nodes). In the Figure, \\(V_1 = \\{A, B, C, D, E\\}\\) and \\(V_2 = \\{1, 2, 3, 4, 5\\}\\). The edge set \\(E\\) is \\(\\{A1, A2, B2, B3, C2, C4, D4, D3, E3, E5\\}\\).\n\nOne common example of two-mode networks that be represented using bipartite graphs in sociology are corporate interlock networks (Mizruchi 1983). If 1) represented such a network, we could think of the circles as members of the company’s board, and the triangles are the board from each company. Because the same executive can be a member of more than one company’s board, board member A is on the board of both companies 1 and 2, while board member B is on the board of companies 2 and 3.\nNote that edges in a bipartite graph are symmetrical and thus bipartite graphs are (generally) undirected. This makes sense, since the relationship affiliation or membership is indeed symmetrical by definition. If person A is a member of the board in company 2 then it is understood that company 2 has person A as a board member.\nIn the same way, note that there is no reason why the cardinality of two node sets in a bipartite graph have to be same (although they are in the example provided). In a real world corporate interlock network, for instance, there will generally be more people than companies, so \\(|V_1| > |V_2|\\)."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "href": "10-lesson-affiliation-networks.html#unipartite-projections-of-bipartite-graphs",
    "title": "18  Affiliation Networks",
    "section": "18.2 Unipartite Projections of Bipartite Graphs",
    "text": "18.2 Unipartite Projections of Bipartite Graphs\nWhile the information we can glean from looking at the original bipartite graph alone may be useful, you might realize that board members A and B both are on the boards of company 2! In fact, board member C is also on the board of company 2! We might thus conclude that board members A, B, and C all know each other from sitting in the same company board.\n\n\n\n\n\nFigure 18.2: A unipartite graph. People are linked if they serve in the same company board\n\n\n\n\nIf this sort of information was important, we could convert the bipartite into a simple unipartite graph capturing connections between the same level of analysis. This is called a projection of the original bipartite graph. In the projected graph, two board members are joined by a symmetric tie if they both serve on the board of at least one company together.\n\n\n\n\n\nFigure 18.3: Another unipartite graph. Boards are linked if they share members.\n\n\n\n\nThus, we could, as shown in Figure 18.2, create a graph that shows board members who know each other because they work at the same company. The resulting (simple, undirected) graph shows that board members A, B, and C all know each other as a result of serving in the board of company 2 together.\nLikewise, we can transform the bipartite graph into a simple unipartite graph that captures companies that share board members. Company 2 is thus connected to Companies 1 (because of person A), 3 (because of person B), and 4 (because of person 5). This is shown in Figure 18.3). In fact, the reason why these are called interlock networks, is because it is easy to see that, ultimately, by virtue of sharing members across boards, most big corporations in the U.S. (and other countries), end up forming part of a single giant network."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "href": "10-lesson-affiliation-networks.html#from-biparite-graph-to-affiliation-matrix",
    "title": "18  Affiliation Networks",
    "section": "18.3 From Biparite Graph to Affiliation Matrix",
    "text": "18.3 From Biparite Graph to Affiliation Matrix\nConsider the two-mode network shown in Figure 18.4. This is an affiliation network meant to represent the memberships of six students in five college activity clubs. As discussed earlier, we use a bipartite graph to represent the network. The bipartite graph represents the two sets of nodes using different shapes or colors (blue and red nodes in Figure 18.4), and draws a link between the people and the group if the person is affiliated with the group.\n\n\n\n\n\nFigure 18.4: Bipartite graph of a two-mode network of students and clubs.\n\n\n\n\nHow can we translate the graph representation into a matrix?\nThe procedure is the same as that used to build the adjacency matrix of the symmetric graph. We build a rectangular matrix whose number of rows is the same as the number of people in the affiliation network, and whose number of rows is the same as the number of groups. The matrix is rectangular (as opposed to square) because in a two-mode network, there is no restriction that the size of the two vertex sets be the same (although if they happen to be the same then you end up with a square matrix; after all, a square is a special case of a rectangle!).\nIn graph theory terms, this is a matrix that we call A, for affiliation matrix of dimensions \\(R \\times C\\), where the number of rows \\(R = |V_1|\\) is the cardinality of the first vertex set in the bipartite graph (persons in Figure 18.4), and where the number of rows \\(C = |V_2|\\) is the cardinality of the second vertex set (clubs in Figure 18.4)). The cells of the affiliation matrix, \\(a_{ij} = 1\\) if person i belongs to club j (there’s an symmetric edge in the graph linking the person to the group), otherwise, \\(a_{ij} = 0\\).\nFollowing these instructions would yield the affiliation matrix shown in Table 18.1.\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nGabriela\n1\n1\n1\n1\n0\n\n\nParker\n1\n0\n1\n0\n0\n\n\nBrandon\n0\n1\n1\n1\n0\n\n\nMarie\n0\n0\n1\n0\n1\n\n\nRahul\n0\n1\n0\n1\n0\n\n\nMinjoo\n0\n0\n0\n0\n1\n\n\n\nTable 18.1: Affiliation matrix of a bipartite graph.\n\n\nThe affiliation matrix has some interesting properties. For instance, just like the adjacency matrix, it can be used to compute node degree centrality for each set of nodes. But since we have two different sets of nodes, we end up with two different sets of centrality scores; one set of centrality scores for the people and another set for the groups (Faust 1997).\nLet us see how this works."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#group-and-person-centralities",
    "href": "10-lesson-affiliation-networks.html#group-and-person-centralities",
    "title": "18  Affiliation Networks",
    "section": "18.4 Group and Person Centralities",
    "text": "18.4 Group and Person Centralities\n\n18.4.1 Person Centralities\nIf we wanted to figure out the degree centrality of the people node set (abbreviated P) in the affiliation matrix, we would sum cell entries across the rows, according to the now familiar equation:\n\\[\n    C_P^{DEG} = \\sum_j a_{ij}\n\\tag{18.2}\\]\nWhich leads to the following vector of degree centrality scores for the people:\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\n4\n2\n3\n2\n2\n1\n\n\n\nTable 18.2: Degree centrality scores for the people.\n\n\nThe degree centrality scores for the people can be interpreted as giving us a sense of their joining activity (e.g., high versus low). Some people, (like Gabriela) join a lot of clubs; they have multiple interests spread out across many organizations. Other people, (like Minjoo), just have a single interest, and thus join only one club (the Cheese Club). If centrality is defined using the “more/more principle” discussed in lesson on centrality, then we would say that Gabriela is more central than Minjoo in the affiliation network.\n\n\n18.4.2 Group Centralities\nIn the same way, if wanted to compute the degree centralities of other mode (the club node set, abbreviated as G), then we would calculate the column sums of the affiliation matrix using a slight variation of Equation 18.2), like we did when we switched from outdegree to indegree:\n\\[\n    C_G^{DEG} = \\sum_i a_{ij}\n\\tag{18.3}\\]\nWhich leads to the following degree centrality scores for the clubs:\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\n2\n3\n4\n3\n2\n\n\n\nTable 18.3: Degree centrality scores for the clubs.\n\n\nJust like the people, the centrality scores for the clubs tell us something about the popularity of each group. Some groups are popular (have lots of members), others are less so. So, it seems like in this student group, the Magic Club is definitely the most popular, containing four members. The Cheese and Fashion Clubs on the other hand, seem to be more niche pursuits, with only two members each."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#the-affiliation-matrix-transpose",
    "href": "10-lesson-affiliation-networks.html#the-affiliation-matrix-transpose",
    "title": "18  Affiliation Networks",
    "section": "18.5 The Affiliation Matrix Transpose",
    "text": "18.5 The Affiliation Matrix Transpose\nAs discussed in Section 16.4.2, it is possible to “flip” the rows and columns of any matrix, so what was previously the rows become the columns, and what was previously the columns become the rows. This is called the matrix transpose and if the original matrix was called A, then the transpose is called A’.2 If the original matrix A was of dimensions \\(R \\times C\\) then the transpose A’ is of dimensions \\(C \\times R\\).\nThe transpose of the affiliation matrix shown in Table 18.1 is shown in Table 18.4.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nFashion\n1\n1\n0\n0\n0\n0\n\n\nNerdfighters\n1\n0\n1\n0\n1\n0\n\n\nMagic\n1\n1\n1\n1\n0\n0\n\n\nSuper Smash Brs.\n1\n0\n1\n0\n1\n0\n\n\nCheese\n0\n0\n0\n1\n0\n1\n\n\n\nTable 18.4: Transpose of the affiliation Matrix.\n\n\nNote that the transpose of the affiliation matrix contains exactly the same information as the original affiliation matrix. The group affiliations of every person are preserved as are memberships of each group. If we used equations Equation 18.2, and Equation 18.3) to compute the person and group centralities using the affiliation matrix transpose A’, we would get the same results, except that the first equation (summing across the rows) would now give us the group centralities, and the second equation (summing down the columns) would give use the people centralities!\nWe learn from matrix algebra that an important property of rectangular matrices is that you can always multiply a rectangular matrix by its transpose (see Section 16.4.1). Recall a key condition of matrix multiplication is that the two matrices be conformable so that the columns of the first matrix need to match the number of rows of the second matrix. Well, it’s clear than since any matrix that is of dimensions \\(R \\times C\\), will have a transpose of dimensions \\(C \\times A\\) then the multiplication of the two matrices will be defined:\n\\[\n    A^{}_{R \\times C} \\times A^{'}_{C \\times R} = defined!\n\\tag{18.4}\\]\nIn the same way, the transpose of a matrix can always be multipled by the original matrix:\n\\[\n    A^{'}_{C \\times R} \\times A^{}_{R \\times C} = defined!\n\\tag{18.5}\\]"
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "href": "10-lesson-affiliation-networks.html#the-person-and-group-overlap-matrices",
    "title": "18  Affiliation Networks",
    "section": "18.6 The Person and Group Overlap Matrices",
    "text": "18.6 The Person and Group Overlap Matrices\nIf the transpose of the affiliation matrix contains the same information as the original why do we care about it? Well the reason is that we can use the multiplication property described in Section 16.4.2 to extract two new matrices that contain new (or at least not obvious, especially for large two-mode networks), information from the original affiliation matrix. The first is called the person overlap matrix (written \\(O^P\\)), this is defined for an original affiliation matrix, in which people are listed in the rows and groups, events, or project, listed in the columns using the following matrix equation:\n\\[\n    O^P = A^{ }_{R \\times C} \\times A^{'}_{C \\times R}\n\\tag{18.6}\\]\n\n18.6.1 The Person Overlap Matrix\nUsing the rules for matrix multiplication discussed Section 16.4.1, the person overlap matrix obtained using the affiliation matrix shown in Table 18.1 is shown in Table 18.5.\n\n\n\n\n\n\n\n\nGabriela\nParker\nBrandon\nMarie\nRahul\nMinjoo\n\n\n\n\nGabriela\n4\n2\n3\n1\n2\n0\n\n\nParker\n2\n2\n1\n1\n0\n0\n\n\nBrandon\n3\n1\n3\n1\n2\n0\n\n\nMarie\n1\n1\n1\n2\n0\n1\n\n\nRahul\n2\n0\n2\n0\n2\n0\n\n\nMinjoo\n0\n0\n0\n1\n0\n1\n\n\n\nTable 18.5: Person Overlap Matrix.\n\n\nThe person overlap matrix transforms the initial rectangular affiliation matrix, which has people in the rows and groups in the columns, to a square matrix, which like the usual relationship matrices we have been dealing with, feature people in both the rows and the columns. Each entry in the person overlap matrix \\(o^P_{ij}\\) now gives us the number of groups in which person i and j mutually belong to (Breiger 1974). So we learn that Gabriela and Brandon have three memberships in common (I bet they seen another a lot!) but that Rahul and Parker have no memberships in common (so they are less likely to encounter one another).\nNote also that, in the person overlap matrix, (in contrast to the usual adjacency matrix), there are valid entries along the diagonal cells (\\(o^P_{ii}\\)). These cells now record the total number of memberships that the node corresponding to that row (or column) has. Which we ascertained by computing the node centralities in the original affiliation matrix using Equation 18.2). You can see that the vector of degree centralities shown in Table 18.2) is the same as the vector formed by the diagonal entries in ?tbl-comem).\n\n\n18.6.2 The Group Overlap Matrix\nIn the same way we can compute the person overlap matrix, it is possible to calculate another matrix, called the group overlap matrix (written \\(O^G\\)), this time by multiplying the transpose of the original affiliation matrix times the original We do that using the following equation:\n\\[\n    O^G = A^{'}_{C \\times R} \\times A^{ }_{R \\times C}\n\\tag{18.7}\\]\nRecall from Section 16.4 that matrix multiplication is not commutative (if \\(A\\) is a rectangular matrix, then \\(A \\times A^{'} \\neq A^{'} \\times A\\))), so Equation 18.7 gives you a different answer than Equation 18.6. The result is shown in Table 18.6.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFashion\nNerdfighters\nMagic\nSuper Smash Brs.\nCheese\n\n\n\n\nFashion\n2\n1\n2\n1\n0\n\n\nNerdfighters\n1\n3\n2\n3\n0\n\n\nMagic\n2\n2\n4\n2\n1\n\n\nSuper Smash Brs.\n1\n3\n2\n3\n0\n\n\nCheese\n0\n0\n1\n0\n2\n\n\n\nTable 18.6: Group Overlap Matrix.\n\n\nThe group overlap matrix (O), like the person overlap matrix, is also square. But this time it has groups in both the rows and columns. Each cell in the group overlap matrix \\(o^P_{ij}\\) records the number of people groups i and groups j have in common (Breiger 1974). Thus, we learn that the Super Smash Brothers group and the Nerdfighters groups share three members in common but that the Super Smash Brothers and the Cheese group have no members in common (pointing to a disaffinity between these activities).\nNote that both the person and group overlap matrices are symmetric. It is easy to see why this is; if I have three group overlaps with you, then you by definition also have three group overlaps with me; if group A has three members in common with group B, then group B has three members in common with group A. This means that if they were to be taken as representing a network, then the resulting graph would be undirected (but weighted because there can be more or less overlap between people and groups). We will see how to do that below."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "href": "10-lesson-affiliation-networks.html#overlapping-node-neighborhoods-in-two-mode-networks",
    "title": "18  Affiliation Networks",
    "section": "18.7 Overlapping Node Neighborhoods in Two-Mode Networks",
    "text": "18.7 Overlapping Node Neighborhoods in Two-Mode Networks\nThe notion of overlap used to construct the person and group overlap matrix is the same as the idea of overlapping node neighborhoods for regular networks, discussed in Chapter 3 Thus, while nodes of the same kind cannot be connected in a two-mode network (by construction), they can share neighbors. In a two-mode network if a node belong to one of the vertex sets, let’s say \\(V_1\\), then all of their neighbors have to belong to the other vertex set (\\(V_2\\)) and vice versa.\nFor instance, in Figure 18.4), Gabriela’s node neighborhood is:\n\\[\n    Gab_{NN} = \\{Fashion, Nerdfighters, Magic, SuperSmashBros\\}\n\\]\nRahul’s node neiborhood is:\n\\[\n    Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nThe intersection between their neighborhoods is:\n\\[\n    Gab_{NN} \\cap Rah_{NN} = \\{Nerdfighters, SuperSmashBros\\}\n\\]\nSo now we can see that the number “2” recorded in the cell that corresponds to Gabriela and Rahul in the person overlap matrix shown in Table 18.5) is the cardinality of the subset formed by the intersection of their two neighborhoods, which in this case contain two members (the Nerdfighters and Super Smash Brothers clubs). The same procedure can be used to figure out the overlap between the node neighborhoods of groups (which happen to be subsets of people in the larger two-mode network)."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "href": "10-lesson-affiliation-networks.html#one-mode-projections-of-two-mode-networks",
    "title": "18  Affiliation Networks",
    "section": "18.8 One Mode Projections of two-mode Networks",
    "text": "18.8 One Mode Projections of two-mode Networks\nNote that both the comembership and group overlap matrices, being square matrix with values that go beyond zero and one in the cells, look like a lot like the adjacency matrix that could be obtained from a weighted graph as discussed in the lesson on types of graphs.\n\n\n\n\n\nFigure 18.5: One mode (persons) projection of the original bipartite graph.\n\n\n\n\nSo using formulas Equation 18.6) and Equation 18.7), it is possible to go from a two-mode network in which no links exist between nodes of the same kind, to a weighted graph, in which the links between nodes of the same kind are defined by the overlap of their neighborhoods in the original bipartite graph. As we noted earlier, this is called the one mode projection of the two-mode network. Each two-mode network thus has two one mode projection one for each node set.\n\n\n\n\n\nFigure 18.6: One mode (groups) projection of the original bipartite graph.\n\n\n\n\nThe one mode projection for the person node set of the bipartite graph show in Figure 18.4) is shown in Figure 18.5), this is an undirected weighted graph with the edge weight between people being set to the number of comemberships between each dyad as recorded in the person overlap matrix shown in Table 18.5). In this respect, the number of comemberships can be seen as a proxy of the tie strength between two people, when we only have information on their affiliations. As the Figure shows, Brandon, Gabriela and Rahul form a tightly connected clique, given the number of memberships they share. Minjoo, who does not share many affiliations with anyone, stands toward the periphery of the person-to-person comembership network.\nThe corresponding one-mode projection for the group node set is shown in Figure 18.6). This weighted graph can be read the same way: The thickness of the ties between groups are proportion to the people they share as recorded in the group overlap matrix shown in Table 18.6), thus speaking to the similarity or strength of connectivity between groups.\nSo we see, as we noted before, that Super Smash Brothers and Nerdfighters are tightly connected, but that the Cheese Club is largely peripheral in the group-to-group network. This peripheral status mirrors the marginal status of Minjoo (one of the few members of the Cheese Club) in the person-to-person network.\nThe fact that peripheral people belong peripheral groups and central people belong to central groups encodes a fundamental principle in the analysis of two-mode networks (Breiger 1974) and that is the duality principle.\nThe duality principle in two-mode network analysis says that the position of people in a two-mode network is defined by the positions the groups they affiliate with occupy, and in the same way, the position of the groups in a two-mode network is defined by the positions of the people that belong to them (Bonacich 1991)."
  },
  {
    "objectID": "10-lesson-affiliation-networks.html#references",
    "href": "10-lesson-affiliation-networks.html#references",
    "title": "18  Affiliation Networks",
    "section": "References",
    "text": "References\n\n\n\n\nBonacich, Phillip. 1991. “Simultaneous Group and Individual Centralities.” Social Networks 13 (2): 155–68.\n\n\nBreiger, Ronald L. 1974. “The Duality of Persons and Groups.” Social Forces 53 (2): 181–90.\n\n\nFaust, Katherine. 1997. “Centrality in Affiliation Networks.” Social Networks 19 (2): 157–91.\n\n\nMizruchi, Mark S. 1983. “Who Controls Whom? An Examination of the Relation Between Management and Boards of Directors in Large American Corporations.” Academy of Management Review 8 (3): 426–35."
  },
  {
    "objectID": "11-lesson-centrality.html#the-big-three-centrality-metrics",
    "href": "11-lesson-centrality.html#the-big-three-centrality-metrics",
    "title": "19  Centrality",
    "section": "19.1 The “big three” centrality metrics",
    "text": "19.1 The “big three” centrality metrics\nLinton Freeman (1979), in the aforementioned paper, defined the “big three” classic centrality metrics, roughly corresponding to the extent that a node accumulates one of the three network goods mentioned above. - So the degree centrality metric deal with nodes that have more edges directly incident upon them (Nieminen 1974). - The closeness centrality metric has to do with nodes that can reach more nodes via smallest shortest paths and thus accumulate as many of these paths in which they figure as the origin node as possible (Sabidussi 1966). - Finally, the betweenness centrality metric has to do with a node’s accumulation of the largest share of shortest paths in which they intermediate between two other nodes, and thus featuring them as one of the inner nodes in the paths between others (Freeman 1977).\nOther centrality metrics can be seen as generalizations or special cases of any of these three basic notions (Borgatti 2005).\nThe rest of the lesson goes over the basic interpretation and calculation (using the graph theory and matrix algebra tools discussed in previous lessons) of “big three” centrality metrics."
  },
  {
    "objectID": "11-lesson-centrality.html#the-star-graph",
    "href": "11-lesson-centrality.html#the-star-graph",
    "title": "19  Centrality",
    "section": "19.2 The Star Graph",
    "text": "19.2 The Star Graph\nFreeman showed that the three basic measures reach their theoretical maximum for the central node in a star graph, such as the one shown in Figure 19.1).\n\n\n\n\n\nFigure 19.1: A star graph\n\n\n\n\nA star graph is a graph containing a central or inner node (in Figure 19.1, node A), who is connected to all the other nodes in the graph, called the satellite or outer nodes (in Figure 19.1, nodes B through F). These nodes in contrast have only one connection and that is to the central node, none among themselves.\nBecause of these restrictions, it is easy to see that if \\(G = (V, E)\\) is a star graph of order \\(n\\), then we know that that graph size \\(m = |E|\\) (the size of the edge set), has to be \\(n-1\\). So in the example shown in Figure 19.1, \\(n =7\\) and \\(m = n-1 = 7-1=6\\). Neat!"
  },
  {
    "objectID": "11-lesson-centrality.html#degree-centrality",
    "href": "11-lesson-centrality.html#degree-centrality",
    "title": "19  Centrality",
    "section": "19.3 Degree Centrality",
    "text": "19.3 Degree Centrality\nThe first way of defining centrality is simply as a measure of how many alters an ego is connected to. This simply takes a node’s degree as introduced in the lesson on graph theory, and begins to consider this measure as a reflection of importance of the node in the network. The logic is that those with more direct connections to others, compared to those with fewer, hold a more prominent place in the network.\nOnce we have constructed the adjacency matrix for the network (A), then degree centrality is easy to calculate. As Equation 19.1) for a given node i the degree centrality is given by summing the entries of its corresponding row.\n\\[\n  C_i^{DEG} = \\sum_{j= 1}^{n}a_{ij}\n\\tag{19.1}\\]\nEquation 19.1 thus ranks each node in the graph based on the number of other nodes that it is adjacent to. Just like real life, some nodes will be popular (they will be adjacent to lots of other nodes), while others will be unpopular.\nAlthough it might seem a simple task to just add up the number of connections of each node, that is essentially what the below mathematical equation is doing! Mathematical notation plays an important role in expressing network measures in succinct formats.\nFor instance, if we were to use Equation 19.1 to calculate the degree centrality of each node from the symmetric adjacency matrix corresponding to the graph shown in Figure 5.1) then we would end up with the following degree centralities for each node:\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n4\n3\n4\n5\n3\n4\n3\n3\n3\n\n\n\nTable 19.1: Degree centralities of nodes in an undirected graph.\n\n\n\n19.3.1 How to Read Equations for Matrix Operations\nBefore we continue, a note on something obvious. A lot of centrality measures are expressed as equations and these can be hard to interpret initially. Depending on your background in math, you may or may not already know how to interpret Equation 19.1). Essentially, the number at the bottom of the sigma is where to start. The number at the top of the Sigma symbol (\\(\\Sigma\\)) is where to end. The equation to the left is the operation to be performed. Thus, one reads the Equation 19.1 as, starting at column \\(j=1\\), and ending at the last possible column \\(n\\) (\\(n\\) is simply the total number of rows in the matrix, \\(n\\) means to go to the final value in the matrix), add up all possible values of the cells designated by the row \\(i\\) and column \\(j\\) combination in matrix A. Thus, to calculate the degree centrality of each \\(j = a, b, c\\) in the below matrix, each of the following calculations would be performed.\n\n\nTable 19.2: Simple matrix.\n\n\n\na\nb\nc\n\n\n\n\na\n-\n1\n0\n\n\nb\n1\n-\n1\n\n\nc\n0\n1\n-\n\n\n\n\n\\(C_D(a)=aa+ab+ac=1\\)\n\\(C_D(b)=ba+bb+bc=2\\)\n\\(C_D(c)=ca+cb+cb=1\\)\nIn the same way if we had the formula:\n\\[\n  C_D(j) = \\sum_{i = 1}^{n}a_{ij}\n\\]\nThen it would be telling us to sum values of each column \\(j\\) down each row \\(i\\):\n\\(C_D(a)=aa+ba+ca=1\\)\n\\(C_D(b)=ab+bb+cb=2\\)\n\\(C_D(c)=ac+bc+cc=1\\)\nThe sigma notation is useful for summarizing this repetitive process in a simple, condensed form."
  },
  {
    "objectID": "11-lesson-centrality.html#indegree-and-outdegree-centrality",
    "href": "11-lesson-centrality.html#indegree-and-outdegree-centrality",
    "title": "19  Centrality",
    "section": "19.4 Indegree and Outdegree Centrality",
    "text": "19.4 Indegree and Outdegree Centrality\nIf we are talking about a directed graph, then there are two types of degree centralities that can be calculated. On the one hand, we may be interested in how central a node is in terms of sociability or expansiveness that is how many other nodes in the graph a given node sends links to. This is called the outdegree centrality of that node, written as \\(C_i^{OUT}\\). As with the undirected case, this is computed by summing across the rows of the asymmetric adjacency matrix corresponding to the directed graph in question, using Equation 19.1:\n\\[\n  C_i^{OUT} = \\sum_ja_{ij}\n\\tag{19.2}\\]\nHowever, in a directed graph, we may also be interested in how popular or sought after by others a given node is. That is, how many other actors send ties to that node. In which case we need to sum across the columns of the asymmetric adjacency matrix, and modify the formula as follows:\n\\[\n  C_i^{IN} = \\sum_ia_{ij}\n\\tag{19.3}\\]\nNote that in this version of the equation, we are summing over j (the columns) not over i (the rows) as given by subscript under the \\(\\sum\\) symbol.\nFor instance, if we were to use equations Equation 19.2 and Equation 19.2 to calculate the outdegree and indegree centrality of each node from the asymmetric adjacency matrix corresponding to the graph shown in Figure 5.2), then we would up with the following centralities for each node:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nOutdegre\n2\n2\n1\n1\n2\n1\n2\n\n\nIndegree\n2\n3\n1\n3\n0\n2\n0\n\n\n\nTable 19.3: Out and Indegree centralities of nodes in a directed graph.\n\n\nJust like the degree centrality for undirected graphs, the outdegree and indegree centralities rank each node in a directed graph. The first, outdegree centrality, ranks each node based on the number of other nodes that they are connected to. This is a kind of popularity based on sociability, or the tendency to seek out the company of others. The second, indegree centrality, ranks each node in the graph based on the number of other nodes that connect to that node. This is a kind of popularity based on on being sought after a kind of status.\n\n19.4.1 Normalized Degree Centrality\nWhen we compute the degree centrality of a node, are counting the number of other nodes that they are connected to. Obviously, the more nodes there are to connect to, the more opportunities there will be to reach a larger number. But what happens if we wanted to compare the degree centrality of nodes in two very different networks?\nFor instance, if your high-school has one thousand people and you have twenty friends, that’s very different from having twenty friends in a high-school of only one hundred people. It seems like the second person, with twenty friends (covering 20% of the population) in a high-school of one-hundred people is definitely more popular than the second person with twenty friends (covering 2% of the population), in a high school with one thousand people.\nThat’s why Freeman Freeman (1979) proposed normalizing the degree centrality of each node by the maximum possible it can take in a given network. As you may have guessed, the maximum degree in a network is \\(N-1\\) the order of the graph minus one. Essentialy, everyone but you!\nWe can compute the normalized degree centrality using the following equation:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{C_{i}^{DEG}}{N-1}\n\\tag{19.4}\\]\nWhere we just divide the regular degree centrality computed using Equation 19.1 by the order of the graph minus one. This will be equal to \\(1.0\\) if a person knows everyone and \\(0\\) is a person knows no one. For all the other nodes it will be a number between zero and one.\nMoreover, this measure is sensitive to the order of the graph. Thus, for a person with twenty friends in a high-school of a thousand people, the normalized degree centrality is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{1000-1}= 0.02\n\\tag{19.5}\\]\nBut for the person with the same twenty friends in a high-school of one-hundred people, it is equal to:\n\\[\n  C_{i(norm)}^{DEG} = \\frac{20}{100-1}= 0.20\n\\tag{19.6}\\]\nIndicating that a person with the same number of friends in the smaller place is indeed more central!"
  },
  {
    "objectID": "11-lesson-centrality.html#closeness-centrality",
    "href": "11-lesson-centrality.html#closeness-centrality",
    "title": "19  Centrality",
    "section": "19.5 Closeness Centrality",
    "text": "19.5 Closeness Centrality\nSometimes it not important how many people you directly connected to. Instead, what is important is that you are indirectly connected to a lot of others. As we saw in the lesson on indirect connectivity, the best way to conceptualize indirect connectivity in social networks is via the idea of shortest paths. So if you can reach the most other people in the network via shortest paths with only a few hops, then you are better connected that someone who has to use longer paths to reach the same other people.\n\n\n\n\n\nFigure 19.2: An undirected graph showing the node with the maximum closeness centrality (in red).\n\n\n\n\nThis insight serves as an inspiration for a measure of centrality based on closeness. The closeness between two nodes is the inverse of the geodesic distance them (Bavelas 1950). Recall that the geodesic distance is given by the length of the shortest path linking two nodes in the graph. The smallest the length of the shortest path separating two nodes in the graph, the closer the two nodes and vice versa.\nRemember that for any number \\(n\\), the mathematical operation of taking the inverse simply means dividing one by that number. So, the inverse of \\(n\\) is \\(\\frac{1}{n}\\). This means that if \\(d_{ij}\\) is the geodesic distance between nodes i and j in graph \\(G\\), then the closeness between two nodes is \\(\\frac{1}{d+_{ij}}\\).\nThe information on the pairwise geodesic distances between every pair of nodes in a given graph is captured in the geodesic distance matrix, as discussed in Chapter 15. For instance, take the graph shown in Figure 19.2. The distance matrix for this graph is shown in Table 19.4.\n\n\n\n\n\n\n\n\nE\nA\nM\nL\nB\nJ\nG\nN\nF\nH\nD\nC\nK\nI\n\n\n\n\nE\n0\n1\n2\n1\n2\n1\n2\n1\n2\n1\n3\n2\n2\n2\n\n\nA\n1\n0\n3\n1\n3\n2\n2\n2\n3\n2\n2\n1\n1\n2\n\n\nM\n2\n3\n0\n3\n1\n2\n2\n1\n2\n2\n1\n3\n2\n4\n\n\nL\n1\n1\n3\n0\n3\n2\n1\n2\n2\n2\n2\n1\n2\n1\n\n\nB\n2\n3\n1\n3\n0\n1\n2\n2\n1\n2\n2\n2\n3\n4\n\n\nJ\n1\n2\n2\n2\n1\n0\n2\n1\n1\n2\n2\n1\n3\n3\n\n\nG\n2\n2\n2\n1\n2\n2\n0\n3\n1\n2\n1\n2\n2\n2\n\n\nN\n1\n2\n1\n2\n2\n1\n3\n0\n2\n1\n2\n2\n2\n3\n\n\nF\n2\n3\n2\n2\n1\n1\n1\n2\n0\n1\n1\n2\n2\n3\n\n\nH\n1\n2\n2\n2\n2\n2\n2\n1\n1\n0\n2\n3\n1\n3\n\n\nD\n3\n2\n1\n2\n2\n2\n1\n2\n1\n2\n0\n3\n1\n3\n\n\nC\n2\n1\n3\n1\n2\n1\n2\n2\n2\n3\n3\n0\n2\n2\n\n\nK\n2\n1\n2\n2\n3\n3\n2\n2\n2\n1\n1\n2\n0\n3\n\n\nI\n2\n2\n4\n1\n4\n3\n2\n3\n3\n3\n3\n2\n3\n0\n\n\n\nTable 19.4: Geodesic distance matrix for an undirected graph.\n\n\nAs shown in Table 19.4, a node like I, who seems to be at the outskirts of the network, also shows up as having the largest geodesic distances from other nodes in the graph. Other nodes, like E, G, and L seem to be “closer” to others, in terms of having to traverse smaller geodesic distances to reach them.\nThat means that we can use the distance table to come up with a measure of centrality called closeness centrality for each node. We can do that by adding up the entries corresponding to each row in the distance matrix (\\(\\sum_j d_{ij}\\)), to get a summary the total pairwise distances separating the node corresponding to row i in the matrix from the other nodes listed in each column j.\nNote that because closeness is better than “farness,” we would want the node with highest closeness centrality to be the one with the smallest sum of pairwise distances. This can be calculated using the following equation:\n\\[\n  C_i^{CLOS} = \\frac{1}{\\sum_jd_{ij}}\n\\tag{19.7}\\]\nIn Equation 19.7, the denominator is the sum across each column j, for each row i in Table 19.4 which corresponds to the distance between node i and each of the other nodes in the graph j (skipping the diagonal cell when \\(i=j\\), because the geodesic distance of node to itself is always zero!).\nAs noted, we take the mathematical inverse of this quantity, dividing one by the sum of the distances, so that way, the smallest number comes out on top and the bigger number comes out on the bottom (since, as we said, we want to measure closeness not “farness.”)\nLet’s see how this work for the graph in Figure 19.2. First, we get the row sums of geodesic distances from Table 19.4. These are shown in the first column of Table 19.5, under the heading “Sum of Distances.” This seems to work; node \\(E\\) has the smallest number here (\\(\\sum_j d_{Ej} = 22\\)) suggesting it can reach the most nodes via the shortest paths. Node \\(I\\) has the largest number (\\(\\sum_j d_{Ij} = 35\\)) indicating it is the most isolated from the other nodes.\n\n\n\n\n\n \n  \n      \n    Sum of Distances (d) \n    Inverse (1/d) \n    Normalized (N-1/d) \n  \n \n\n  \n    E \n    22 \n    0.045 \n    0.59 \n  \n  \n    A \n    25 \n    0.040 \n    0.52 \n  \n  \n    M \n    28 \n    0.036 \n    0.46 \n  \n  \n    L \n    23 \n    0.043 \n    0.57 \n  \n  \n    B \n    28 \n    0.036 \n    0.46 \n  \n  \n    J \n    23 \n    0.043 \n    0.57 \n  \n  \n    G \n    24 \n    0.042 \n    0.54 \n  \n  \n    N \n    24 \n    0.042 \n    0.54 \n  \n  \n    F \n    23 \n    0.043 \n    0.57 \n  \n  \n    H \n    24 \n    0.042 \n    0.54 \n  \n  \n    D \n    25 \n    0.040 \n    0.52 \n  \n  \n    C \n    26 \n    0.038 \n    0.50 \n  \n  \n    K \n    26 \n    0.038 \n    0.50 \n  \n  \n    I \n    35 \n    0.029 \n    0.37 \n  \n\n\n\nTable 19.5:  Sum of geodesic distances for each node in an undirected graph and its inverse. \n\n\nBut we want closeness, not farness, so the second column of Table 19.5 shows what happens when we divide one by the number in the second column. Now, node \\(E\\) has the largest score \\(CC^{CLOS}_E = 0.045\\) which is what we want.\nHowever, because we are dividing one by a relatively large number, we end up with a bunch of small decimal numbers as centrality scores, and like it happened with degree, this number is sensitive to how big the network is (the larger the network, the more likely there is to be really long short paths). So Freeman (1979) proposes a normalized version of closeness that takes into account network size. It is a variation of Equation 19.7:\n\\[\n  C_i^{CLOS} = \\frac{N-1}{\\sum_jd_{ij}}\n\\tag{19.8}\\]\nEquation 19.8 is the same as Equation 19.7, except that instead of dividing one by the sum of distances, we divide \\(N-1\\) by the sum of distances, where \\(N\\) is the order of the graph (the number of nodes). In this case, \\(N=14\\).\nNormalizing the sum of distances shown in the second column of Table 19.5 according to Equation 19.8, gives us the centrality scores shown in the fourth column of the table, under the heading “Normalized.” These scores range from zero to one, with one being the maximum possible closeness centrality score for that graph.\nThe normalized closeness centrality scores listed in the fourth column of Table 19.5 agree with our informal impressions. Node I comes out at the bottom (\\(CC_I^{CLOS} = 0.37\\)), showing it to be the one with the least closeness centrality, given the relatively large geodesic distances separating it from the other nodes in the graph. Node E (marked red in Figure 19.2) comes out on top (\\(CC_E^{CLOS} = 0.59\\)), given its relative geodesic proximity to other nodes in the graph.\nAs we will see later, having closeness centrality information for nodes in a graph can be useful. For instance, if Figure 19.2 was a social network, and we wanted to spread an innovation or a new product among the actors in the fastest amount of time, we would want to give it to node E first. Note however that if something bad (like a disease) was spreading across the network, then it would also be very bad if actor E got it first!4"
  },
  {
    "objectID": "11-lesson-centrality.html#houston-we-have-a-problem",
    "href": "11-lesson-centrality.html#houston-we-have-a-problem",
    "title": "19  Centrality",
    "section": "19.6 Houston, We Have a Problem",
    "text": "19.6 Houston, We Have a Problem\nSo far, so good. Closeness seems to be a great measure of node importance, giving us a sense of who can reach most others in a network in the most efficient way. However, what would happen if we tried to compute closeness centrality for a disconnected graph like the one shown in Figure Figure 12.2 (b)? Well, the shortest paths distance matrix for that graph looks like the one in Table 19.6.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1\n1\n1\n1\nInf\nInf\nInf\nInf\n\n\nB\n1\n0\n1\n1\n2\nInf\nInf\nInf\nInf\n\n\nC\n1\n1\n0\n1\n1\nInf\nInf\nInf\nInf\n\n\nD\n1\n1\n1\n0\n1\nInf\nInf\nInf\nInf\n\n\nE\n1\n2\n1\n1\n0\nInf\nInf\nInf\nInf\n\n\nF\nInf\nInf\nInf\nInf\nInf\n0\n1\n1\n1\n\n\nG\nInf\nInf\nInf\nInf\nInf\n1\n0\n1\n1\n\n\nH\nInf\nInf\nInf\nInf\nInf\n1\n1\n0\n1\n\n\nI\nInf\nInf\nInf\nInf\nInf\n1\n1\n1\n0\n\n\n\nTable 19.6: Geodesic distance matrix for an undirected, disconnected graph.\n\n\nNote that in Table 19.6, pairs of nodes that cannot reach one another in the disconnected graph, get a geodesic distance of “Inf” (infinity) in the respective cell of the geodesic distance matrix. This is a problem because when we compute the row sums of the geodesic distance matrix to try to calculate centrality according to Equation 19.7, we get the “numbers” shown in Table 19.7.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\nInf\n\n\n\nTable 19.7: Row sums of a geodesic distance matrix from a disconnected graph.\n\n\nSo that’s a bummer since all the “numbers” in Table 19.7, are just infinity. Not to get too philosophical, but the problem is that when you add any number to “infinity,” the answer is, well, infinity.5 This means that closeness centrality is only defined for connected graphs. When it comes to disconnected graphs, we are out of luck.\nThankfully, there is a solution develoed by Beauchamp (1965). It consists of a modification of Equation 19.7 called harmonic closeness centrality. The formula goes as follows:\n\\[\n  C_i^{HARM} = \\frac{1}{N-1}\\sum_j\\frac{1}{d_{ij}}\n\\tag{19.9}\\]\nNow, this might seem like we just re-arranged the stuff in Equation 19.8, and indeed that’s what we did! But the re-arrangement matters a lot, because it changes the order in which we do the various arithmetic operations (Boldi and Vigna 2014).\nSo, in English, while Equation 19.8 says “first sum the geodesic distances for each node (to get the denominator), and then divide \\(N-1\\) by this sum,” Equation 19.9 says “first divide one by the geodesic distance, and then sum the result of all these divisions, and then multiply this sum by one over \\(N-1\\).\nOnce again, the philosophy of mathematical infinity kicks in here, since the main difference is that one divided by infinity is actually a real number: zero.6\nSo let’s check by taking every entry in Table 19.6 and dividing one by the number in each cell (except for the diagonals, which we don’t care about). The results are shown in Table 19.8.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\nA\n0\n1.0\n1\n1\n1.0\n0\n0\n0\n0\n\n\nB\n1\n0.0\n1\n1\n0.5\n0\n0\n0\n0\n\n\nC\n1\n1.0\n0\n1\n1.0\n0\n0\n0\n0\n\n\nD\n1\n1.0\n1\n0\n1.0\n0\n0\n0\n0\n\n\nE\n1\n0.5\n1\n1\n0.0\n0\n0\n0\n0\n\n\nF\n0\n0.0\n0\n0\n0.0\n0\n1\n1\n1\n\n\nG\n0\n0.0\n0\n0\n0.0\n1\n0\n1\n1\n\n\nH\n0\n0.0\n0\n0\n0.0\n1\n1\n0\n1\n\n\nI\n0\n0.0\n0\n0\n0.0\n1\n1\n1\n0\n\n\n\nTable 19.8: Reciprocal of the geodesic distance matrix for an undirected, disconnected graph.\n\n\nBeautiful! Now, instead of weird “Inf”s we have zeroes, which is great because we can add stuff to zero and get a real number back. We can then apply Equation 19.9 to the numbers in Table 19.8 (e.g., computing the sum of each row and then multiplying that by \\(\\frac{1}{N-1}\\)) to get the harmonic closeness centrality for each node in Figure 12.2 (b). These are shown in Table 19.9.\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\n\n\n\n\n0.5\n0.44\n0.5\n0.5\n0.44\n0.38\n0.38\n0.38\n0.38\n\n\n\nTable 19.9: Harmonic Closeness Centrality scores for nodes in a disconnected, undirected graph.\n\n\nGreat! Now we have a measure of closeness centrality we can apply to all kinds of graphs, whether they are connected or disconnected."
  },
  {
    "objectID": "11-lesson-centrality.html#betweenness-centrality",
    "href": "11-lesson-centrality.html#betweenness-centrality",
    "title": "19  Centrality",
    "section": "19.7 Betweenness Centrality",
    "text": "19.7 Betweenness Centrality\nRecall that in our discussion of shortest paths between pair of nodes in the lesson on indirect connections, we noted the importance of the inner nodes that intervene or mediate between a node that wants to reach another one. Nodes that stand in these brokerage or gatekeeper slots in the network, occupy an important position (Marsden 1983), and this is different from having a lot of contacts (like degree centrality), or being able to reach lots of other nodes by traversing relatively small distances (like closeness centrality). Instead, this is about being in-between the indirect communications of other nodes in graph. We can compute a centrality metric for each node called betweenness centrality that captures this idea Freeman (1980).\n\n\n\n\n\nFigure 19.3: An undirected graph showing the node with the maximum betweenness centrality (in red)\n\n\n\n\nFor instance, let’s say you were actor K in the network shown in Figure 19.2, and you wanted to know who is the person that you depend on the most to communicate with actor J. Here dependence means that you are forced to “go through them” if I wanted to reach N via a shortest path. One way K could figure this out is by listing every shortest path having them as the origin node and having N as the destination node. After you have this list, you can see which of other other nodes shows up as an inner node—an intermediary or gatekeeper—in those paths the most times.\nThis shortest path list would look like this:\n\n\\(\\{KH, HF, FJ\\}\\)\n\\(\\{KD, DF, FJ\\}\\)\n\\(\\{KH, HN, NJ\\}\\)\n\\(\\{KA, AC, CJ\\}\\)\n\\(\\{KA, AE, EJ\\}\\)\n\\(\\{KH, HE, EJ\\}\\)\n\nThere are six shortest paths of length three indirectly connecting actors K and J in Figure 19.2), with nodes \\(\\{A, C, D, E, F, H, N\\}\\) showing up as an inner node in at least one of those paths. To see which other actor in the network is the most frequent intermediary between J and K, we can create a list with the number of times each of these nodes shows up as an intermediary in this shortest path list. This would look like this:\n\n\n\n\n\n\n\nNode\nFreq.\nProp.\n\n\n\n\nA\n2\n0.33\n\n\nC\n1\n0.17\n\n\nD\n1\n0.17\n\n\nE\n2\n0.33\n\n\nF\n2\n0.33\n\n\nH\n3\n0.50\n\n\nN\n1\n0.17\n\n\n\nTable 19.10: Intermediaries between nodes J and K\n\n\nSo it looks like, looking at the second column of Table 19.10, that H is the other actor that J depends on the most to reach K. A better way to quantify this, is to actually look at the proportion of paths linking J and K that a particular other node (like H) shows up in. Let’s call this \\(p_{K(H)J}\\) which can be read as “the proportion of paths between K and J featuring H as an inner node.” This is shown in the third column of Table 19.10 We can write this in equation form like this:\n\\[\n  p_{K(H)J} = \\frac{g_{K(H)J}}{g_{KJ}} = \\frac{3}{6} = 0.5\n\\tag{19.10}\\]\nIn Equation 19.10, \\(g_{K(H)J}\\) is the number of shortest paths linking K and J featuring H as an inner node, and \\(g_{KJ}\\) is the total number of paths linking K and J. Freeman (1980) calls this measure the pair-dependency of actor K on actor H to reach a given node J. In this case, \\(g_{K(H)J} = 3\\) and \\(g_{KJ} = 6\\), which means that actor K depends on actor H for fifty percent of their shortest path access to J. Making H the actor in the network J depends on the most to be able to reach J.\nGeneralizing this approach, we can do the same for each triplet of actors i, j, and k in the network. This is the basis for calculating betweenness centrality. That is, we can count the number of times k stands on the shortest path between two other actors i and j. We can all this number \\(g_{i(k)j}\\). We can then divide it by the total number of shortest paths linking actors i and j in the network, which we refer by \\(g_{ij}\\). Remember that two actors can be indirectly linked by multiple shortest paths of the same length, and that we can figure out how many short paths links pairs of actors in the network using the shortest paths matrix.\nThis ratio, written \\(\\frac{g_{i(k)j}}{g_{ij}}\\) then gives us the proportion of shortest paths in the network that have i and j as the end nodes and that feature k as an intermediary inner node. This can range from zero (no shortest paths between i and j feature node k as an intermediary) to one (all the shortest paths between i and j feature node k as an intermediary).\nWe can then use the following equation to compute the average of this proportion for each node k across each pair of actors in the network i and j:\n\\[\n  C_k^{BET} = \\sum_i \\sum_j \\frac{g_{ikj}}{g_{ij}}\n\\tag{19.11}\\]\nComputing this quantity for the graph shown in Figure 19.3, yields the betweenness centrality scores shown in Table 19.11.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nJ\nK\nL\nM\nN\nI\n\n\n\n\n5\n1.5\n3\n6.8\n11.4\n9.3\n6.3\n4.8\n10.8\n3.7\n16.4\n2.8\n5.3\n0\n\n\n\nTable 19.11: Betweenness centrality scores.\n\n\nThe numbers in the Table can be readily interpreted as percentages. Thus, the fact that node J has a a betweenness centrality score of 10.8 tells us that they stand in about 11% of the shortest paths between pairs of nodes in the graph. Interestingly, as shown in Figure 19.3, the node that ends up with the highest betweenness score is L (\\(C_L^{BET} = 16.4\\)), mostly due to the fact that node I, who has the lowest possible betweenness score of zero, depends on this node for access to every other actor in the network.\nNote also that two different nodes end up being ranked first on closeness and betweenness centrality in the same network (compare the red nodes in Figure 19.2 and Figure 19.3). This tells us that closeness and betweenness are analytically distinct measures of node position. One (closeness) gets at reachability, and the other (betweenness) gets at intermediation potential."
  },
  {
    "objectID": "11-lesson-centrality.html#the-big-three-centralities-in-the-star-graph",
    "href": "11-lesson-centrality.html#the-big-three-centralities-in-the-star-graph",
    "title": "19  Centrality",
    "section": "19.8 The Big Three Centralities in the Star Graph",
    "text": "19.8 The Big Three Centralities in the Star Graph\nDegree, Closeness, and Betweenness centralities have an interesting property that provides a conceptual connection between them (Freeman 1979). Consider the star graph shown in Figure 19.1 with central node A. The degree, closeness, and betweenness centralities of the different nodes are shown in Table 19.12).\nOf course, by definition, we know beforehand that the central node in a star graph has to have the highest degree, since the degree of peripheral nodes is fixed to one and the degree of the central node is always \\(n-1\\), where \\(n\\) is the graph order.\nHowever, note also that the central node has to have the highest closeness, since it is directed by a path of length one (and edge) to every peripheral node, but each peripheral node can only reach other peripheral nodes in the graph by a path of length two. They are farther away from other nodes than the central node.\nFinally, note that the central node in the star will also always have the highest betweenness because each of the paths of length two connecting every pair of peripheral nodes to one another has to include the central node. So it serves as the intermediary between any communication between peripheral nodes.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nDegree\n6.0\n1.0\n1.0\n1.0\n1.0\n1.0\n1.0\n\n\nCloseness\n8.2\n4.5\n4.5\n4.5\n4.5\n4.5\n4.5\n\n\nBetwenness\n15.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\nTable 19.12: Centralities in a star graph of order 7.\n\n\nThe mathematical sociologist Linton Freeman (1979) thus thinks that the “big three” centrality measures are the big three precisely because they are maximized for the central node in a star graph."
  },
  {
    "objectID": "11-lesson-centrality.html#references",
    "href": "11-lesson-centrality.html#references",
    "title": "19  Centrality",
    "section": "References",
    "text": "References\n\n\n\n\n\n\nBavelas, Alex. 1950. “Communication Patterns in Task-Oriented Groups.” The Journal of the Acoustical Society of America 22 (6): 725–30.\n\n\nBeauchamp, Murray A. 1965. “An Improved Index of Centrality.” Behavioral Science 10 (2): 161–63.\n\n\nBoldi, Paolo, and Sebastiano Vigna. 2014. “Axioms for Centrality.” Internet Mathematics 10 (3-4): 222–62.\n\n\nBorgatti, Stephen P. 2005. “Centrality and Network Flow.” Social Networks 27 (1): 55–71.\n\n\nBorgatti, Stephen P, and Martin G Everett. 2006. “A Graph-Theoretic Perspective on Centrality.” Social Networks 28 (4): 466–84.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35–41.\n\n\n———. 1979. “Centrality in Social Networks Conceptual Clarification.” Social Networks 1 (3): 215–39.\n\n\n———. 1980. “The Gatekeeper, Pair-Dependency and Structural Centrality.” Quality and Quantity 14 (4): 585–92.\n\n\nMarsden, Peter V. 1983. “Restricted Access in Networks and Models of Power.” American Journal of Sociology 88 (4): 686–717.\n\n\nNieminen, Juhani. 1974. “On the Centrality in a Graph.” Scandinavian Journal of Psychology 15 (1): 332–36.\n\n\nSabidussi, Gert. 1966. “The Centrality Index of a Graph.” Psychometrika 31 (4): 581–603."
  },
  {
    "objectID": "12-lesson-groups-cliques.html#categories-everywhere",
    "href": "12-lesson-groups-cliques.html#categories-everywhere",
    "title": "20  Clique Analysis",
    "section": "20.1 Categories Everywhere",
    "text": "20.1 Categories Everywhere\nA common feature of social life is to divide people into categories. For instance, there are ethnoracial categories (like Black, Asian, Hispanic, Pacific Islander), there are ethnonational categories (Philipino, Puerto Rican, Taiwanese, Nigerian, etc.), ethnolinguistic categories (based on the language(s) you speak), and ethnoreligious categories (based on the religion you practice). People also divide themselves into gender identity or sexual orientation categories. Sociologists like to divide up people based on things like categories based on such markers of “social position” like occupation, education, and social class. In the news, sometimes you hear about battles between “Millennials” (who kill everything) and out of touch “Boomers” (who hate everything), with sarcastic “Gen-Exers” in between. This is nothing but a division of people into categories based on age and generation (sometimes also referred to as cohort). The possibilities are endless!\nWhy are sociologists (and people) so obsessed with dividing up people into categories? Well, the basic idea is that category labels provide a mechanism to explain group formation, and they allow for sociologists to get a sense of the position people occupy in society. For instance, following the principle of homophily discussed in the lesson of ego networks, we may surmise that race, gender, and generation serve as principles of group formation (so that groups tend to be homogeneous with respect to age, gender, and generation). We may also say that a stockbroker who makes 500K a year and lives in Malibu occupies a very different social position than a high-school teacher who makes 50K a year and lives in the valley."
  },
  {
    "objectID": "12-lesson-groups-cliques.html#the-anti-categorical-imperative",
    "href": "12-lesson-groups-cliques.html#the-anti-categorical-imperative",
    "title": "20  Clique Analysis",
    "section": "20.2 The Anti-Categorical Imperative",
    "text": "20.2 The Anti-Categorical Imperative\nSocial network analysis rejects the idea that the best way to divide up people into categories is to use pre-existing labels. This penchant has been labeled (riffing on the philosopher Immanuel Kant) the anti-categorical imperative (Emirbayer and Goodwin 1994). It is not that social network analysts necessarily reject the idea that people can be divided up into groups or that we should try to get a sense of the social position that people occupy. Instead, what network analysts insist on is that we should use the pattern of interconnections in social networks to come up with groups and assign people to social positions.\nAt its simplest, then, finding groups and assigning nodes to social position boil down to the same thing, which is to take the initial set of nodes in the graph and divide them up into (some times mutually exclusive, sometimes overlapping) subsets, so that nodes in the same subset belong to the same group or occupy the same social position in the network.\nIn network terms, then, what is the difference between being in the same group and occupying the same social position? Just like with the traditional sociological conceptions, the key difference is that belonging to the same group logically implies that you share a lot of connections with the other people in your group (e.g., a group of friends is necessarily linked to one another). However, being in the same social position does not require that you are connected to others in your same position. For instance, you can be doctor in New York City and share the same social position with a doctor in Los Angeles without necessarily being connected to that person. So in network analysis, nodes in a network who share the same position have similar patterns of connectivity with others without necessarily being connected to one another (although they could be!).\nSo the group and the social position imagery leads to two distinct ways of partitioning (a fancy word for “splitting”) the nodes in a graph (representing a social network) into subsets. One based on whether the subsets are strongly interconnected among themselves (the group approach) or whether the subsets have similar patterns of connectivity with others (the position approach). While the group approach is global (it uses information from the connectivity structure of the whole graph), the position approach is local (it uses information from the local neighborhoods of each node to assign them to categories). They are many versions of the group approach, but only two main versions of the position approach. In this lesson, we will cover the main varieties of the group approach. The next two lessons deals with varieties of the position approach."
  },
  {
    "objectID": "12-lesson-groups-cliques.html#the-group-approach",
    "href": "12-lesson-groups-cliques.html#the-group-approach",
    "title": "20  Clique Analysis",
    "section": "20.3 The Group Approach",
    "text": "20.3 The Group Approach\nThe basic idea behind the group approach is to find densely connected subgraphs in the original graph. Densely connected subgraphs are the social network equivalent of “groups” in the real world (Freeman 1992). Just like in the real world, people can belong to more than one group at a time.\nThe most densely connected subgraph in a graph is a subgraph with density equal to one. This is a complete subgraph of the larger graph. A complete subgraph of a larger graph is called a clique (Luce and Perry 1949). The identity of a clique is given by the nodes that are inside the clique. So when say that the set of nodes \\(\\{A, B, C, D\\}\\) is a clique of size four with nodes A, B, C, and D inside of it. For instance, Figure 20.1 shows a clique of size four with nodes \\(\\{A, B, C, D\\}\\) as members.\n\n\n\n\n\nFigure 20.1: A clique of size four.\n\n\n\n\nCliques come in different sizes. A clique of size four is a maximally complete subgraph with four nodes in it. A complete subgraph of order five would yield a clique of size five and so forth. A subgraph is maximal for a given property (like being complete) if adding one more node to the subgraph gets rid of the property. For instance, a maximally complete subgraph of order four (a clique of size four) means that adding on more node to the subgraph would make its density drop below one, and thus the subgraph of order five is no longer complete.\nNote that, technically, the closed triad we considered in the lesson on dyads and triads is a clique of size three!\n\n\n\n\n\nFigure 20.2: An undirected graph with four cliques of size 4.\n\n\n\n\nFigure 20.2 shows a graph containing four separate cliques of size four, with nodes who belong to these four cliques highlighted in different colors. Nodes \\(\\{A, B, C, D\\}\\) belong to one clique, nodes \\(\\{E, F, G, H\\}\\), to another, nodes \\(\\{I, J, K, L\\}\\) to yet another clique, and nodes \\(\\{M, N, O, P\\}\\) to a final clique of size four.\nNodes can belong to multiple cliques at once. This is for two reasons:\n\nFirst, cliques of smaller size are nested within cliques of larger size. So inside a clique of size four like those formed by nodes \\(\\{A, B, C, D\\}\\) in Figure 20.2), there are four separate cliques of size three: \\(\\{A, B, C\\}\\), \\(\\{B, C, D\\}\\), \\(\\{A, B, D\\}\\), and \\(\\{A, C, D\\}\\). So that means that each node in a clique of size four technically belongs four distinct cliques! The larger clique of size four and the three smaller cliques of size three.\nSecond, nodes can be in multiple cliques is that the same node can be shared by multiple cliques even if the cliques are not nested. This situation is shown in Figure 20.3 where node D is shared by cliques \\(\\{A, B, C, D\\}\\) and \\(\\{D, E, F, G, H\\}\\). Using the set theory lingo we resorted to talk about node neighborhoods, another way of saying this is that the nodes shared by two cliques is given by the intersection of their set of members. So in the example shown in Figure 20.3:\n\n\\[\n\\{D, E, F, G, H\\} \\cap \\{A, B, C, D\\} = D\n\\]\n\n\n\n\n\nFigure 20.3: An undirected graph with two cliques, one of size four and the other one of size 5.\n\n\n\n\nWhen studying a social network, we may be interested in finding how many cliques are inside of it. This means enumerating the full set of cliques above a certain minimum clique size. For instance, the total number of cliques of size four or larger in the graph shown in Figure 20.4 goes as follows:\n\n\\(\\{D, E, F, G, H\\}\\)\n\\(\\{D, E, G, H\\}\\)\n\\(\\{I, J, K, L\\}\\)\n\\(\\{A, B, C, D\\}\\)\n\\(\\{E, F, G, H\\}\\)\n\\(\\{D, F, G, H\\}\\)\n\\(\\{D, E, F, G\\}\\)\n\\(\\{D, E, F, H\\}\\)\n\nSo there are eight cliques of size four or larger in the graph!\n\n\n\n\n\nFigure 20.4: An undirected graph. How many cliques of size four or larger are there?"
  },
  {
    "objectID": "12-lesson-groups-cliques.html#cliques-as-affliation-networks",
    "href": "12-lesson-groups-cliques.html#cliques-as-affliation-networks",
    "title": "20  Clique Analysis",
    "section": "20.4 Cliques as Affliation Networks",
    "text": "20.4 Cliques as Affliation Networks\nThere is a connection between cliques and affiliation networks. Recall from the lesson affiliation networks that we use these types of networks, represented using bipartite graphs whenever we want to highlight the linkages between people and groups. Well, cliques are groups, which means that the network formed by arranging people by their membership in cliques is an affiliation network!\n\n\n\n\n\n\n\n\nC1\nC2\nC3\nC4\nC5\nC6\nC7\nC8\n\n\n\n\nA\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nB\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nC\n0\n0\n0\n1\n0\n0\n0\n0\n\n\nD\n1\n1\n0\n1\n1\n0\n1\n1\n\n\nE\n1\n1\n0\n0\n0\n1\n1\n1\n\n\nF\n1\n0\n0\n0\n1\n1\n1\n1\n\n\nG\n1\n1\n0\n0\n1\n1\n1\n0\n\n\nH\n1\n1\n0\n0\n1\n1\n0\n1\n\n\nI\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nJ\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nK\n0\n0\n1\n0\n0\n0\n0\n0\n\n\nL\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n\nTable 20.1: Person by clique affiliation matrix.\n\n\n\n\n\n\n\nFigure 20.5: Bipartite graph of clique affliations.\n\n\n\n\nTable 20.1 shows the Clique Affiliation Matrix (\\(C\\)) with people in the rows (i) and all the cliques of size four or larger in Figure 20.4 we listed previously (j) in the columns. Each cell entry \\(C_{ij}\\) in the clique affiliation matrix is set to one if node i belongs to clique j. Otherwise it is set to zero.\nAs we can see, some nodes (like node A) belong to only one clique, but other nodes, like node D belongs to six cliques! This is consistent with the idea that while some people belong to just a few groups, other over-committed people belong to multiple groups. Figure 20.5) shows the corresponding bipartite graph displaying the clique affiliations in the network. People are shown as circles and cliques are shown as triangles.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nB\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nC\n1\n1\n1\n1\n0\n0\n0\n0\n0\n0\n0\n0\n\n\nD\n1\n1\n1\n6\n4\n4\n4\n4\n0\n0\n0\n0\n\n\nE\n0\n0\n0\n4\n5\n4\n4\n4\n0\n0\n0\n0\n\n\nF\n0\n0\n0\n4\n4\n5\n4\n4\n0\n0\n0\n0\n\n\nG\n0\n0\n0\n4\n4\n4\n5\n4\n0\n0\n0\n0\n\n\nH\n0\n0\n0\n4\n4\n4\n4\n5\n0\n0\n0\n0\n\n\nI\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n\n\nJ\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n\n\nK\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n\n\nL\n0\n0\n0\n0\n0\n0\n0\n0\n1\n1\n1\n1\n\n\n\nTable 20.2: Clique co-membership matrix.\n\n\nJust like with regular affiliation networks, we can compute the matrix multiplication product of the clique affiliation matrix \\(C\\) times its transpose \\(C'\\). This gives us the clique co-membership matrix (\\(M\\)).\n\\[\n  M = C \\times C'\n\\tag{20.1}\\]\nThe off-diagonal cells of the clique co-membership matrix tell us the number of common cliques nodes i and j share, and the diagonal cells tell us the number of clique that each node belongs to. This is shown in Table 20.2).\n\n\n\n\n\n\n\n\nC1\nC2\nC3\nC4\nC5\nC6\nC7\nC8\n\n\n\n\nC1\n5\n4\n0\n1\n4\n4\n4\n4\n\n\nC2\n4\n4\n0\n1\n3\n3\n3\n3\n\n\nC3\n0\n0\n4\n0\n0\n0\n0\n0\n\n\nC4\n1\n1\n0\n4\n1\n0\n1\n1\n\n\nC5\n4\n3\n0\n1\n4\n3\n3\n3\n\n\nC6\n4\n3\n0\n0\n3\n4\n3\n3\n\n\nC7\n4\n3\n0\n1\n3\n3\n4\n3\n\n\nC8\n4\n3\n0\n1\n3\n3\n3\n4\n\n\n\nTable 20.3: Clique overlap matrix.\n\n\nWe can compute the matrix multiplication product of the transpose of the clique affiliation matrix \\(C'\\) times the original matrix \\(C\\). This gives us the clique overlap matrix (\\(O\\)).\n\n\n\n\n\nFigure 20.6: Clique graph.\n\n\n\n\n\\[\nO = C' \\times C\n\\tag{20.2}\\]\nThe off-diagonal cells of the clique overlap matrix tell us the number of common members cliques i and j share, and the diagonal cells tell us size of each clique. This is shown in Table 20.3). Note that clique three is an isolate in the clique graph, as it shares no members with other cliques.\nFrom this information we can construct what Everett and Borgatti (1998) call the Clique Graph. This is a weighted graph with cliques as the nodes and the edges weighted by the number of people shared by each clique. The clique graph corresponding to the clique affiliation matrix in Table 20.1) is shown in Figure 20.6)."
  },
  {
    "objectID": "12-lesson-groups-cliques.html#n-cliques",
    "href": "12-lesson-groups-cliques.html#n-cliques",
    "title": "20  Clique Analysis",
    "section": "20.5 N-Cliques",
    "text": "20.5 N-Cliques\nConsider the network represented by the graph shown in Figure 20.7). In this figure, it is clear that the set of nodes \\(\\{E, F, G, H\\}\\) are members of a clique of size four. But what about the set of nodes \\(\\{A, B, C, D\\}\\)? I mean, they do look “groupy” but they technically do not meet the requirements of a sociometric clique. The reason is that the subgraph formed by nodes \\(\\{A, B, C, D\\}\\) is not complete (it is missing the edge \\(AC\\)) and thus has a density below one. However, we still have a strong intuition that there are pretty close to being a group.\n\n\n\n\n\nFigure 20.7: A graph featuring n-cliques.\n\n\n\n\nThe mathematician Duncan Luce Luce (1950) had the same intuition and that’s why he developed the notion of an n-clique. The basic idea is simple. Instead of defining a group based on a complete subgraph, we can define a group based on a subgraph with a (minimum) desired level of indirect connectivity (see the lesson on indirect connectivity), which we denote as n. For instance, we can say that any subset of nodes where every node in the subset is connected to every other node in the subset by a path of length two or smaller (which means a direct connection as an edge is a “path” of length one) form a 2-clique (\\(n = 2\\)).\nNote that the subgraph formed by the nodes \\(\\{A, B, C, D\\}\\) meets this criterion: While node A is not connected to node C, it can reach node C by two paths of length two (either \\(\\{AD, DC\\}\\) or \\(\\{AB, BC\\}\\)). Note that adding any other node to the subgraph (e.g., J, or H) breaks this property and no longer makes it a 2-clique. This means that the subgraph \\(\\{A, B, C, D\\}\\) is maximal for the property of being a 2-clique\nCan you see other subset of nodes in the graph that also form a 2-clique?1\nOf course we can keep on going an make up even weaker and more relaxed definitions of a group based on the idea of indirect connectivity (e.g., \\(n = 3\\), \\(n = 4\\), and so forth). For instance, we can define a 3-clique as any subgraph in which nodes are separated by a minimum of three steps (a path of length three). For instance, the hexagonal “ring” formed by the subgraph containing nodes \\(\\{A, B, G, H, J, L\\}\\) in Figure 20.7) is such a 3-clique. Every actor can reach every other actor in the subgraph via a path of length 3 or smaller. Note that while this set of actors still looks kind of groupy (in a ring around the rosie kind of way) it does not look as much like a group as a 2-clique.\nOf course, it would be silly to keep on going with larger values of n, since the subgraphs so defined would be so loosely connected as to no longer count as groups. That’s why when defining n-cliques, values of \\(n\\) in the range of two or three are the most commonly used."
  },
  {
    "objectID": "12-lesson-groups-cliques.html#references",
    "href": "12-lesson-groups-cliques.html#references",
    "title": "20  Clique Analysis",
    "section": "References",
    "text": "References\n\n\n\n\nEmirbayer, Mustafa, and Jeff Goodwin. 1994. “Network Analysis, Culture, and the Problem of Agency.” American Journal of Sociology 99 (6): 1411–54.\n\n\nEverett, Martin G, and Stephen P Borgatti. 1998. “Analyzing Clique Overlap.” Connections 21 (1): 49–61.\n\n\nFreeman, Linton C. 1992. “The Sociological Concept of\" Group\": An Empirical Test of Two Models.” American Journal of Sociology 98 (1): 152–66.\n\n\nLuce, R Duncan. 1950. “Connectivity and Generalized Cliques in Sociometric Group Structure.” Psychometrika 15 (2): 169–90.\n\n\nLuce, R Duncan, and Albert D Perry. 1949. “A Method of Matrix Analysis of Group Structure.” Psychometrika 14 (2): 95–116."
  },
  {
    "objectID": "13-lesson-positions-struct-equiv.html#the-position-approach",
    "href": "13-lesson-positions-struct-equiv.html#the-position-approach",
    "title": "21  Equivalence and Similarity",
    "section": "21.1 The Position Approach",
    "text": "21.1 The Position Approach\nThe basic idea behind the position approach to dividing up the nodes in a graph is to come up with a measure of how similar two nodes are in terms of their patterns of connectivity with others. This measure then can be used to partition the nodes into what are called equivalence or similarity classes. Nodes in the same equivalence class are said to occupy the same position in the social structure described by the network.\nThere are two main ways to partition nodes into equivalence classes. The first is based on the idea that two nodes occupy the same position is they have similar patterns of connectivity to the same other nodes in the graph. This is called structural equivalence.\nThe second is based on the idea that two nodes are equivalent if they are connected to people who are themselves equivalent, even if these are not literally the same people. This is called regular equivalence.\nThis lesson will deal mainly with various ways of partitioning the nodes in a network based on structural equivalence (Section 21.2) and its more relaxed cousin, structural similarity."
  },
  {
    "objectID": "13-lesson-positions-struct-equiv.html#sec-equiv",
    "href": "13-lesson-positions-struct-equiv.html#sec-equiv",
    "title": "21  Equivalence and Similarity",
    "section": "21.2 Structural Equivalence",
    "text": "21.2 Structural Equivalence\nTwo nodes are structurally equivalent if they are connected to the same others. Thus, their patterns of connectivity (e.g., their row in the adjacency matrix) is exactly the same.\n\n\n\n\n\nFigure 21.1: An undirected graph with nodes colored by membership in the same structural equivalence class.\n\n\n\n\nFor instance in Figure 21.1, nodes C and D are structurally equivalent because they are connected to the same neighbors \\(\\{A, B, E\\}\\). In the same way, nodes A and B are structurally equivalent because they are connected to the same neighbors \\(\\{C, D\\}\\). Finally, nodes E and F occupy unique positions in the network because their neighborhoods are not equivalent to that of any other nodes. Node E is the only node that has a neighborhood composed of nodes \\(\\{C, D, F\\}\\), and node F is the only node that has a neighborhood composed of node \\(\\{E\\}\\) only. Perhaps F is the main boss, and E is the second in command.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    B \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n  \n    C \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    D \n    1 \n    1 \n    0 \n    0 \n    1 \n    0 \n  \n  \n    E \n    0 \n    0 \n    1 \n    1 \n    0 \n    1 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n  \n\n\n\nTable 21.1:  Adjancency Matrix of an Undirected Graph \n\n\nWe can also see by looking at Table 21.1) that, indeed, the rows corresponding to the structurally equivalent nodes \\(\\{A, B\\}\\) and \\(\\{C, D\\}\\) in the corresponding adjacency matrix are indistinguishable from one another. The nodes that have unique positions in the network \\(\\{E, F\\}\\), also have a unique pattern of 0s and 1s across the rows of the adjacency matrix.\n\n21.2.1 Structural Similarity\nIn most real-world applications, the standard definition of structural equivalence is much too strong. In answer to the question of whether two nodes occupy the same position in the network it only allows for a “yes/no” answer. Yes, if their neighborhoods are exactly the same, and “no” if there aren’t.\nWhat we need is a measure of position that allows for “more or less” rather than “yes” and “no.” This is what is called structural similarity (Leicht, Holme, and Newman 2006). Two nodes are structurally similar if they have similar patterns of connectivity with the same others. There are various versions of structural similarity between nodes. Here we will consider some popular ones."
  },
  {
    "objectID": "13-lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-euclidian-distance",
    "href": "13-lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-euclidian-distance",
    "title": "21  Equivalence and Similarity",
    "section": "21.3 Measuring Structural Similarity Using the Euclidian Distance",
    "text": "21.3 Measuring Structural Similarity Using the Euclidian Distance\nGiven an adjacency matrix for a graph, how can we find out which nodes are structurally similar without staring at a picture for a long time? A classic way of measuring structural similarity, developed by the sociologist Ronald Burt Burt (1976) is to use the Euclidean Distance between the row vectors corresponding to each node in an adjacency matrix.\nTake for instance Table 21.1. The row vector for node A is \\(a_{(A)j} = (0, 0, 1, 1, 0, 0)\\), and so is the row vector for node B \\(a_{(B)j}\\), because we already know they are structurally equivalent! Remember, the row vector is just the entries in each row corresponding to each node in Table 21.1. So the row vector for node C (\\(a_{(C)j}= (1, 1, 0, 0, 1, 0)\\) and so forth. Here the subscript \\(j\\) refers to each column entry of the adjacency matrix \\((A, B, C...F)\\).\nThe Euclidean Distance between the row row-vectors of two nodes \\(k\\) and \\(l\\) is given by:\n\\[\nd^{Euclid}_{k,l} = \\sqrt{\\sum_j (a_{(k)j}-a_{(l)j})^2}\n\\tag{21.1}\\]\nWhat equation Equation 21.1 says is that we take each corresponding entry of the row vectors, subtract them from one another, square them, sum them, and take the square root of the resulting sum. As noted, structurally equivalent nodes will receive a score of zero, while structurally similar nodes will receive scores that are close to zero. The larger the Euclidean distance between two nodes, the less structurally similar they are.\nSo let’s say we wanted to find out the structural similarity between between nodes A and C in Table 21.1 using the Euclidean distance. We would proceed as follows:\n\nFirst, get the row vector for node A that’s \\(a_{(A)j} = (0, 0, 1, 1, 0, 0)\\), as we saw earlier.\nSecond, get the row vector for node C that’s \\(a_{(C)j}= (1, 1, 0, 0, 1, 0)\\), as we saw earlier.\nThird, line them up, so that you can compute the differences and then square them:\n\n\n\n\n\n\n\n\nA\n0\n0\n1\n1\n0\n0\n\n\nC\n1\n1\n0\n0\n1\n0\n\n\nA - C\n(0 - 1)\n(0 - 1)\n(1 - 0)\n(1 - 0)\n(0 - 1)\n(0 - 0)\n\n\nA - C\n-1\n-1\n1\n1\n-1\n0\n\n\n(A - C)^2\n1\n1\n1\n1\n1\n0\n\n\n\nTable 21.2: Euclidean distance calculation.\n\n\nThe Euclidean distance between A and C is thus the square root of the sum of the numbers in the last row of Table 21.2: \\(\\sqrt{1+1+1+1+1+0} = \\sqrt{5}= 2.2\\).\nAs we noted, for structurally equivalent node pairs (which have identical row vectors), the Euclidean distance should reach its minimum value of zero. We can check that by computing the Euclidean distance of nodes A and B in Table 21.1:\n\n\n\n\n\n\n\nA\n0\n0\n1\n1\n0\n0\n\n\nB\n0\n0\n1\n1\n0\n0\n\n\nA-C\n0\n0\n0\n0\n0\n0\n\n\n(A-C)^2\n0\n0\n0\n0\n0\n0\n\n\n\nTable 21.3: Euclidean distance calculation.\n\n\nIndeed since the sum of the last row of the numbers in Table 21.3 is zero, then so will the square root!\n\n21.3.1 The Structural Similarity Matrix\nTable 21.4 shows the structural similarity matrix (\\(\\mathbf{S}\\)) produced by computing euclidean distance between of each pair of nodes in the adjacency matrix shown in Table 21.1 (corresponding to the graph shown in Figure 21.1) according to Equation 21.1.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n0\n2.2\n2.2\n1\n1.7\n\n\nB\n0\n–\n2.2\n2.2\n1\n1.7\n\n\nC\n2.2\n2.2\n–\n0\n2.4\n1.4\n\n\nD\n2.2\n2.2\n0\n–\n2.4\n1.4\n\n\nE\n1\n1\n2.4\n2.4\n–\n2\n\n\nF\n1.7\n1.7\n1.4\n1.4\n2\n–\n\n\n\nTable 21.4: Structural Equivalence matrix for an undirected graph.\n\n\nIn the \\(\\mathbf{S}\\) matrix, each cell \\(\\mathbf{s}_{ij}\\) gives us the Euclidean distance between nodes i and j. Note that the \\(\\mathbf{S}\\) matrix is symmetric, meaning that the same information is contained in the lower and upper triangles (\\(\\mathbf{s}_{ij}= \\mathbf{s}_{ji}\\)). This makes sense, because the distance between point a and point b should be the same as the distance between point b and point a. In the same way, if you are similar to your friend, then your friend should be equally similar to you!\nChecking the values in Table 21.4, we can see that structurally equivalent pairs of nodes—are connected to exactly the same others, like nodes A and B or nodes C and D—have similarity \\(\\mathbf{d}_{ji}= 0\\) in the matrix based on the Euclidian distnce. As nodes become less similar—are connected to different others like nodes E and C—their Euclidean distance becomes larger.\nSo, we can say that structurally similar nodes occupy the same position in the network. So A and B occupy the same position, and so do C and D.\n\n\n21.3.2 Euclidian Distance in Directed Graphs\nSo far we have considered the case of structural similarity for undirected graphs composed of symmetric ties.\nBut what happens when the network we are studying is composed of asymmetric ties?\nWell, in the directed graph case, we have to distinguish between two ways nodes in a graph can be structurally similar to one another based on the directionality of the ties we are considering.\n\nIn the first case, two nodes are structurally similar if they send ties to the same others.\nIn the second case, two nodes are structurally similar if they receive ties from the same others.\n\nThese don’t necessarily have to go together. One node (A) can send ties to the same others as another node (B) (and thus A and B can be structurally similar when it comes to their out-neighbors), but receive ties from a different set of others (and thus A and B can fail to be structurally similar in terms of their in-neighbors)\nThis means that in the directed graph case, two nodes are structurally equivalent if and only if they send ties to the same others and receive ties from the same others.\nThis added complication means that we have to modify the way we measure structural similarity in the directed graph case when using the euclidian distance. Particularly, to consider the distance between pairs of nodes, we now have to consider both their row-vectors (capturing the pattern of sending ties) and their column vectors (capturing the patterns of their receiving ties) in calculating the distance.\nThis means Equation 21.1 now turns into:\n\\[\n  d^{Euclidean}_{k,l} = \\sqrt{\\sum_j (a_{(k)j}-a_{(l)j})^2 + \\sum_i (a_{(k)i}-a_{(l)i})^2}\n\\tag{21.2}\\]\nThe first part of Equation 21.2 inside the square root operator is just like Equation 21.1: \\(\\sum_j (a_{(k)j}-a_{(l)j})^2\\). In this case, this tracks the Euclidean distance between the respective row vectors of nodes k and l (which is we sum across the columns j), which captures the extent to which they send links to the same others. When this part of the equation equals zero, it means k and l are structurally equivalent when it comes to sending ties.\nThe second part that is added is \\(\\sum_j (a_{(k)i}-a_{(l)i})^2\\), which measures the Euclidean distance between the column vectors of nodes k and l in the asymmetric adjacency matrix (which is why we sum across the rows i); this captures the extent to which k and l receive ties from the same others. When this part of the equation equals zero, it means k and l are structurally equivalent when it comes to receiving ties.\n\n\n\n\n\nFigure 21.2: A directed graph\n\n\n\n\nLet’s look at an example. Consider the graph shown in Figure 21.2. In the graph, nodes rendered in the same color are structurally equivalent according to Equation 21.2. The corresponding asymmetric adjacency matrix for the graph in Figure 21.2 is shown in Table 21.5.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n1\n1\n1\n0\n1\n0\n\n\nB\n1\n–\n1\n1\n0\n1\n0\n\n\nC\n1\n0\n–\n0\n0\n0\n0\n\n\nD\n1\n1\n0\n–\n1\n0\n1\n\n\nE\n0\n0\n1\n0\n–\n1\n0\n\n\nF\n1\n0\n0\n0\n0\n–\n0\n\n\nG\n0\n0\n1\n0\n0\n1\n–\n\n\n\nTable 21.5: Asymmetric adjacency matrix for a directed graph.\n\n\nLet’s say we wanted to figure out whether nodes E and G are structurally equivalent (they are). First, we would compare their respective row-vectors in the adjacency matrix:\n\n\n\n\nTable 21.6: Row vectors for nodes E and G.\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nE\n0\n0\n1\n0\n–\n1\n0\n\n\nG\n0\n0\n1\n0\n0\n1\n–\n\n\n\n\n\n\nThen we would compare their respective column vectors:\n\n\n\n\nTable 21.7: Column vectors for nodes E and G.\n\n\n\nE\nG\n\n\n\n\nA\n0\n0\n\n\nB\n0\n0\n\n\nC\n0\n0\n\n\nD\n1\n1\n\n\nE\n–\n0\n\n\nF\n0\n0\n\n\nG\n0\n–\n\n\n\n\n\n\nAnd indeed, they are both the same! When we compute the structural similarity matrix based on the Euclidean distance for the directed graph shown in Figure 21.2 using Equation 21.2, we end up with:\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\n\n\n\n\nA\n–\n3.4\n4.6\n4.4\n3.1\n4.6\n3.1\n\n\nB\n3.4\n–\n3.7\n3.8\n2.4\n3.7\n2.4\n\n\nC\n4.6\n3.7\n–\n3.1\n3.9\n0\n3.9\n\n\nD\n4.4\n3.8\n3.1\n–\n4.1\n3.1\n4.1\n\n\nE\n3.1\n2.4\n3.9\n4.1\n–\n3.9\n0\n\n\nF\n4.6\n3.7\n0\n3.1\n3.9\n–\n3.9\n\n\nG\n3.1\n2.4\n3.9\n4.1\n0\n3.9\n–\n\n\n\nTable 21.8: Structural Equivalence matrix for an undirected graph.\n\n\nWhich tells us that nodes E and G in Figure 21.2 are structurally equivalent \\(d_{E,G} = 0\\), but so are nodes C and F. These two pair of nodes send ties to the same out-neighbors and receive ties from the same in-neighbors.1 So, we can say that nodes E and G occupy one position in the network and nodes C and F occupy another position. Perhaps if this were an office, and the relation was one of advice, this would reveal two set of actors who have a similar role in the office network."
  },
  {
    "objectID": "13-lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-neighborhood-overlap",
    "href": "13-lesson-positions-struct-equiv.html#measuring-structural-similarity-using-the-neighborhood-overlap",
    "title": "21  Equivalence and Similarity",
    "section": "21.4 Measuring Structural Similarity Using the Neighborhood Overlap",
    "text": "21.4 Measuring Structural Similarity Using the Neighborhood Overlap\nAs we noted in the original graph theory lesson, it is possible for the neighborhood of two nodes in a graph to overlap. Recall that for each node, we define its neighborhood as the set of other nodes that they are adjacent to. That means the neighborhood between two nodes can have members in common. The more members they have in common the more structurally similar two nodes are.\n\n\n\n\n\nFigure 21.3: An undirected graph.\n\n\n\n\nFor instance, imagine you have a friend and that friend knows all your friends and you know all their friends. In which case we would say that the overlap between your node neighborhoods is pretty high; in fact the two neighborhoods overlap completely, which makes you structurally equivalent! But even if your friend knows 90% of the people in your network (and you know 90% of the people in their network) that would make you very structurally similar to one another.\nNow imagine you just met a new person online who lives in a far away country, and as far as you know, they know none of your friends and you know none of their friends. In which case, we would say that the overlap if the two neighborhoods is nil or as close to zero as it can get. You occupy completely different positions in the network.\n\n21.4.1 Jaccard Similarity\nWe can use this reasoning to construct a measure of structural similarity between two nodes called Jaccard’s Similarity Coefficient (\\(J_{ij}\\)). It goes like this (Jaccard 1901). Let’s say \\(n_{ij}\\) is the number of friends that nodes i and j have in common, and the total number of i’s friends if \\(k_i\\) (i’s degree) and the total number of j’s friends if \\(k_j\\) (j’s degree). Then the structural similarity of i and j is given by:\n\\[\n  J_{ij} = \\frac{n_{ij}}{k_i + k_j - n_{ij}}\n\\tag{21.3}\\]\nIt says that the structural similarity of two nodes is equivalent to the number of friends that the two persons know in common, divided by the sum of their degrees minus the number of people they know in common. Jaccard’s coefficient ranges from zer (when \\(n_{ij}=0\\) and the two nodes have no neighbors in common) to 1.0 (when \\(n_{ij} = k_i\\) and \\(n_{ij} = k_j\\) and the two nodes are structurally equivalent).\n\n\n21.4.2 Dice Similarity\nA second measure of structural similarity between nodes based on the neighborhood overlap is the Dice Similarity Index. It goes like this (Dice 1945):\n\\[\n  D_{ij} = \\frac{2n_{ij}}{k_i + k_j}\n\\tag{21.4}\\]\nWhich says that the structural similarity between two nodes is equivalent to the twice the number of people the know in common, divided by the sum of their degrees.\n\n\n21.4.3 Cosine Similarity\nA third and final measure of structural similarity between two nodes based on the neighborhood overlap is the cosine similarity between their respective neighborhoods (\\(C_{ij}\\)). This is given by:\n\\[\n  C_{ij} = \\frac{n_{ij}}{\\sqrt{k_ik_j}}\n\\tag{21.5}\\]\nWhich says that the structural similarity between two nodes is equivalent to the number of people they know in common divided by the square root of the product of their degrees (which is also referred to as the geometric mean of their degrees).\nA lot of the times, these three measures of structural similarity will tend to agree. Table 21.9), Table 21.10), and Table 21.11) show the similarities between each pair of nodes in the graph depicted in Figure 21.3.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n–\n0.29\n0.14\n0.12\n0\n0.22\n0.38\n0.33\n0.2\n0.33\n0.57\n0.38\n\n\nB\n–\n–\n0.25\n0\n0.17\n0.14\n0.33\n0.29\n0\n0.12\n0.33\n0\n\n\nC\n–\n–\n–\n0.25\n0\n0.17\n0.17\n0.14\n0.14\n0.33\n0\n0\n\n\nD\n–\n–\n–\n–\n0.17\n0.14\n0\n0.29\n0.29\n0.5\n0.14\n0.14\n\n\nE\n–\n–\n–\n–\n–\n0.29\n0.12\n0.43\n0.25\n0.25\n0.12\n0.29\n\n\nF\n–\n–\n–\n–\n–\n–\n0\n0.1\n0.38\n0.38\n0.25\n0.25\n\n\nG\n–\n–\n–\n–\n–\n–\n–\n0.38\n0.22\n0.1\n0.25\n0.25\n\n\nH\n–\n–\n–\n–\n–\n–\n–\n–\n0.2\n0.5\n0.38\n0.38\n\n\nI\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.33\n0.22\n0.38\n\n\nJ\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.22\n0.38\n\n\nK\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.25\n\n\nL\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\n\nTable 21.9: Structural similarity matrix based on Jaccard’s index.\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n–\n0.44\n0.25\n0.22\n0\n0.36\n0.55\n0.5\n0.33\n0.5\n0.73\n0.55\n\n\nB\n–\n–\n0.4\n0\n0.29\n0.25\n0.5\n0.44\n0\n0.22\n0.5\n0\n\n\nC\n–\n–\n–\n0.4\n0\n0.29\n0.29\n0.25\n0.25\n0.5\n0\n0\n\n\nD\n–\n–\n–\n–\n0.29\n0.25\n0\n0.44\n0.44\n0.67\n0.25\n0.25\n\n\nE\n–\n–\n–\n–\n–\n0.44\n0.22\n0.6\n0.4\n0.4\n0.22\n0.44\n\n\nF\n–\n–\n–\n–\n–\n–\n0\n0.18\n0.55\n0.55\n0.4\n0.4\n\n\nG\n–\n–\n–\n–\n–\n–\n–\n0.55\n0.36\n0.18\n0.4\n0.4\n\n\nH\n–\n–\n–\n–\n–\n–\n–\n–\n0.33\n0.67\n0.55\n0.55\n\n\nI\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.5\n0.36\n0.55\n\n\nJ\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.36\n0.55\n\n\nK\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.4\n\n\nL\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\n\nTable 21.10: Structural similarity matrix based on Dice’s index.\n\n\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\n\n\n\n\nA\n–\n0.47\n0.29\n0.24\n0\n0.37\n0.55\n0.5\n0.33\n0.5\n0.73\n0.55\n\n\nB\n–\n–\n0.41\n0\n0.29\n0.26\n0.52\n0.47\n0\n0.24\n0.52\n0\n\n\nC\n–\n–\n–\n0.41\n0\n0.32\n0.32\n0.29\n0.29\n0.58\n0\n0\n\n\nD\n–\n–\n–\n–\n0.29\n0.26\n0\n0.47\n0.47\n0.71\n0.26\n0.26\n\n\nE\n–\n–\n–\n–\n–\n0.45\n0.22\n0.61\n0.41\n0.41\n0.22\n0.45\n\n\nF\n–\n–\n–\n–\n–\n–\n0\n0.18\n0.55\n0.55\n0.4\n0.4\n\n\nG\n–\n–\n–\n–\n–\n–\n–\n0.55\n0.37\n0.18\n0.4\n0.4\n\n\nH\n–\n–\n–\n–\n–\n–\n–\n–\n0.33\n0.67\n0.55\n0.55\n\n\nI\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.5\n0.37\n0.55\n\n\nJ\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.37\n0.55\n\n\nK\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n0.4\n\n\nL\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n–\n\n\n\nTable 21.11: Structural similarity matrix based on the cosine distance.\n\n\n\n\n21.4.4 Neighborhood Overlap in Directed Graphs\nSimilarity works in similar (pun intended) ways when studying asymmetric ties in directed graph. The main difference, as usual, is that in a directed graph pairs of nodes can structurally similar in two different ways. Fist, pairs of nodes can be similar with respect to their out-neighborhoods, in which case we say that nodes are structural similar if they point to the same set of neighbors. This is called the out-similarity. Second, pairs of nodes can be similar with respect to their in-neighborhoods, in which case we say that nodes are structural similar if they receive ties or nominations from the same set of neighbors. This is called the in-similarity.\nSpecial cases of the out and in-similarities between nodes show up in particular types of networks. For instance, consider an information network composed of scientific papers. Here a directed tie emerges when paper A cites or refers to paper B. This is called a citation network.\nIn a citation network out-similar papers are papers that cite the same other papers. Out-similar papers are said to exhibit bibliographic coupling (essentially the overlap or set intersection between their reference lists). A weighted network of similarities between papers, where the weight of the edge is the number of other papers that that they both cite in common is called a bibliographic coupling network. A bibliographic coupling network is essentially a network of out-similarities between papers in a scientific information network.\nIn a citation network, in-similar papers are papers that get cited by the same set of others. In this case, we say that the two papers are co-cited a third paper. A weighted network of similarities between papers, where the weight of the edge is the number of other papers that cite both of them is called co-citation network. A co-citation network is essentially a network of in-similarities between papers in a scientific information network.\nThe two measures of out and in-similarities can be defined in the same way as before. If \\(n^{out}_{ij}\\) is the number of common out-neighbors of nodes i and j and \\(n^{in}_{ij}\\) is the number of their common out-neighbors, \\(k_{out}\\) is the total number of out-neighbors of a particular node, and \\(k_{in}\\) is the total number of in-neighbors, then the structural out and in-similarities between pairs of nodes i and j are given by (using the cosine distance measure) by:\n\\[\n  C_{ij}^{out} = \\frac{n_{ij}^{out}}{\\sqrt{k_i^{out}k_j^{out}}}\n\\tag{21.6}\\]\n\\[\n  C_{ij}^{in} = \\frac{n_{ij}^{in}}{\\sqrt{k_i^{in}k_j^{in}}}\n\\tag{21.7}\\]"
  },
  {
    "objectID": "13-lesson-positions-struct-equiv.html#references",
    "href": "13-lesson-positions-struct-equiv.html#references",
    "title": "21  Equivalence and Similarity",
    "section": "References",
    "text": "References\n\n\n\n\nBurt, Ronald S. 1976. “Positions in Networks.” Social Forces 55 (1): 93–122.\n\n\nDice, Lee R. 1945. “Measures of the Amount of Ecologic Association Between Species.” Ecology 26 (3): 297–302.\n\n\nJaccard, Paul. 1901. “Distribution of the Alpine Flora in the Dranse’s Basin and Some Neighbouring Regions.” Bulletin de La Societe Vaudoise Des Sciences Naturelles 37 (1): 241–72.\n\n\nLeicht, Elizabeth A, Petter Holme, and Mark EJ Newman. 2006. “Vertex Similarity in Networks.” Physical Review E 73 (2): 026120."
  },
  {
    "objectID": "14-lesson-positions-blockmod.html#the-correlation-distance",
    "href": "14-lesson-positions-blockmod.html#the-correlation-distance",
    "title": "22  Blockmodeling",
    "section": "22.1 The Correlation Distance",
    "text": "22.1 The Correlation Distance\nIt turns out there is an even fancier way to find out whether two nodes in a graph are structurally similar. It relies on a more complicated measure of distance called the correlation distance. This measure compares the row (or column) vectors of nodes in a graph and returns a number between \\(-1\\) and \\(+1\\). When it comes to structural equivalence, the correlation distance works like this:\n\nPairs of structurally equivalent nodes get a \\(+1\\). Nodes that are almost structurally equivalent but not quite (they are structurally similar) get a positive number larger than zero. The closer that number is to \\(+1\\) the more structurally similar the two nodes are.\nNodes that are completely different from one another (that is connect to completely disjoint sets of neighbors) get a \\(-1\\). In this case, nodes are opposites: Every time one node i has a \\(1\\) in their row vector in the adjacency matrix, the other has a \\(0\\) and vice versa. Nodes that are structurally dissimilar thus get a negative number between zero and \\(-1\\). The closer that number is to \\(-1\\), the more structurally dissimilar the two nodes are.\nNodes that have an even combination of similarities and differences in their pattern of connectivity to others get a number close to zero, with \\(0\\) indicating that two nodes have an even number of commonalities and differences.\n\nThe correlation distance between two nodes k and l is computed using the following formula:\n\\[\n    d^{corr}_{k, l} =\n    \\frac{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j}) \\times\n    (a_{(l)j} - \\overline{a}_{(l)j})\n    }\n    {\n    \\sqrt{\n    \\sum_j\n    (a_{(k)j} - \\overline{a}_{(k)j})^2 \\times\n    \\sum_j\n    (a_{(l)j} - \\overline{a}_{(l)j})^2\n        }\n    }\n\\tag{22.1}\\]\nEquation 22.1 looks like a monstrously complicated one, but it is actually not that involved.\nLet’s go through each components that we have already encountered in the lesson structural equivalence and structural similarity:\n\n\\(a_{(k)j}\\) is the row vector for node k in the adjacency matrix.\n\\(a_{(l)j}\\) is the row vector for node l in the adjacency matrix.\n\nNow let’s introduce ourselves to some new friends. For instance, what the heck is \\(\\overline{a}_{(k)j}\\)? The little “bar” on top the \\(a\\) indicates that we are taking the mean or the average of the elements of the row vector.\nIn equation form:\n\\[\n\\overline{a}_{(k)j} = \\frac{\\sum_j a_{kj}}{N}\n\\tag{22.2}\\]\nIn Equation 22.2, \\(\\sum_i a_{kj}\\) is the sum of all the elements in the vector, and \\(N\\) is the length of the vector, which is equivalent to the order of the graph from which adjacency matrix came from (the number of nodes in the network).\nSo for instance, in Table 21.1, the row vector for node A is:\n\n\n\n\nTable 22.1: Row vector for node A.\n\n\n0\n0\n1\n1\n0\n0\n\n\n\n\n\n\nWhich implies:\n\\[\n\\sum_i a_{(A)j} = 0 + 0 + 1 + 1 + 0 + 0 = 2\n\\]\nAnd we know that \\(N = 6\\), so that implies:\n\\[\n\\overline{a}_{(A)j} = \\frac{\\sum_i a_{Aj}}{N} = \\frac{2}{6}=0.33\n\\] The term \\(\\overline{a}_{(k)j}\\) is called the row mean for node k in the adjacency matrix. Just like we can compute row means, we can also compute column means by using the elements of the column vector \\(\\overline{a}_{(k)i}\\)\nNow that we know what the row means are, we can make sense of the term \\((a_{(k)j} - \\overline{a}_{(k)j})\\) in Equation 22.1. This is a vector composed of the differences between the row vector entries in the adjacency matrix and the row mean for that node. So in the case of node A and the row vector in Table 22.1 that would imply:\n\n\nTable 22.2: Row vector of mean differences for node A.\n\n\n\n\n(a) Vector of row entries minus the row mean.\n\n\n\n\n\n\n\n\n\n\n(0 - 0.33)\n(0 - 0.33)\n(1 - 0.33)\n(1 - 0.33)\n(0 - 0.33)\n(0 - 0.33)\n\n\n\n\n\n\n\n\n(b) Result of subtracting row mean from row entries.\n\n\n-0.33\n-0.33\n0.67\n0.67\n-0.33\n-0.33\n\n\n\n\n\n\nWhich implies: \\[\n\\sum_j (a_{(k)j} - \\overline{a}_{(k)j}) = -0.33 -0.33 + 0.67 + 0.67 -0.33 -0.33 = 0.02\n\\]\nThe numerator of Equation 22.1, just says: “Take the entries in the row vector for the first node, and create a new vector composed of the those entries minus the row means and sum the vector. Then do the same for the other node and multiply the two numbers” And in the denominator of the equation we just square the same vectors sum them, multiply each of the two numbers and take the square root of the result product. Once we have the numerator and denominator we can evaluate the fraction and compute the correlation distance between those two nodes.\nWhen we do that for each pair of nodes in Table 21.1, we end up with the structural similarity matrix shown in Table 22.3, based on the correlation distance.\n\n\n\n\n\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.71\n-0.71\n0.71\n-0.32\n\n\nB\n1\n–\n-0.71\n-0.71\n0.71\n-0.32\n\n\nC\n-0.71\n-0.71\n–\n1\n-1\n0.45\n\n\nD\n-0.71\n-0.71\n1\n–\n-1\n0.45\n\n\nE\n0.71\n0.71\n-1\n-1\n–\n-0.45\n\n\nF\n-0.32\n-0.32\n0.45\n0.45\n-0.45\n–\n\n\n\nTable 22.3: Correlation distance matrix for an undirected graph.\n\n\nIn the Table 22.3, the structurally equivalent pairs of nodes, A and B and C and D have \\(d^{corr} = 1.0\\). Nodes that are completely non-equivalent like C and E and D and E have \\(d^{corr} = -1.0\\). Nodes that are structurally similar, but not equivalent, like nodes C and F (\\(d^{corr} = 0.45\\)) get a positive number that is less than \\(1.0\\)."
  },
  {
    "objectID": "14-lesson-positions-blockmod.html#iterated-correlational-distances-concor",
    "href": "14-lesson-positions-blockmod.html#iterated-correlational-distances-concor",
    "title": "22  Blockmodeling",
    "section": "22.2 Iterated Correlational Distances: CONCOR",
    "text": "22.2 Iterated Correlational Distances: CONCOR\nWhat happens if we were to try to use Equation 22.1 to find the correlation distance of a correlation distance matrix? If we were to do this and use Table 22.3 as our input matrix, we end up with Table 22.4 (a).\n\n22.2.1 We Need to go Deepah\nNow, as Leo (when stuck in a dream, about a dream, about a dream…) always says: “We need to go deeper.”1 And, indeed, we can. We can take the correlation distance of the nodes based on Table 22.4 (a). If we do that, we end up with the entries in Table 22.4 (b). If we keep on going, we end up with the entries in Table 22.4 (c). Note that in Table 22.4 (c), there are only two values for all the entries: \\(+1\\) and \\(-1\\)!\n\n\nTable 22.4: Iterated correlational distances.\n\n\n\n\n(a) Second Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-0.97\n-0.97\n0.97\n-0.84\n\n\nB\n1\n–\n-0.97\n-0.97\n0.97\n-0.84\n\n\nC\n-0.97\n-0.97\n–\n1\n-1\n0.84\n\n\nD\n-0.97\n-0.97\n1\n–\n-1\n0.84\n\n\nE\n0.97\n0.97\n-1\n-1\n–\n-0.84\n\n\nF\n-0.84\n-0.84\n0.84\n0.84\n-0.84\n–\n\n\n\n\n\n\n\n\n(b) Third Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-0.99\n\n\nB\n1\n–\n-1\n-1\n1\n-0.99\n\n\nC\n-1\n-1\n–\n1\n-1\n0.99\n\n\nD\n-1\n-1\n1\n–\n-1\n0.99\n\n\nE\n1\n1\n-1\n-1\n–\n-0.99\n\n\nF\n-0.99\n-0.99\n0.99\n0.99\n-0.99\n–\n\n\n\n\n\n\n\n\n(c) Fourth Iteration\n\n\n\nA\nB\nC\nD\nE\nF\n\n\n\n\nA\n–\n1\n-1\n-1\n1\n-1\n\n\nB\n1\n–\n-1\n-1\n1\n-1\n\n\nC\n-1\n-1\n–\n1\n-1\n1\n\n\nD\n-1\n-1\n1\n–\n-1\n1\n\n\nE\n1\n1\n-1\n-1\n–\n-1\n\n\nF\n-1\n-1\n1\n1\n-1\n–\n\n\n\n\n\n\nMore importantly, as shown in Table 22.5, the structurally equivalent nodes have exactly the same pattern of \\(+1\\)s and \\(-1\\)s across the rows. This procedure of iterated correlations, invented by a team of sociologists and psychologists at Harvard University in the mid-1970s (Breiger, Boorman, and Arabie 1975), is called CONCOR—and acronym for the hard to remember title of “convergence of iterate correlations’’—and is designed to extract structurally equivalent positions from networks even when the input matrix is just based on structural similarities.\n\n\n\n\n\n \n  \n      \n    A \n    B \n    C \n    D \n    E \n    F \n  \n \n\n  \n    A \n    -- \n    1 \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    B \n    1 \n    -- \n    -1 \n    -1 \n    1 \n    -1 \n  \n  \n    C \n    -1 \n    -1 \n    -- \n    1 \n    -1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    1 \n    -- \n    -1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    -1 \n    -1 \n    -- \n    -1 \n  \n  \n    F \n    -1 \n    -1 \n    1 \n    1 \n    -1 \n    -- \n  \n\n\n\nTable 22.5:  Structurally Equivalent Positions in an undirected graph."
  },
  {
    "objectID": "14-lesson-positions-blockmod.html#blockmodeling",
    "href": "14-lesson-positions-blockmod.html#blockmodeling",
    "title": "22  Blockmodeling",
    "section": "22.3 Blockmodeling",
    "text": "22.3 Blockmodeling\nThe example we considered previously concerns the relatively small network shown in Figure 21.1. What happens when we apply the method of iterated correlations (CONCOR) to a bigger network, something like the one shown in ?fig-eigen?\nWell, we can begin by computing the correlation distance across all ,the \\(V=22\\) nodes in that network using Equation 22.1. The result of that looks like Table 22.6 (a). Note that even before we do any iterated correlations of correlation matrices we can see that the peripheral, single-connection nodes \\(E, F, G, H\\), \\(I, J, K, L, M\\) and \\(N, O, P, Q, R\\) are perfectly structurally equivalent. This makes sense, because all the nodes in each of these three groups have identical neighborhoods, since they happen to be connected to the same central node \\(A\\) for the first group, \\(B\\) for the second group and \\(C\\) for the third group. Note also that \\(U\\) and \\(V\\) are structurally equivalent, since their neighborhoods are the same: Their single connection is to node \\(S\\).\n\n\nTable 22.6: Correlation Distance Matrices Corresponding to an Undirected Graph.\n\n\n\n\n(a) Original Correlation Distance Matrix. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1.00 \n    -0.15 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    0.05 \n    -0.13 \n    -0.13 \n  \n  \n    B \n    -0.15 \n    1.00 \n    -0.15 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    C \n    -0.15 \n    -0.15 \n    1.00 \n    -0.24 \n    -0.19 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.24 \n    -0.13 \n    -0.13 \n  \n  \n    D \n    -0.24 \n    -0.24 \n    -0.24 \n    1.00 \n    0.34 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    0.55 \n    -0.16 \n    -0.09 \n    -0.09 \n  \n  \n    T \n    -0.19 \n    -0.19 \n    -0.19 \n    0.34 \n    1.00 \n    0.69 \n    0.69 \n    0.69 \n    0.69 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.07 \n    -0.13 \n    0.69 \n    0.69 \n  \n  \n    E \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    F \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    G \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    H \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    0.69 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    I \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    J \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    K \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    L \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    M \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    N \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    O \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    P \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    Q \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    R \n    -0.13 \n    -0.13 \n    -0.13 \n    0.55 \n    -0.07 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    1.00 \n    -0.09 \n    -0.05 \n    -0.05 \n  \n  \n    S \n    0.05 \n    -0.24 \n    -0.24 \n    -0.16 \n    -0.13 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    -0.09 \n    1.00 \n    -0.09 \n    -0.09 \n  \n  \n    U \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n  \n    V \n    -0.13 \n    -0.13 \n    -0.13 \n    -0.09 \n    0.69 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.05 \n    -0.09 \n    1.00 \n    1.00 \n  \n\n\n\n\n\n\n\n\n(b) Original Correlation Distance Matrix After Ten Iterations. \n \n  \n      \n    A \n    B \n    C \n    D \n    T \n    E \n    F \n    G \n    H \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    S \n    U \n    V \n  \n \n\n  \n    A \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(c) Correlation Distance Matrix in (a) with Rows and Columns Reshuffled to Show Hidden Pattern. \n \n  \n      \n    V \n    U \n    S \n    H \n    G \n    F \n    E \n    T \n    C \n    A \n    B \n    R \n    Q \n    P \n    O \n    N \n    M \n    L \n    K \n    J \n    D \n    I \n  \n \n\n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    H \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    G \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    F \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    E \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    M \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    L \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    K \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    J \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    D \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    I \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWhat happens when we take the correlation distance of the correlation distance matrix shown in Table 22.6 (a), and the correlation distance of the resulting matrix, and keep going until we only have zeros and ones? The results is Table 22.6 (b). This matrix seems to reveal a much deeper pattern of commonalities in structural positions across the nodes in ?fig-eigen. In fact, if we were a conspiracy theorist like Charlie from Always Sunny in Philadelphia, we might even surmise that there is a secret pattern that can be revealed if we reshuffled the rows and the columns of the matrix (without changing any of the numbers of course!).2\nIf we do that, we end up with Table 22.6 (c). So it turns out that there is indeed a secret pattern! The reshuffling shows that the nodes in the network can be divided into two blocks such within blocks all nodes are structurally similar (and some structurally equivalent) and across blocks, all nodes are structurally dissimilar. Thus \\(V, U, S, H, G, F, E, T, C, A, B\\) are members of one structurally similar block (let’s called them “Block 1”), and nodes \\(R, Q, P, O, N, M, L, K, J, D, I\\) are members of another structurally similar block (let’s called them “Block 2”). Nodes in “Block 1” are structurally dissimilar from nodes in “Block 2,” but structurally similar to one another and vice versa. To illustrate, Figure 22.1 is the same as ?fig-eigen, but this time nodes are colored by their memberships in two separate blocks.\n\n\n\n\n\nFigure 22.1: An undirected graph with block membership indicated by node color.\n\n\n\n\nNote that we haven’t changed any of the information in Table 22.6 (b) to get Table 22.6 (c). If you check, the row and column entries for each node in both figures are identical. It’s just that we changed the way the rows ordered vertically and the way the columns are ordered horizontally. For instance, node \\(A\\)’s pattern of connections is negatively correlated with node \\(I\\)’s in Table 22.6 (b), and has the same negative correlation entry in Table 22.6 (c). The same goes for each one of node \\(A\\)’s other correlations, and the same for each node in the table. Table 22.6 (b) and Table 22.6 (c) contain the same information it’s just that Table 22.6 (c) makes it easier to see a hidden pattern.\nThis property of the method of iterated correlations is the basis of a strategy for uncovering blocks of structurally similar actors in a network developed by a team of sociologists, physicists, and mathematicians working at Harvard in the 1970s. The technique is called blockmodeling (White, Boorman, and Breiger 1976). Let’s see how it works.\n\n22.3.1 We Need to go Deepah\nOf course, as Leo always says: “We need to go deeper.” And indeed we can. What happens if we do the same analysis as above, but this time in the two node-induced subgraphs defined by the set of structurally similar nodes in each of the two blocks we uncovered in the original graph? Then we get Table 22.7 (a) and Table 22.7 (b).\n\n\nTable 22.7: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    B \n    A \n    C \n    S \n    V \n    U \n    T \n    E \n    F \n    H \n    G \n  \n \n\n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    S \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    V \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    U \n    1 \n    1 \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    T \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    E \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    F \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    H \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    G \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    I \n    J \n    K \n    M \n    L \n    D \n    N \n    O \n    P \n    R \n    Q \n  \n \n\n  \n    I \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    J \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    K \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    M \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    L \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    O \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    P \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    R \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n  \n    Q \n    -1 \n    -1 \n    -1 \n    -1 \n    -1 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n  \n\n\n\n\n\n\nWe can see that Table 22.6 (a) separates our original Block 1 into two further sub-blocks. Let’s call them “Block 1a” and “Block 1b.” Block 1a is composed of nodes \\(A, B, C, S, U, V\\) and Block 1b is composed of nodes \\(E, F, G, H, T\\). Table 22.6 (b) separates our original Block 2 into three further sub-blocks. There’s the block composed of nodes \\(I, J, K, L, M\\). Let’s call this “Block 2a”, the block composed of nodes \\(N, O, P, Q, R\\). Let’s call this “Block 2b.” Then, there’s node \\(D\\). Note that this node is only structurally similar to itself and is neither similar nor dissimilar to the other nodes in the subgraph \\(d^{corr} = 0\\), so it occupies a position all by itself! Let’s call it “Block 2c.”\n\n\nTable 22.8: Subgraph Blockmodels\n\n\n\n\n(a) Blockmodel of a subgraph. \n \n  \n      \n    S \n    C \n    B \n    A \n    V \n    U \n  \n \n\n  \n    S \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    1 \n    1 \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    V \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    U \n    -1 \n    -1 \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\n(b) Another Blockmodel of another subgraph \n \n  \n      \n    B \n    C \n    A \n    S \n  \n \n\n  \n    B \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    C \n    1 \n    1 \n    -1 \n    -1 \n  \n  \n    A \n    -1 \n    -1 \n    1 \n    1 \n  \n  \n    S \n    -1 \n    -1 \n    1 \n    1 \n  \n\n\n\n\n\n\nNow let’s do a couple of final splits of the subgraph composed of nodes \\(A, B, C, S, U, V\\). This is shown in Table 22.8. The first split separates nodes in block \\(A, B, C, S\\) from those in block \\(U, V\\) (Table 22.8 (a)). The second splits the nodes in subgraph \\(A, B, C, S\\) into two blocks composed of \\(A, S\\) and \\(B, C\\), respectively (Table 22.8 (b)).\n\n\n\n\n\nFigure 22.2: An undirected graph with block membership indicated by node color.\n\n\n\n\nFigure 22.2 shows the nodes in ?fig-eigen colored according to our final block partition. It is clear that the blockmodeling approach captures patterns of structural similarity. For instance, all the single-connection nodes connected to more central nodes get assigned to their own position: Block 1b: \\(E, F, G, H, T\\), Block 2a: \\(I, J, K, L, M\\), and Block 2b: \\(N, O, P, Q, R\\). The most central node \\(D\\) (in terms of Eigenvector centrality) occupies a unique position in the graph. Two of the three central nodes (in terms of degree centrality) \\(B, C\\) get assigned to their own position. Meanwhile \\(A, S\\) form their own structurally similar block. Finally, \\(U, V\\) also form their own structurally similar block as both are structurally equivalent in the orignal graph.\n\n\n22.3.2 The Blocked Adjancency Matrix\nWhat happens if we were to go back fo the adjacency matrix corresponding to ?fig-eigen, and then reshuffle the rows and columns to correspond to all these wonderful blocks we have uncovered? Well, we would en up with something like Table 22.9. This is called the blocked adjacency matrix. In the blocked adjacency matrix, the division between the nodes corresponding to each block of structurally similar nodes in Table 22.7 and Table 22.8 is marked by thick black lines going across the rows and columns.\nEach diagonal rectangle in Table 22.9 corresponds to within-block connections. Each off-diagonal rectangle corresponds to between block connections. There are two kinds of rectangles in the blocked adjacency matrix. First, there are rectangles that only contains zero entries. These are called zero blocks. For instance the top-left rectangle in Table 22.9 is a zero block. Then there rectangles that have some non-zero entries in them (ones, since this is a binary adjacency matrix). These are called one blocks. For instance, the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) is a one block.\n\n\n\n\n\n \n  \n      \n    I \n    J \n    K \n    L \n    M \n    N \n    O \n    P \n    Q \n    R \n    T \n    E \n    F \n    G \n    H \n    B \n    C \n    A \n    S \n    U \n    V \n    D \n  \n \n\n  \n    I \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    J \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    K \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    L \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    M \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    N \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    O \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    P \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    Q \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    R \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    T \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    E \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    F \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    G \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    H \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n  \n    B \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    C \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    S \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n  \n  \n    U \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    V \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    0 \n    1 \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n  \n\n\n\nTable 22.9:  Blocked adjancency matrix. \n\n\nZero-blocks indicate that the members of the row block don’t have any connections with the members the column block (which can include themselves!). For instance, the zero-block in the top-left corner of the blocked adjacency matrix in Table 22.9 indicates that the members of this block are not connected to one another in the network (and we can verify from Figure 22.2 that this is indeed the case).\nOne blocks indicate that the members of the column block share some connections with the members of the column block (which can also include themselves!). For instance, the one-block in the fourth rectangle going down the rows (corresponding to the block that nodes \\(B, C\\) belong to) tells us that members of this block are connected to at least one member of the \\(I, J, K, L, M\\) block (and we can verify from Figure 22.2 that this is indeed the case, since \\(B\\) is connected to all of them).\n\n\n22.3.3 The Image Matrix\nFrom this reshuffled adjacency matrix, we can get to a reduced image matrix containing the relations not between the nodes in the graph, but between the blocks in the graph. The way we proceed to construct the image matrix is as follows:\n\nFirst we create an empty matrix \\(\\mathbf{B}\\) of dimensions \\(b \\times b\\) where \\(B\\) is the number of blocks in the blockmodel. In our example, \\(b = 7\\) so the image matrix has seven rows and seven columns. The \\(ij^{th}\\) cell in the image matrix \\(\\mathbf{B}\\) records the relationship between row block i and column block j in the blockmodel.\nSecond, we put a zero in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 22.9 is a zero-block.\nThird, we put a one in the image matrix if the rectangle corresponding to the relationship between blocks i and j in Table 22.9 is a one-block.\n\nThe result is Table 22.10.\n\n\n\n\n\n \n  \n      \n    I, J, K, L \n    N, O, P, Q, R \n    T, E, F, G, H \n    B, C \n    A, S \n    U, V \n    D \n  \n \n\n  \n    I, J, K, L \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    N, O, P, Q, R \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n    0 \n  \n  \n    T, E, F, G, H \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    B, C \n    1 \n    1 \n    0 \n    0 \n    0 \n    0 \n    1 \n  \n  \n    A, S \n    0 \n    0 \n    1 \n    0 \n    0 \n    1 \n    1 \n  \n  \n    U, V \n    0 \n    0 \n    0 \n    0 \n    1 \n    0 \n    0 \n  \n  \n    D \n    0 \n    0 \n    0 \n    1 \n    1 \n    0 \n    0 \n  \n\n\n\nTable 22.10:  Image matrix Corresponding to the blockmodel of an Undirected Graph. \n\n\nSo the big blocked adjacency matrix in Table 22.6 (c) can be reduced to the image matrix shown Table 22.10, summarizing the relations between the blocks in the graph. This matrix, can then even be represented as a graph, so that we can see the pattern of relations between blocks! This is shown in Figure 22.3\n\n\n\n\n\nFigure 22.3: Graph representation of reduced image matrix from a blockmodel.\n\n\n\n\nThis is how blockmodeling works!"
  },
  {
    "objectID": "14-lesson-positions-blockmod.html#references",
    "href": "14-lesson-positions-blockmod.html#references",
    "title": "22  Blockmodeling",
    "section": "References",
    "text": "References\n\n\n\n\nBreiger, Ronald L, Scott A Boorman, and Phipps Arabie. 1975. “An Algorithm for Clustering Relational Data with Applications to Social Network Analysis and Comparison with Multidimensional Scaling.” Journal of Mathematical Psychology 12 (3): 328–83.\n\n\nWhite, Harrison C, Scott A Boorman, and Ronald L Breiger. 1976. “Social Structure from Multiple Networks. I. Blockmodels of Roles and Positions.” American Journal of Sociology 81 (4): 730–80."
  },
  {
    "objectID": "9-2-lesson-applied-matrix-operations.html#references",
    "href": "9-2-lesson-applied-matrix-operations.html#references",
    "title": "17  Applied Matrix Operations",
    "section": "References",
    "text": "References\n\n\n\n\nFestinger, Leon. 1949. “The Analysis of Sociograms Using Matrix Algebra.” Human Relations 2 (2): 153–58.\n\n\nLuce, RD, and Albert D Perry. 1949. “A Method of Matrix Analysis of Group Structure.” Psychometrika 14 (2): 95–116."
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#matrix-multiplication-of-vectors",
    "href": "9-1-lesson-matrix-operations.html#matrix-multiplication-of-vectors",
    "title": "16  Matrix Operations",
    "section": "16.6 Matrix Multiplication of Vectors",
    "text": "16.6 Matrix Multiplication of Vectors\nRecall from ?sec-degset that a vector is a sequence of numbers of a given length. So for instance, the vector \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\) is a vector of length five.\nWell, and here comes the big reveal, it turns out that another way to think of a vector, is as a special case of matrix. That is, a matrix with one row, and as many columns as the length of the vector! This is a called a row vector. So the row vector \\(\\mathbf{a}\\) vector can be thought of as a matrix of dimensions \\(1 \\times 5\\) (one row and five columns) or \\(\\mathbf{A}_{1 \\times 5}\\).\nIn matrix form:\n\n\n\n\n\n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\nTable 16.8:  Matrix resulting from multiplying a matrix times its transpose \n\n\nSince vectors are matrices, we can perform the same type of matrix operations on them as we did with matrices. For instance, we can compute the transpose of a vector. In the case of \\(\\mathbf{a}\\), the transpose \\(\\mathbf{a}^T\\) is:\n\n\n\n\n\n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\nTable 16.9:  Matrix resulting from multiplying a matrix times its transpose \n\n\nThe transpose of a row vector is called (you may have guessed) a column vector. The column vector in Table 16.9 is a matrix with five rows and one column.\nThis also means that the same rules of matrix multiplication apply. For instance, we can always multiply a row vector times a column vector, because it is the equivalent of multypling a matrix times its transpose, and we have already seen in Section 16.4.2, that this can always be done:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{a}^T_{5 \\times 1} = b_{1 \\times 1}\n\\tag{16.6}\\]\nEquation 16.7 says that the product of the \\(1 \\times 5\\) row vector \\(\\mathbf{a}\\) times a \\(5 \\times 1\\) column vector \\(\\mathbf{a}^T\\) is a \\(1 \\times 1\\) “matrix” otherwise known as a scalar (that is, a regular old number). We’ve already seen examples of this, because in regular matrix multiplication, each cell of the product matrix is a scalar obtained from multiplying the corresponding terms taken from a row of the first matrix (which is a row vector) times those of the column of the second matrix (which is a column vector).\nSo in this case this would be:\n\\[\n(2 \\times 2) + (4 \\times 4) + (7 \\times 7) + (2 \\times 2) + (4 \\times 4) =\n\\]\n\\[\n4 + 16 + 49 + 4 + 16 = 89\n\\]\n\nSo the first rule of vector matrix multiplication is that you can always multiply a row vector times a column vector (even when their entries are not the same) as long as the number of columns of the row vector equal the number of rows of the column vector.\nThe second rule of vector matrix multiplication is that when you multiply a row vector times another a column vector the result is always scalar (a single number).\n\nNow notice that if we change the order, and multiply the transpose of a vector times the original? This should be allowed because it conforms to the rules that we have already discussed:\n\\[\n\\mathbf{a}^T_{5 \\times 1} \\times \\mathbf{a}_{1 \\times 5} = B_{5 \\times 5}\n\\tag{16.7}\\]\nThis matrix multiplication is defined because the inner dimensions of the two matrices (the column and row vectors) are the same (one). But note that, according to the rules of matrix multiplication, when you multiply the transpose of a vector times the original, the result is a square matrix, with dimensions \\(n \\times n\\) where \\(n\\) is the length of the original row vector (the number of columns). In our example if the original vector is \\(\\mathbf{a} = \\{2, 4, 7, 2, 4\\}\\), then \\(\\mathbf{a}^T \\times \\mathbf{a}\\) is equal to the matrix shown in Table 16.10.\n\n\n\n\n\n\n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n  \n    14 \n    28 \n    49 \n    14 \n    28 \n  \n  \n    4 \n    8 \n    14 \n    4 \n    8 \n  \n  \n    8 \n    16 \n    28 \n    8 \n    16 \n  \n\n\n\nTable 16.10:  Matrix resulting from multiplying a matrix times its transpose"
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#multiplying-matrices-times-vector-and-vice-versa",
    "href": "9-1-lesson-matrix-operations.html#multiplying-matrices-times-vector-and-vice-versa",
    "title": "16  Matrix Operations",
    "section": "16.7 Multiplying Matrices Times Vector (and Vice Versa)",
    "text": "16.7 Multiplying Matrices Times Vector (and Vice Versa)\nSince vectors are just matrices, it means that we can always multiply a vector times a matrix (and a matrix times a vector), as long as we follow the matrix multiplication rules laid out in Section 16.4.1. For instance, take the row vector \\(\\mathbf{b} = \\{4, 9 3. 5\\}\\) and the binary matrix \\(\\mathbf{A}\\) shown in Table 16.7 (a). Because \\(\\mathbf{b}\\) is of dimensions \\(1 \\times 4\\) and \\(\\mathbf{A}\\) is of dimensions \\(4 \\times 4\\), it is possible to multiply the vector times the matrix as follows:\n\\[\n\\mathbf{b}_{1 \\times 5} \\times \\mathbf{A}_{5 \\times 5} = \\mathbf{c}_{1 \\times 5}\n\\]"
  },
  {
    "objectID": "9-1-lesson-matrix-operations.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "href": "9-1-lesson-matrix-operations.html#multiplying-a-vector-times-a-matrix-and-vice-versa",
    "title": "16  Matrix Operations",
    "section": "16.7 Multiplying a Vector Times A Matrix (and Vice Versa)",
    "text": "16.7 Multiplying a Vector Times A Matrix (and Vice Versa)\nSince vectors are just matrices, it means that we can always multiply a vector times a matrix (and a matrix times a vector), as long as we follow the matrix multiplication rules laid out in Section 16.4.1.\n\n16.7.1 Row Vector Times Matrix\nFor instance, take the row vector \\(\\mathbf{b} = \\{4, 9, 3, 5\\}\\) and the binary matrix \\(\\mathbf{A}\\) shown in Table 16.7 (a). Because \\(\\mathbf{b}\\) is of dimensions \\(1 \\times 4\\) and \\(\\mathbf{A}\\) is of dimensions \\(4 \\times 4\\), it is possible to multiply the vector times the matrix as follows:\n\\[\n\\mathbf{b}_{1 \\times 4} \\times \\mathbf{A}_{4 \\times 4} = \\mathbf{c}_{1 \\times 4}\n\\tag{16.8}\\]\nEquation 16.8 says that the product of a \\(1 \\times 4\\) row vector times a \\(4 \\times 4\\) square matrix is another vector of dimensions equal to the original row vector. The result for our example is shown in Table 16.11.\n\n\nTable 16.11: Row vector resulting from multiplying a row vector times a square matrix\n\n\n\n\n(a) 1 X 4 row vector \n\n  \n    4 \n    9 \n    3 \n    5 \n  \n\n\n\n\n\n\n\n\n(b) 4 X 4 square binary matrix \n\n  \n    0 \n    1 \n    0 \n    1 \n  \n  \n    0 \n    1 \n    1 \n    0 \n  \n  \n    1 \n    0 \n    1 \n    1 \n  \n  \n    1 \n    0 \n    1 \n    0 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 4 product vector \n\n  \n    8 \n    13 \n    17 \n    7 \n  \n\n\n\n\n\n\nOf course, it is also possible to multiply a row vector times a rectangular matrix (where the number of rows is not necessarily equal to the number of columns), as long as the number of rows of the matrix equals the length of the original row vector. For instance, take a row vector \\(\\mathbf{a}_{1 \\times 5}\\) shown in Table 16.12 (a) and a matrix \\(B_{5 \\times 3}\\). Its product would be given by:\n\\[\n\\mathbf{a}_{1 \\times 5} \\times \\mathbf{B}_{5 \\times 3} = \\mathbf{c}_{1 \\times 5}\n\\tag{16.9}\\]\nEquation 16.9 says that the product of a row vector of dimensions \\(1 \\times 5\\) and a matrix of dimensions \\(5 \\times 3\\) is another row vector \\(\\mathbf{c}\\) of dimensions equal to the original row vector (\\(1 \\times 5\\)).\nA numerical example is shown in Table 16.12.\n\n\nTable 16.12: Row vector resulting from multiplying row vector times a rectangular matrix\n\n\n\n\n(a) 1 X 5 row vector \n\n  \n    2 \n    4 \n    7 \n    2 \n    4 \n  \n\n\n\n\n\n\n\n\n(b) 5 x 3 matrix \n\n  \n    2 \n    13 \n    18 \n  \n  \n    5 \n    4 \n    14 \n  \n  \n    11 \n    7 \n    1 \n  \n  \n    15 \n    20 \n    10 \n  \n  \n    12 \n    16 \n    19 \n  \n\n\n\n\n\n\n\n\n(c) 1 X 3 product vector \n\n  \n    179 \n    195 \n    195 \n  \n\n\n\n\n\n\nTo get the “179” entry in row one and column one of Table 16.12, we take the entries of the row vector shown in Table 16.12 (a) and multiply them by the corresponding entries in the first column of the matrix shown in Table 16.12 (b) and add up the results:\n\\[\n(2 \\times 2) + (4 \\times 5) + (7 \\times 11) + (2 \\times 15) + (4 \\times 12) =\n\\]\n\\[\n4 + 20 + 77 + 30 + 48 = 179\n\\] And so on for the other two entries in Table 16.12 (c). So the first rule of multiplying a row vector times a matrix is that the result will always be another row vector of length equal to the number of columns of the matrix.\n\n\n16.7.2 Matrix Times Column Vector\nIn the same way, we can always multiply a matrix times a column vector, as long as the the number of rows of the matrix is equal to the length of the column vector. For instance, take the binary square matrix \\(A_{5 \\times 5}\\) shown in Table 16.13 (a) and the column vector \\(\\mathbf{b}_{5 \\times 1}\\) shown in Table 16.12 (b). Their product \\(\\mathbf{c}\\) would be given by:\n\\[\n\\mathbf{A}_{5 \\times 5} \\times \\mathbf{b}_{5 \\times 1} = \\mathbf{c}_{5 \\times 1}\n\\tag{16.10}\\]\nEquation 16.10 says that the product of a matrix of dimensions \\(5 \\times 5\\) and a column vector of dimensions \\(5 \\times 1\\) and is another column vector \\(\\mathbf{c}\\) of dimensions equal to the original column vector (\\(5 \\times 1\\)).\n\n\nTable 16.13: Column vector resulting from multiplying a square matrix times a column vector\n\n\n\n\n(a) 5 x 5 square matrix \n\n  \n    15 \n    16 \n    10 \n    10 \n    1 \n  \n  \n    3 \n    16 \n    15 \n    3 \n    14 \n  \n  \n    0 \n    10 \n    17 \n    0 \n    11 \n  \n  \n    12 \n    16 \n    15 \n    18 \n    4 \n  \n  \n    9 \n    5 \n    7 \n    7 \n    17 \n  \n\n\n\n\n\n\n(b) 5 x 1 column vector \n\n  \n    2 \n  \n  \n    4 \n  \n  \n    7 \n  \n  \n    2 \n  \n  \n    4 \n  \n\n\n\n\n\n\n(c) 5 X 1 product vector \n\n  \n    188 \n  \n  \n    237 \n  \n  \n    203 \n  \n  \n    245 \n  \n  \n    169 \n  \n\n\n\n\n\n\nA numerical example of this situation is shown in Table 16.13."
  }
]