---
title: "Positions"
---

```{r setup, include=FALSE}
    require(ggraph)
    require(tidygraph)
    require(igraph)
    require(kableExtra)
    require(lsa)
    #function:
    corr.dist <- function(x) {
         r <- nrow(x)
         c <- ncol(x)
         r.c <- matrix(0, r, r)
         c.c <- matrix(0, c, c)
         r.m <- rowMeans(x)
         c.m <- colMeans(x)
         
         for (i in 1: r) {
              for (j in 1:r) {
                   r.x <- x[i, ] - r.m[i]
                   r.y <- x[j, ] - r.m[j]
                   r.xy <- r.x * r.y
                   r.xx <- r.x^2
                   r.yy <- r.y^2
                   r.num <- sum(r.xy)
                   r.den <- sqrt(sum(r.xx)) * sqrt(sum(r.yy))
                   r.c[i, j] <- round(r.num / r.den, 2)
              }
         }
         rownames(r.c) <- rownames(x)
         colnames(r.c) <- rownames(x)
         return(r.c)
        }
```

## The Position Approach
The basic idea behind the position approach to dividing up the nodes in a graph is to come up with a measure of how *similar* two nodes are in terms of their patterns of connectivity with others. This measure then can be used to partition the nodes into what are called **equivalence** or **similarity** classes. Nodes in the same equivalence class are said to occupy the same **position** in the social structure described by the network. 

There are two main ways to partition nodes into equivalence classes. The first is based on the idea that two nodes occupy the same position is they have similar patterns of connectivity to the same other nodes in the graph. This is called **structural equivalence**. 

The second is based on the idea that two nodes are equivalent if they are connected to people who are themselves equivalent, even if these are not literally the same people. This is called **regular equivalence**. 

This lesson will deal mainly with various ways of partitioning the nodes in a network based on **structural equivalence** (@sec-equiv) and its more relaxed cousin, **structural similarity** (@sec-sim).

## Structural Equivalence {#sec-equiv}
*Two nodes are structurally equivalent if they are connected to the same others*. Thus, their patterns of connectivity (e.g., their row in the adjacency matrix) is exactly the same. 

```{r}
#| label: fig-equivex 
#| fig-cap: "An undirected graph with nodes colored by membership in the same structural equivalence class."
#| fig-cap-location: margin 
#| fig-width: 12
#| fig-height: 12

    gr <- create_empty(6, directed = FALSE)
    gr <- gr %>% 
        bind_edges(data.frame(from = 1, to = 3:4)) %>% 
        bind_edges(data.frame(from = 2, to = 3:4)) %>% 
        bind_edges(data.frame(from = 3, to = 1:2)) %>% 
        bind_edges(data.frame(from = 4, to = 1:2)) %>% 
        bind_edges(data.frame(from = 5, to = 3:4)) %>% 
        bind_edges(data.frame(from = 6, to = 5)) %>% 
        mutate(name = toupper(letters[1:6]))
    p <- ggraph(gr, layout = 'kk') 
    p <- p + geom_edge_link(color = "steelblue", edge_width = 1.1)
    p <- p + geom_node_point(aes(x = x, y = y, color = c("a", "a", "b", "b", "c", "d")), size = 24) 
    p <- p + geom_node_text(aes(label = name), size = 12, color = "white")
    p <- p + theme_graph() + theme(legend.position = "none")
    p
```

For instance in @fig-equivex, nodes *C* and *D* are structurally equivalent because they are connected to the same neighbors $\{A, B, E\}$. In the same way, nodes *A* and *B* are structurally equivalent because they are connected to the same neighbors $\{C, D\}$. Finally, nodes *E* and *F* occupy *unique positions* in the network because their neighborhoods are not equivalent to that of any other nodes. Node *E* is the only node that has a neighborhood composed of nodes $\{C, D, F\}$, and node *F* is the only node that has a neighborhood composed of node $\{E\}$ only. Perhaps *F* is the main boss, and *E* is the second in command. 

```{r adjmat1}
#| label: tbl-adjmat1 
#| tbl-cap: "Adjancency Matrix of an Undirected Graph"
#| tbl-cap-location: margin

    gr <- simplify(gr)
    a <- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))
    rownames(a) <- V(gr)$name
    colnames(a) <- V(gr)$name
    a <- a[order(row.names(a)), order(row.names(a))]
    kbl(a, format = "html", padding = 2, align = "c") %>% 
        row_spec(1:2, background = "salmon", bold = TRUE) %>% 
        row_spec(3:4, background = "lightgreen", bold = TRUE) %>% 
        row_spec(5, background = "cyan", bold = TRUE) %>% 
        row_spec(6, background = "violet", bold = TRUE)
```

We can also see by looking at @tbl-adjmat1) that, indeed, the rows corresponding to the structurally equivalent nodes  $\{A, B\}$ and  $\{C, D\}$ in the corresponding **adjacency matrix** are indistinguishable from one another. The nodes that have unique positions in the network $\{E, F\}$, also have a unique pattern of 0s and 1s across the rows of the adjacency matrix. 

## Measuring Structural Equivalence in Undirected Graphs
Given an adjacency matrix for a graph, how can we find out which nodes are structurally equivalent without staring at a picture for a long time? A classic way of measuring structural equivalence, developed by the sociologist Ronald Burt @burt76 is to use the **Euclidean Distance** between the row vectors corresponding to each node in an adjacency matrix. 

Take for instance @tbl-adjmat1. The row vector for node *A* is $a_{(A)j} = (0, 0, 1, 1, 0, 0)$, and so is the row vector for node *B* $a_{(B)j}$, because we already know they are structurally equivalent! Remember, the row vector is just the entries in each row corresponding to each node in @tbl-adjmat1. So the row vector for node *C* ($a_{(C)j}= (1, 1, 0, 0, 1, 0)$ and so forth. Here the subscript $j$ refers to each column entry of the adjacency matrix $(A, B, C...F)$.

The **Euclidean Distance** between the row row-vectors of two nodes $k$ and $l$ is given by:

$$
d^{Euclidean}_{k,l} = \sqrt{\sum_j (a_{(k)j}-a_{(l)j})^2}
$$ {#eq-euclid}

What equation @eq-euclid says is that we take each corresponding entry of the row vectors, subtract them from one another, square them, sum them, and take the square root of the resulting sum.

So let's say we wanted to find out the Euclidean distance between nodes *A* and *C* in @tbl-adjmat1. We would proceed as follows:

- First, get the row vector for node *A* that's $a_{(A)j} = (0, 0, 1, 1, 0, 0)$, as we saw earlier.
- Second, get the row vector for node *C* that's $a_{(C)j}= (1, 1, 0, 0, 1, 0)$, as we saw earlier.
- Third, line them up, so that you can compute the differences and then square them:

```{r}
#| label: tbl-eudist-ex1 
#| tbl-cap: "Euclidean distance calculation."
#| tbl-cap-location: margin

    a <- c(0, 0, 1, 1, 0, 0)
    c <- c(1, 1, 0, 0, 1, 0)
    d <- a-c
    e <- d^2
    c.diff <- c("(0 - 1)", "(0 - 1)", "(1 - 0)", "(1 - 0)", "(0 - 1)", "(0 - 0)")
    t <- matrix(c(a, c, c.diff, d, e), nrow = 5, byrow = TRUE)
    rownames(t) <- c("A", "C", "A - C", "A - C", "(A - C)^2")
    kbl(t, format = "pipe", align = "c")
```

The Euclidean distance between *A* and *C* is thus the square root of the sum of the numbers in the last row of @tbl-eudist-ex1: $\sqrt{1+1+1+1+1+0} = \sqrt{5}= 2.2$. The *larger* the Euclidean distance, the *less* equivalent two nodes in a graph are. 

This means that for two nodes that are perfectly structurally equivalent (they have identical row vectors), the Euclidean distance should reach its minimum value of zero. We can check that by computing the Euclidean distance of nodes *A* and *B* in @tbl-adjmat1:

```{r}
#| label: tbl-eudist-ex2
#| tbl-cap: "Euclidean distance calculation."
#| tbl-cap-location: margin

    a <- c(0, 0, 1, 1, 0, 0)
    d <- a-a
    e <- d^2
    t <- matrix(c(a, a, d, e), nrow = 4, byrow = TRUE)
    rownames(t) <- c("A", "B", "A-C", "(A-C)^2")
    kbl(t, format = "pipe", align = "c")
```

Indeed since the sum of the last row of the numbers in @tbl-eudist-ex2 is zero, then so will the square root!

## The Structural Equivalence Matrix
@tbl-dist1 shows the **structural equivalence distance matrix** ($\mathbf{D}$) produced by computing euclidian distance between of each pair of nodes in the adjacency matrix shown in @tbl-adjmat1 (corresponding to the graph shown in @fig-equivex) according to @eq-euclid. 


```{r}
#| label: tbl-dist1
#| tbl-cap: Structural Equivalence matrix for an undirected graph.
#| tbl-cap-location: margin

    eudist <- function(x) {
      r <- nrow(x)
      edist <- matrix(0, r, r)
      for (i in 1: r) {
          for (j in 1: r) {
            e <- x[i, ] - x[j, ]
            e <- e^2
            e <- round(sqrt(sum(e)), 1)
            edist[i, j] <- e
          }
        }
    return(edist)
    }
    gr <- simplify(gr)
    a <- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))
    rownames(a) <- V(gr)$name
    colnames(a) <- V(gr)$name
    d <- eudist(a)
    rownames(d) <- rownames(a)
    colnames(d) <- colnames(a)
    diag(d) <- "--"
    kbl(d, format = "pipe", align = "c")

```

In the $\mathbf{D}$ matrix, each cell $\mathbf{d}_{ij}$ gives us the Euclidean distance between nodes *i* and *j*. Note that the $\mathbf{D}$ matrix is **symmetric**, meaning that the same information is contained in the lower and upper triangles ($\mathbf{d}_{ij}= \mathbf{d}_{ji}$). This makes sense, because the distance between point *a* and point *b* should be the same as the distance between point *b* and point *a*.

Checking the values in @tbl-dist1, we can see that structurally equivalent pairs of nodes---are connected to the same others, like nodes *A* and *B* or nodes *C* and *D*---have distance $\mathbf{d}_{ji}= 0$ in the matrix. As nodes become less equivalent---are connected to different others like nodes *E* and *C*---their Euclidean distance becomes larger.

So, we can say that *structurally equivalent nodes occupy the same position in the network*. So *A* and *B* occupy the same position, and so do *C* and *D*. 

## Measuring Structural Equivalence in Directed Graphs
So far we have considered the case of structural equivalence for undirected graphs composed of **symmetric ties**. 

But what happens when the network we are studying is composed of **asymmetric ties**? 

Well, in the directed graph case, we have to distinguish between *two ways* nodes in a graph can be structurally equivalent to one another based on the directionality of the ties we are considering.

- In the first case, two nodes are structurally equivalent if they *send ties to the same others*.
- In the second case, two nodes are structurally equivalent if they *receive ties from the same others*.

These don't necessarily have to go together. One node (*A*) can send ties to the same others as another node (*B*) (and thus *A* and *B* can be structurally equivalent when it comes to their **out-neighbors**), but receive ties from a different set of others (and thus *A* and *B* can fail to be structurally equivalent in terms of their **in-neighbors**)

This means that in the directed graph case, two nodes are structurally equivalent if and only if *they send ties to the same others and receive ties from the same others*. 

This added complication means that we have to modify the way we measure structural equivalence in the directed graph case. Particularly, to consider the distance between pairs of nodes, we now have to consider both their row-vectors (capturing the pattern of *sending ties*) *and* their column vectors (capturing the patterns of their *receiving ties*) in calculating the distance. 

This means @eq-euclid now turns into:

$$
  d^{Euclidean}_{k,l} = \sqrt{\sum_j (a_{(k)j}-a_{(l)j})^2 + \sum_i (a_{(k)i}-a_{(l)i})^2}
$$ {#eq-deuclid}

The first part of @eq-deuclid inside the square root operator is just like @eq-euclid: $\sum_j (a_{(k)j}-a_{(l)j})^2$. In this case, this tracks the Euclidean distance between the respective *row* vectors of nodes *k* and *l* (which is we sum across the columns *j*), which captures the extent to which they *send* links to the same others. When this part of the equation equals zero, it means *k* and *l* are structurally equivalent when it comes to sending ties.

The second part that is added is $\sum_j (a_{(k)i}-a_{(l)i})^2$, which measures the Euclidean distance between the *column vectors* of nodes *k* and *l* in the **asymmetric adjacency matrix** (which is why we sum across the rows *i*); this captures the extent to which *k* and *l* *receive* ties from the same others. When this part of the equation equals zero, it means *k* and *l* are structurally equivalent when it comes to receiving ties.

```{r}
#| label: fig-direquiv
#| fig-cap: "A directed graph"
#| fig-cap-location: margin
#| fig-height: 10
#| fig-width: 10

    fr <- c("A", "A", "A", "A", "B", "B", "B", "C", "D", "D", "E", "E", "F", "G", "G", "D", "D", "B")
    to <- c("C", "D", "B", "F", "A", "D", "C", "A", "A", "B", "C", "F", "A", "F", "C", "E", "G", "F")
  edge.dat <- data.frame(fr, to)
  node.dat <- data.frame(name = union(fr, to))
  gr <- tbl_graph(edges = edge.dat, nodes = node.dat)
  p <- ggraph(gr, layout = 'tree')
  p <- p + geom_edge_parallel(color = "gray", edge_width = 1.25,
              arrow = arrow(length = unit(5, 'mm')),
              end_cap = circle(10, 'mm'), 
              sep = unit(6, 'mm'))
  p <- p + geom_node_point(aes(x = x, y = y,
                           color = c("a", "b", "c", "d", "e", "c", "e")), size = 24)
  p <- p + geom_node_text(aes(label = name), size = 12, color = "white")
  p <- p + theme_graph() + theme(legend.position = "none")
  p
  
```
Let's look at an example. Consider the graph shown in @fig-direquiv. In the graph, nodes rendered in the same color are structurally equivalent according to @eq-deuclid. The corresponding **asymmetric adjacency matrix** for the graph in @fig-direquiv is shown in @tbl-directed.

```{r}
#| label: tbl-directed
#| tbl-cap: "Asymmetric adjacency matrix for a directed graph."
#| tbl-cap-location: margin

    A <- as.matrix(as_adjacency_matrix(gr))
    diag(A) <- "--"
    kbl(A, format = "pipe", align = "c") 
```

Let's say we wanted to figure out whether nodes *E* and *G* are structurally equivalent (they are). First, we would compare their respective row-vectors in the adjacency matrix:

```{r}
#| label: tbl-rowvecs
#| tbl-cap: Row vectors for nodes E and G.

    A <- as.matrix(as_adjacency_matrix(gr))
    diag(A) <- "--"
    A <- A[c(5, 7), ]
    kbl(A, format = "pipe", align = "c") 
```

Then we would compare their respective column vectors:

```{r}
#| label: tbl-colvecs
#| tbl-cap: Column vectors for nodes E and G.

    A <- as.matrix(as_adjacency_matrix(gr))
    diag(A) <- "--"
    A <- A[, c(5, 7)]
    kbl(A, format = "pipe", align = "c") 
```

And indeed, they are both the same! When we compute the **structural equivalence matrix** based on the Euclidean distance for the graph shown in fig-direquiv using @eq-deuclid, we end up with:

```{r}
#| label: tbl-dist2
#| tbl-cap: Structural Equivalence matrix for an undirected graph.
#| tbl-cap-location: margin

    eudist <- function(x) {
      r <- nrow(x)
      edist <- matrix(0, r, r)
      for (i in 1: r) {
          for (j in 1: r) {
            e1 <- x[i, ] - x[j, ]
            e1 <- e1^2
            e1 <- round(sqrt(sum(e1)), 1)
            e2 <- x[, i] - x[, j]
            e2 <- e2^2
            e2 <- round(sqrt(sum(e2)), 1)
            edist[i, j] <- e1 + e2
          }
        }
    return(edist)
    }

    fr <- c("A", "A", "A", "A", "B", "B", "B", "C", "D", "D", "E", "E", "F", "G", "G", "D", "D", "B")
    to <- c("C", "D", "B", "F", "A", "D", "C", "A", "A", "B", "C", "F", "A", "F", "C", "E", "G", "F")
    edge.dat <- data.frame(fr, to)
    node.dat <- data.frame(name = union(fr, to))
    gr <- tbl_graph(edges = edge.dat, nodes = node.dat)
    a <- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))
    rownames(a) <- V(gr)$name
    colnames(a) <- V(gr)$name
    d <- eudist(a)
    rownames(d) <- rownames(a)
    colnames(d) <- colnames(a)
    diag(d) <- "--"
    kbl(d, format = "pipe", align = "c")
```

Which tells us that nodes *E* and *G* in @fig-direquiv are structurally equivalent $d_{E,G} = 0$, but so are nodes *C* and *F*. These two pair of nodes send ties to the same out-neighbors and receive ties from the same in-neighbors.^[Can you figure out who those are in the figure?] So, we can say that nodes *E* and *G* occupy one position in the network and nodes *C* and *F* occupy another position. Perhaps if this were an office, and the relation was one of advice, this would reveal two set of actors who have a similar **role** in the office network. 

## Advanced: Measuring Structural Equivalence Using the Correlation Distance

It turns out there is an even fancier way to find out whether two nodes in a graph are structurally equivalent. It relies on a more complicated measure of distance called **the correlation distance**. This measure compares the row (or column) vectors of nodes in a graph and returns a number between $-1$ and $+1$. When it comes to structural equivalence, the correlation distance works like this:

- Pairs of structurally equivalent nodes get a $+1$. Nodes that are *almost* structurally equivalent but not quite get a number close to $+1$ (e.g., $0.90$).
- Nodes that are *completely different* from one another (that is connect to completely **disjoint** sets of neighbors) get a $-1$. In this case, nodes are opposites: Every time one node *i* has a $1$ in their row vector in the adjacency matrix, the other has a $0$ and vice versa. 
- Nodes that have a combination of similarities and differences in their pattern of connectivity to others get a number between $-1$ and $+1$, with $0$ indicating that two nodes have an even number of commonalities and differences. 

The correlation distance between two nodes *k* and *l* is computed using the following formula:

$$
    d^{corr}_{k, l} = 
    \frac{
    \sum_j
    (a_{(k)j} - \overline{a}_{(k)j}) \times
    (a_{(l)j} - \overline{a}_{(l)j})
    }
    {
    \sqrt{
    \sum_j
    (a_{(k)j} - \overline{a}_{(k)j})^2 \times
    \sum_j
    (a_{(l)j} - \overline{a}_{(l)j})^2
        }
    }
$$ {#eq-corr}

@eq-corr looks like a monstrously complicated one, but it is actually not that involved. 

Let's go through each components that we have already encountered: 

- $a_{(k)j}$ is the row vector for node *k* in the adjacency matrix.
- $a_{(l)j}$ is the row vector for node *l* in the adjacency matrix.

Now let's introduce ourselves to some new friends. For instance, what the heck is $\overline{a}_{(k)j}$? The little "bar" on top the $a$ indicates that we are taking the **mean** or the **average** of the elements of the row vector. 

In equation form:

$$
\overline{a}_{(k)j} = \frac{\sum_j a_{kj}}{N}
$$ {#eq-avg}

In @eq-avg, $\sum_i a_{kj}$ is the sum of all the elements in the vector, and $N$ is the **length** of the vector, which is equivalent to the **order** of the graph from which adjacency matrix came from (the number of nodes in the network).

So for instance, in @tbl-adjmat1, the row vector for node *A* is:

```{r}
#| label: tbl-rowvecA
#| tbl-cap: Row vector for node A.

    a <- matrix(c(0, 0, 1, 1, 0, 0), nrow = 1)
    kbl(a, format = "pipe", align = "c")
```

Which means that:

$$
\sum_i a_{(A)j} = 0 + 0 + 1 + 1 + 0 + 0 = 2
$$

And we know that $N = 6$, so that means that:

$$
\overline{a}_{(A)j} = \frac{\sum_i a_{Aj}}{N} = \frac{2}{6}=0.33
$$
The term $\overline{a}_{(k)j}$ is called the **row mean** for node *k* in the adjacency matrix. Just like we can compute row means, we can also compute **column means** by using the elements of the column vector $\overline{a}_{(k)i}$ 

So now that we know what the row means are, we can make sense of the term $(a_{(k)j} - \overline{a}_{(k)j})$ in @eq-corr. This is a vector composed of the *differences* between the row vector entries in the adjacency matrix and the row mean for that node. So in the case of node *A* and the row vector in @tbl-rowvecA that would mean:

```{r}
#| label: tbl-rowmeandiffA
#| tbl-subcap: true
#| layout-ncol: 1
#| tbl-cap: Row vector of mean differences for node A.

    a <- c(0, 0, 1, 1, 0, 0)
    b <- c("(0 - 0.33)", "(0 - 0.33)", "(1 - 0.33)", "(1 - 0.33)", "(0 - 0.33)", "(0 - 0.33)")
    c <- a - 0.33
    rm.1 <- matrix(b, nrow = 1)
    rm.2 <- matrix(c, nrow = 1)
    kbl(rm.1, format = "pipe", align = "c")
    kbl(rm.2, format = "pipe", align = "c")
```

Which means that:
$$
\sum_j (a_{(k)j} - \overline{a}_{(k)j}) = -0.33 -0.33 + 0.67 + 0.67 -0.33 -0.33 = 0.02
$$

So, the numerator of @eq-corr, just says: ``Take the entries in the row vector for the first node, and create a new vector composed of the those entries minus the row means and sum the vector. Then do the same for the other node and multiply the two numbers" And in the denominator of the equation we just square the same vectors sum them, multiply each of the two numbers and take the square root of the result product. Once we have the numerator and denominator we can evaluate the fraction and compute the correlation distance between those two nodes.

When we do that for each pair of nodes in @tbl-adjmat1, we end up with the **structural equivalence matrix** shown in @tbl-cor 

```{r}
#| label: tbl-cor
#| tbl-cap: Correlation distance matrix for an undirected graph.
#| tbl-cap-location: margin
 
    gr <- create_empty(6, directed = FALSE)
    gr <- gr %>% 
        bind_edges(data.frame(from = 1, to = 3:4)) %>% 
        bind_edges(data.frame(from = 2, to = 3:4)) %>% 
        bind_edges(data.frame(from = 3, to = 1:2)) %>% 
        bind_edges(data.frame(from = 4, to = 1:2)) %>% 
        bind_edges(data.frame(from = 5, to = 3:4)) %>% 
        bind_edges(data.frame(from = 6, to = 5)) %>% 
        mutate(name = toupper(letters[1:6]))
    gr <- simplify(gr)
    a <- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))
    rownames(a) <- V(gr)$name
    colnames(a) <- V(gr)$name
    a <- a[order(row.names(a)), order(row.names(a))]
    a <- corr.dist(a)
    diag(a) <- "--"
    kable(a, format = "pipe", padding = 2, align = "c") 
```

In the @tbl-cor, the structurally equivalent pairs of nodes, *A* and *B* and *C* and *D* have $d^{corr} = 1.0$. Nodes that are completely non-equivalent like *C* and *E* and *D* and *E* have $d^{corr} = -1.0$.

### Iterated Correlational Distances: Concor
What happens if we were to try to use @eq-cor to find the *correlation distance of a correlation distance* matrix? If we were to do this and use @tbl-cor as our input matrix, we end up with @tbl-concor (a). Now, as Leo always says: "We need to go deeper."^[[https://knowyourmeme.com/memes/we-need-to-go-deeper](https://knowyourmeme.com/memes/we-need-to-go-deeper)] And, indeed, we can. We can take the correlation distance of the nodes based on @tbl-concor (a). If we do that, we end up with the entries in @tbl-concor (b). If we keep on going, we end up with the entries in @tbl-concor (c). Note that in this matrix, there are only two entries: $+1$ and $-1$! 

```{r}
#| label: tbl-concor
#| tbl-cap: "Iterated correlational distances."
#| tbl-subcap:
#|   - "Second Iteration"
#|   - "Third Iteration"
#|   - "Fourth Iteration"
#| layout-ncol: 1
 
    gr <- create_empty(6, directed = FALSE)
    gr <- gr %>% 
        bind_edges(data.frame(from = 1, to = 3:4)) %>% 
        bind_edges(data.frame(from = 2, to = 3:4)) %>% 
        bind_edges(data.frame(from = 3, to = 1:2)) %>% 
        bind_edges(data.frame(from = 4, to = 1:2)) %>% 
        bind_edges(data.frame(from = 5, to = 3:4)) %>% 
        bind_edges(data.frame(from = 6, to = 5)) %>% 
        mutate(name = toupper(letters[1:6]))
    gr <- simplify(gr)
    a <- matrix(as_adjacency_matrix(gr), nrow = length(V(gr)))
    rownames(a) <- V(gr)$name
    colnames(a) <- V(gr)$name
    a <- a[order(row.names(a)), order(row.names(a))]
    a <- corr.dist(a)
    b <- corr.dist(a)
    c <- corr.dist(b)
    d <- corr.dist(c)
    diag(b) <- "--"
    diag(c) <- "--"
    diag(d) <- "--"
    kbl(b, format = "pipe", align = "c") 
    kbl(c, format = "pipe", align = "c") 
    kbl(d, format = "pipe", align = "c") 
```

More importantly, as shown in @tbl-sepos, the structurally equivalent nodes in this matrix have the exact same pattern of $+1$s and $-1$s across the rows. This procedure of iterated correlations, invented by a team of sociologists and psychologists in the 1970s [@breiger75], is called **CONCOR**---and acronym for the hard to remember title of "**con**vergence of iterate **cor**relations''---and is designed to extract structurally equivalent positions from networks. 

```{r}
#| label: tbl-sepos
#| tbl-cap: "Structurally Equivalent Positions in an undirected graph."
#| tbl-cap-location: margin

    kbl(d, format = "html", align = "c", padding = 2) %>% 
        row_spec(1:2, background = "salmon", bold = TRUE) %>%  
        row_spec(3:4, background = "lightgreen", bold = TRUE) %>% 
        row_spec(5, background = "cyan", bold = TRUE) %>% 
        row_spec(6, background = "violet", bold = TRUE) 
```

## Structural Similarity {#sec-sim}
In most real-world applications, the standard definition of structural equivalence is much too strong. In answer to the question of whether two nodes occupy the same position in the network it only allows for a "yes/no" answer. Yes, if their neighborhoods are *exactly* the same, and "no" if there aren't. 

What we need is a measure of position that allows for "more or less" rather than "yes" and "no." This is what is called **structural similarity** [@leicth_etal06]. Two nodes are structurally similar if they have similar patterns of connectivity with the same others. There are various versions of structural similarity between nodes. Here we will consider some popular ones.

As we noted in the original graph theory lesson, it is possible for the **neighborhood** of two nodes in a graph to *overlap*. Recall that for each node, we define its neighborhood as the set of other nodes that they are adjacent to. That means the neighborhood between two nodes can have members in common. The more members they have in common the more **structurally similar** two nodes are. 

```{r stsim, fig.margin = TRUE, fig.cap="An undirected graph.", fig.width=10, fig.height=12}
    set.seed(456)
    gr <- play_blocks(n = 12, size_blocks = 12, p_between = 0.35, directed = FALSE)
    V(gr)$name <- toupper(letters[1:length(V(gr))])
    p <- ggraph(gr, layout = 'auto') 
    p <- p + geom_edge_link(color = "steelblue", width = 1.25) 
    p <- p + geom_node_point(aes(x = x, y = y), size = 22, color = "tan2") 
    p <- p + geom_node_text(aes(label = name), size = 12, color = "white")
    p <- p + theme_graph() 
    p
```
    
For instance, imagine you have a friend and that friend knows *all* your friends and you know *all* their friends. In which case we would say that the overlap between your node neighborhoods is pretty high; in fact the two neighborhoods overlap *completely*, which makes you structurally equivalent! But even if your friend knows 90% of the people in your network (and you know 90% of the people in their network) that would make you very structurally similar to one another. Now imagine you just met a new person online who lives in a far away country, and as far as you know, they know *none* of your friends and you know *none* of their friends. In which case, we would say that the overlap if the two neighborhoods is nil or as close to zero as it can get. You occupy completely different positions in the network. 


## Measuring Structrual Similarity Among Nodes in Graph
### Jaccard Similarity
We can use this reasoning to construct a measure of structural similarity between two nodes called **[Jaccard's Similarity Coefficient](https://en.wikipedia.org/wiki/Jaccard_index)** ($J_{ij}$). It goes like this. Let's say $n_{ij}$ is the number of friends that nodes *i* and *j* have in common, and the total number of *i*'s friends if $k_i$ (*i*'s degree) and the total number of *j*'s friends if $k_j$. Then the structural similarity of *i* and *j* is given by: 

$$
  J_{ij} = \frac{n_{ij}}{k_i + k_j - n_{ij}}
$$   {#eq-jaccard}

It says that the structural similarity of two nodes is equivalent to the number of friends that the two persons know in common, divided by the sum of their degrees minus the number of people they know in common. Jaccard's coefficient ranges from zer (when $n_{ij}=0$ and the two nodes have no neighbors in common) to 1.0 (when $n_{ij} = k_i$ and $n_{ij} = k_j$ and the two nodes are structurally equivalent). 

### Dice Similarity
A second measure of structural similarity between nodes is the **[Dice Similarity Index]()**. This is given by:

$$
  D_{ij} = \frac{2n_{ij}}{k_i + k_j}
$$   {#eq-dice}

Which says that the structural similarity between two nodes is equivalent to the twice the number of people the know in common, divided by the sum of their degrees. 

### Cosine Similarity
A third and final measure of structural similarity between two nodes is the **cosine similarity** between their respective neighborhoods ($C_{ij}$). This is given by: 

$$
  C_{ij} = \frac{n_{ij}}{\sqrt{k_ik_j}}
$$   {#eq-cosine}


Which says that the structural similarity between two nodes is equivalent to the number of people they know in common divided by the square root of the product of their degrees (which is also referred to as the **geometric mean** of their degrees). 

A lot of the times, these three measures of structural similarity will tend to agree. Tables @tbl-tabj), @tbl-tabd), @tbl-tabc) show the similarities between each pair of nodes in the graph depicted in @fig-stsim).

```{r tabj}
    J <- round(similarity(gr, method = "jaccard"), 2)
    rownames(J) <- V(gr)$name
    colnames(J) <- V(gr)$name
    diag(J) <- "--"
    J[lower.tri(J)] <- "--"
    kbl(J, format = "simple", align = "c",
        caption = "Structural similarities for pairs of nodes in an undirected graph (Jaccard).")
```

```{r tabd}
    D <- round(similarity(gr, method = "dice"), 2)
    rownames(D) <- V(gr)$name
    colnames(D) <- V(gr)$name
    diag(D) <- "--"
    D[lower.tri(D)] <- "--"
    kbl(D, format = "simple", align = "c", 
        caption = "Structural similarities for pairs of nodes in an undirected graph (Dice).")
```
  
```{r tabc}
    a <- as.matrix(as_adjacency_matrix(gr))
    C <- round(cosine(a), 2)
    rownames(C) <- V(gr)$name
    colnames(C) <- V(gr)$name
    diag(C) <- "--"
    C[lower.tri(C)] <- "--"
    kbl(C, format = "simple", align = "c",
        caption = "Structural similarities for pairs of nodes in an undirected graph (cosine).")
```

## Structural Similarity in Directed Graphs
Similarity works in similar (pun intended) ways when studying asymmetric ties in directed graph. The main difference, as usual, is that in a directed graph pairs of nodes can structurally similar in two different ways. Fist, pairs of nodes can be similar with respect to their out-neighborhoods, in which case we say that nodes are structural similar if they point to the same set of neighbors. This is called the **out-similarity**. Second, pairs of nodes can be similar with respect to their in-neighborhoods, in which case we say that nodes are structural similar if they *receive* ties or nominations from the same set of neighbors. This is called the **in-similarity**.

Special cases of the out and in-similarities between nodes show up in particular types of networks. For instance, consider an information network composed of scientific papers. Here a directed tie emerges when paper *A* *cites* or refers to paper *B*. This is called a **citation network**. 

In a citation network out-similar papers are papers that *cite* the same other papers. Out-similar papers are said to exhibit **bibliographic coupling** (essentially the overlap or set intersection between their reference lists). A weighted network of similarities between papers, where the weight of the edge is the number of other papers that that they both cite in common is called a **bibliographic coupling network**. A bibliographic coupling network is essentially a network of out-similarities between papers in a scientific information network. 

In a citation network, in-similar papers are papers that *get cited* by the same set of others. In this case, we say that the two papers are **co-cited** a third paper. A weighted network of similarities between papers, where the weight of the edis the number of other papers that cite both of them is called **co-citation network**. A co-citation network is essentially a network of in-similarities between papers in a scientific information network.   

The two measures of out and in-similarities can be defined in the same way as before. If $n^{out}_{ij}$ is the number of common out-neighbors of nodes *i* and *j* and $n^{in}_{ij}$ is the number of their common out-neighbors, $k_{out}$ is the total number of out-neighbors of a particular node, and $k_{in}$ is the total number of in-neighbors, then the structural out and in-similarities between pairs of nodes *i* and *j* are given by (using the cosine distance measure) by:

$$
  C_{ij}^{out} = \frac{n_{ij}^{out}}{\sqrt{k_i^{out}k_j^{out}}}
$$   {#eq-cosout}



$$
  C_{ij}^{in} = \frac{n_{ij}^{in}}{\sqrt{k_i^{in}k_j^{in}}}
$$   {#eq-cosin}

## References {.unnumbered}
